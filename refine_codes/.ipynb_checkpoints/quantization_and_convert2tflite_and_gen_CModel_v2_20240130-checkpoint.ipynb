{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692379c8-c60a-4ddb-a425-09c0b030bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import glob;\n",
    "import math;\n",
    "import random;\n",
    "import torch;\n",
    "import torch.nn as nn;\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5939dcb3-4805-42d7-9900-637088d59e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1585005-0d41-4964-a79f-63ac0a2fc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7733cacf-1504-4ce6-983c-871ee1dc116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.opts as opts;\n",
    "import th.resources.models as models;\n",
    "import th.resources.calculator as calc;\n",
    "# import resources.train_generator as train_generator;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df514eb-ae13-469e-bb95-c92fbdd7cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# import common.tlopts as tlopts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d04c10f-d1a7-454a-b163-cac1d59dd7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.utils as U;\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from tinynn.converter import TFLiteConverter\n",
    "from tinynn.graph.quantization.quantizer import PostQuantizer\n",
    "from tinynn.graph.tracer import model_tracer\n",
    "from tinynn.util.train_util import DLContext, get_device\n",
    "from tinynn.graph.quantization.algorithm.cross_layer_equalization import cross_layer_equalize\n",
    "from tinynn.converter import TFLiteConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa9206ea-537a-4ad0-a008-a2d2a6c11b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#refsite:https://discuss.pytorch.org/t/how-to-generate-a-fully-quantized-model/175185\n",
    "from torch.ao.quantization.backend_config import BackendConfig, BackendPatternConfig, DTypeConfig, ObservationType\n",
    "from torch.quantization import quantize_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "from torch.ao.quantization.fx.custom_config import PrepareCustomConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ded3e2c-21bc-4bac-9c9f-fa5e6697b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9de080e-60bb-4cba-b9ac-8727f3b422d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076adfb-46fb-44ba-98e0-c38a9db82f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a1baf79-1e4b-4fe0-8f76-7b7163ec3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDataTimeStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H:%M:%S').replace('-',\"\").replace(' ',\"\").replace(':',\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fab4a323-4291-421b-867a-d52ee41063ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = dict([(52,1),(99,2)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[label1]- 1\n",
    "            idx2 = self.mapdict[label2] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb20ff6-42ad-4671-969f-d6ad8abe5bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a30933e-1045-41a0-b258-2ef29e94bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None):\n",
    "    # dataset = np.load(os.path.join(opt.data, opt.dataset, 'wav{}.npz'.format(opt.sr // 1000)), allow_pickle=True);\n",
    "    # dataset = np.load(\"../datasets/fold1_test16000.npz\", allow_pickle=True);\n",
    "    dataset = np.load(\"../datasets/forOneClassModel_alarm/train/trainSet_20240119002902.npz\", allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # print(len(dataset['x']))\n",
    "    # for i in range(1, opt.nFolds + 1):\n",
    "\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['fold{}'.format(1)].item()['sounds']\n",
    "    train_labels = dataset['fold{}'.format(1)].item()['labels']\n",
    "    # print(train_sounds)\n",
    "\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc776e4-0edf-4424-a3af-a946d33db7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='./datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 32;\n",
    "    opt.weightDecay = 5e-3;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.nEpochs = 800;#2000;\n",
    "    opt.LR = 0.1;\n",
    "    opt.schedule = [0.3, 0.6, 0.9];\n",
    "    opt.warmup = 10;\n",
    "\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 2#50;\n",
    "    opt.nFolds = 1;#5;\n",
    "    opt.split = 1#[i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    opt.ch_config = [8,64,32,64,64,128,128,256,256,512,512,2];\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb634178-30c9-4757-9db0-3a65a1c8479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_int8(x, axis):\n",
    "  '''Quantization into int8_t precision, operating on x along axis'''\n",
    "\n",
    "  scaling_factor_shape = tuple(np.append([len(x)],np.ones(x.ndim - 1, dtype = int)))\n",
    "  epsilon = 0.000000001\n",
    "  x_scaling_factor = (2 * np.max(np.abs(x), axis) / 255) + epsilon\n",
    "  x_scaling_factor = x_scaling_factor.reshape(scaling_factor_shape)\n",
    "  x_zero_offset = -0.5\n",
    "  result = (x / x_scaling_factor) + x_zero_offset\n",
    "\n",
    "  return np.rint(result).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d56ed47-bc98-4899-9e46-d4726cfa565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, opt=None, split=0):\n",
    "        self.opt = opt;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.trainX = None;\n",
    "        self.trainY = None;\n",
    "        # self.opt.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "        self.opt.device = torch.device(\"cpu\")\n",
    "        self.trainGen = getTrainGen(self.opt)#train_generator.setup(self.opt, self.opt.split);\n",
    "        self.qunt_nClass = 2;\n",
    "\n",
    "    def load_train_data(self):\n",
    "        print('Preparing calibration dataset..');\n",
    "        x,y = self.trainGen.__getitem__(0);\n",
    "        self.trainX = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "        \"\"\"\n",
    "        trainX size:torch.Size([1, 1, 30225]), but must be [1,1,1,30225]\n",
    "        Due to the reason: raise ValueError(\"Input shape must be `(N, C, H, W)`!\")\n",
    "        \"\"\"\n",
    "        # print(f\"trainX[0] shape:{self.trainX[0].shape}\")\n",
    "        self.trainY = torch.tensor(y).to(self.opt.device);\n",
    "        print('Calibration dataset is ready');\n",
    "        # self.opt.batchSize = 32;\n",
    "\n",
    "    def load_test_data(self):\n",
    "        if(self.testX is None):\n",
    "            data = np.load('../datasets/forOneClassModel_alarm/test_val/final_val_test_npz/final_valSet_20240119004614.npz', allow_pickle=True);\n",
    "            dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "            self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "            self.testY = torch.tensor(data['y']).to(self.opt.device);\n",
    "\n",
    "    def __validate(self, net, testX, testY):\n",
    "        net.eval();\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = len(self.testX);\n",
    "            x = self.testX[:];\n",
    "            \n",
    "            # batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "            # for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "            #     x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "            #     #print(x.shape);\n",
    "            #     # exit();\n",
    "            #     scores = net(x);\n",
    "            #     y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "            scores = net(x);\n",
    "            y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "            acc = self.__compute_accuracy(y_pred, self.testY);\n",
    "        return acc;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def __compute_accuracy(self, y_pred, y_target):\n",
    "        print(y_pred.shape);\n",
    "        with torch.no_grad():\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1);\n",
    "\n",
    "            y_pred = y_pred.argmax(dim=1);\n",
    "            y_target = y_target.argmax(dim=1);\n",
    "\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "        return acc;\n",
    "\n",
    "    def __load_model(self, quant=True):\n",
    "        state = torch.load(self.opt.model_path, map_location=self.opt.device);\n",
    "        print(state['config']);\n",
    "        net = None;\n",
    "        # if quant:\n",
    "        net = models.GetACDNetQuantModel(input_len=self.opt.inputLength, nclass=self.qunt_nClass, sr=self.opt.sr, channel_config=state['config']).to(self.opt.device);\n",
    "        # else:\n",
    "            # net = models.GetACDNetModel(input_len=self.opt.inputLength, nclass=self.qunt_nClass, sr=self.opt.sr, channel_config=state['config']).to(self.opt.device);\n",
    "            # net = GetTLACDNet(opt=self.opt);\n",
    "        calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        net.load_state_dict(state['weight']);\n",
    "        return net;\n",
    "\n",
    "    def __calibrate(self, net):\n",
    "        self.load_train_data();\n",
    "        net.eval();\n",
    "        with torch.no_grad():\n",
    "            for i in range(1,2):\n",
    "                x_pred = None;\n",
    "                for idx in range(math.ceil(len(self.trainX)/self.opt.batchSize)):\n",
    "                    x = self.trainX[idx*self.opt.batchSize : (idx+1)*self.opt.batchSize];\n",
    "                    #print(x.shape);\n",
    "                    # exit();\n",
    "                    scores = net(x);\n",
    "                    x_pred = scores.data if x_pred is None else torch.cat((x_pred, scores.data));\n",
    "\n",
    "                x_pred = x_pred.argmax(dim=1);\n",
    "                x_target = self.trainY.argmax(dim=1);\n",
    "\n",
    "                acc = (((x_pred==x_target)*1).float().mean()*100).item();\n",
    "                print('calibrate accuracy is: {:.2f}'.format(acc));\n",
    "        return acc;\n",
    "\n",
    "    def QuantizeModel(self):\n",
    "        net = self.__load_model(True);\n",
    "        # net = self.__load_model(False);\n",
    "        config = net.ch_config;\n",
    "        net.eval();\n",
    "        net.qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')\n",
    "        #Fuse modules to\n",
    "        torch.quantization.fuse_modules(net.sfeb, ['0','1','2'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.sfeb, ['3','4','5'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['0','1','2'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['4','5','6'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['7','8','9'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['11','12','13'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['14','15','16'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['18','19','20'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['21','22','23'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['25','26','27'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['28','29','30'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['33','34','35'], inplace=True);\n",
    "        # torch.ao.quantization.fuse_modules(net.sfeb, ['0','1','2'], inplace=True)\n",
    "        # torch.ao.quantization.fuse_modules(net.sfeb, ['3','4','5'], inplace=True)\n",
    "        # torch.ao.quantization.fuse_modules(net.tfeb, ['0','1','2'], inplace=True)\n",
    "        # torch.ao.quantization.fuse_modules(net.tfeb, ['4','5','6'], inplace=True)\n",
    "        # Specify quantization configuration\n",
    "        # net.qconfig = torch.quantization.get_default_qconfig('qnnpack');\n",
    "        \n",
    "        torch.backends.quantized.engine = 'qnnpack';\n",
    "        print(net.qconfig);\n",
    "        torch.quantization.prepare(net, inplace=True);\n",
    "        \n",
    "        # Calibrate with the training data\n",
    "        self.__calibrate(net);\n",
    "\n",
    "        # Convert to quantized model\n",
    "        torch.quantization.convert(net, inplace=True);\n",
    "        print('Post Training Quantization: Convert done');\n",
    "\n",
    "        print(\"Size of model after quantization\");\n",
    "        torch.save(net.state_dict(), \"temp.p\")\n",
    "        print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "        os.remove('temp.p')\n",
    "        \n",
    "        self.load_test_data();\n",
    "        val_acc = self.__validate(net, self.testX, self.testY);\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "\n",
    "        # torch.jit.save(torch.jit.script(net), '{}/th/quantized_models/{}.pt'.format(os.getcwd(), self.opt.model_name.format()));\n",
    "        torch.jit.save(torch.jit.script(net), '../th/quantized_models/{}.pt'.format(self.opt.model_name.format()));\n",
    " \n",
    "        # **************convert to tflite**********\n",
    "        with torch.no_grad():\n",
    "            # dummy_input = torch.randn(1, 1, 30225, 1); wrong: RuntimeError: quantized::conv2d (qnnpack): each dimension of output tensor should be greater than 0.\n",
    "            dummy_input = torch.FloatTensor(quantize_int8(torch.randn(1, 1, 1, 30225).numpy(),3)); #correct,workable\n",
    "            # dummy_input = torch.randn(30225,1,1,1); wrong: RuntimeError: quantized::conv2d (qnnpack): each dimension of output tensor should be greater than 0.\n",
    "            # dummy_input = torch.randn(1,30225,1,1); wrong:RuntimeError: Input channel size of weight and bias must match.\n",
    "            converter = TFLiteConverter(net,\n",
    "                                        dummy_input,\n",
    "                                        quantize_input_output_type='int8',#設定此欄，輸入會強制為int8\n",
    "                                        fuse_quant_dequant=True,\n",
    "                                        quantize_target_type='int8',\n",
    "                                        hybrid_conv=False,\n",
    "                                        float16_quantization=True,\n",
    "                                        optimize=5,\n",
    "                                        tflite_path=\"../th/quantized_models/{}.tflite\".format(self.opt.model_name))\n",
    "            converter.convert()\n",
    "\n",
    "        print(net.state_dict())\n",
    "            # qmodel = copy.deepcopy(mynn)\n",
    "            # torch.quantization.convert(qmodel, inplace=False)\n",
    "            #\n",
    "            # torch.backends.quantized.engine = 'qnnpack'\n",
    "            # converter = TFLiteConverter(qmodel.module,\n",
    "            #                             torch.randn(1, 64, nn_h, nn_w,\n",
    "            #                             tflite_path=\"qmodel.tflite\")\n",
    "            # converter.convert()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def TestModel(self, quant=False):\n",
    "        if quant:\n",
    "            net = torch.jit.load('../th/quantized_models/{}.pt'.format(self.opt.model_name))\n",
    "        else:\n",
    "            net = self.__load_model();\n",
    "            # calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        self.load_test_data();\n",
    "        net.eval();\n",
    "        val_acc = self.__validate(net, self.testX, self.testY);\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "\n",
    "    def GetModelSize(self):\n",
    "        orig_net_path = self.opt.model_path;\n",
    "        print('Full precision model size (KB):', os.path.getsize(orig_net_path)/(1024));\n",
    "        save_onnx_name = \"{}.onnx\".format(self.opt.model_name);\n",
    "        quant_net_path = \"../th/onnx_models/\"+save_onnx_name;\n",
    "        print('Quantized model size (KB):', os.path.getsize(quant_net_path)/(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58eff9fb-5fd1-423b-9a60-61c9e9824dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_convert_model_path = \"./th/pruned_models/second_stage_pruned_models/magnitude_pruning/acdnet_tl_hybrid_pruning_magnitude_model_202312281149_80.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f986060d-9a68-4021-b9c4-bfae8d252d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_tensor type:<class 'torch.Tensor'>\n",
      "[[[[ 21 -50  26 ... -12  16  13]]]]\n",
      "(1, 1, 1, 30225)\n"
     ]
    }
   ],
   "source": [
    "#before calling quantize_int8, torch tensor need to be converted to ndarray\n",
    "test_ary = torch.randn(1, 1, 1, 30225).numpy();\n",
    "\n",
    "\n",
    "q_test_ary = quantize_int8(test_ary,3)\n",
    "test_tensor = torch.CharTensor(q_test_ary)\n",
    "print(f\"test_tensor type:{type(test_tensor)}\")\n",
    "print(q_test_ary)\n",
    "print(q_test_ary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce59dbc6-1088-427e-a3bb-5e88d07465d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    opt = getOpts();#opts.parse();\n",
    "    opt.device = 'cpu';\n",
    "    opt.model_path = \"./retrained_models_after_pruned/retrained_model_20240124123209_acc_95.45455169677734_795th_epoch.pt\"\n",
    "    # valid_path = False;\n",
    "    # while not valid_path:\n",
    "    #     model_path = input(\"Enter the model PATH for 8-bit post training quantization\\n:\");\n",
    "    #     file_paths = glob.glob(os.path.join(os.getcwd(), model_path));\n",
    "    #     if len(file_paths)>0 and os.path.isfile(file_paths[0]):\n",
    "    #         state = torch.load(file_paths[0], map_location='cpu');\n",
    "    #         opt.model_path = file_paths[0];\n",
    "    #         print('Model has been found at: {}'.format(opt.model_path));\n",
    "    #         valid_path = True;\n",
    "\n",
    "    opt.model_name = \"quant_retrained_model_95.4_{}\".format(genDataTimeStr());\n",
    "    # valid_model_name = False;\n",
    "    # while not valid_model_name:\n",
    "    #     model_name = input('Enter a name that will be used to save the quantized model model: ');\n",
    "    #     if model_name != '':\n",
    "    #         opt.model_name = model_name;\n",
    "    #         valid_model_name = True;\n",
    "    opt.split = 1;\n",
    "    opt.hasQuated = False;\n",
    "    trainer = Trainer(opt);\n",
    "\n",
    "    print('Testing performance of the provided model.....');\n",
    "    trainer.TestModel();\n",
    "\n",
    "    print('Quantization process is started.....');\n",
    "    trainer.QuantizeModel();\n",
    "    print('Quantization done');\n",
    "\n",
    "    print('Testing quantized model.');\n",
    "    trainer.TestModel(True);\n",
    "    print('Finished');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a95756a3-5cc4-4276-9272-a22ec24efaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of samples:325\n",
      "Testing performance of the provided model.....\n",
      "[5, 32, 9, 16, 23, 33, 29, 56, 47, 65, 90, 2]\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (5, 1, 15109)         45      679,905\n",
      "  BatchNorm2d-2     (5, 1, 15109)     (5, 1, 15109)         10            0\n",
      "         ReLu-3     (5, 1, 15109)     (5, 1, 15109)          0       75,545\n",
      "       Conv2d-4     (5, 1, 15109)     (32, 1, 7553)        800    6,042,400\n",
      "  BatchNorm2d-5     (32, 1, 7553)     (32, 1, 7553)         64            0\n",
      "         ReLu-6     (32, 1, 7553)     (32, 1, 7553)          0      241,696\n",
      "    MaxPool2d-7     (32, 1, 7553)      (32, 1, 151)          0      241,600\n",
      "      Permute-8      (32, 1, 151)      (1, 32, 151)          0            0\n",
      "       Conv2d-9      (1, 32, 151)      (9, 32, 151)         81      391,392\n",
      " BatchNorm2d-10      (9, 32, 151)      (9, 32, 151)         18            0\n",
      "        ReLu-11      (9, 32, 151)      (9, 32, 151)          0       43,488\n",
      "   MaxPool2d-12      (9, 32, 151)       (9, 16, 75)          0       43,200\n",
      "      Conv2d-13       (9, 16, 75)      (16, 16, 75)      1,296    1,555,200\n",
      " BatchNorm2d-14      (16, 16, 75)      (16, 16, 75)         32            0\n",
      "        ReLu-15      (16, 16, 75)      (16, 16, 75)          0       19,200\n",
      "      Conv2d-16      (16, 16, 75)      (23, 16, 75)      3,312    3,974,400\n",
      " BatchNorm2d-17      (23, 16, 75)      (23, 16, 75)         46            0\n",
      "        ReLu-18      (23, 16, 75)      (23, 16, 75)          0       27,600\n",
      "   MaxPool2d-19      (23, 16, 75)       (23, 8, 37)          0       27,232\n",
      "      Conv2d-20       (23, 8, 37)       (33, 8, 37)      6,831    2,021,976\n",
      " BatchNorm2d-21       (33, 8, 37)       (33, 8, 37)         66            0\n",
      "        ReLu-22       (33, 8, 37)       (33, 8, 37)          0        9,768\n",
      "      Conv2d-23       (33, 8, 37)       (29, 8, 37)      8,613    2,549,448\n",
      " BatchNorm2d-24       (29, 8, 37)       (29, 8, 37)         58            0\n",
      "        ReLu-25       (29, 8, 37)       (29, 8, 37)          0        8,584\n",
      "   MaxPool2d-26       (29, 8, 37)       (29, 4, 18)          0        8,352\n",
      "      Conv2d-27       (29, 4, 18)       (56, 4, 18)     14,616    1,052,352\n",
      " BatchNorm2d-28       (56, 4, 18)       (56, 4, 18)        112            0\n",
      "        ReLu-29       (56, 4, 18)       (56, 4, 18)          0        4,032\n",
      "      Conv2d-30       (56, 4, 18)       (47, 4, 18)     23,688    1,705,536\n",
      " BatchNorm2d-31       (47, 4, 18)       (47, 4, 18)         94            0\n",
      "        ReLu-32       (47, 4, 18)       (47, 4, 18)          0        3,384\n",
      "   MaxPool2d-33       (47, 4, 18)        (47, 2, 9)          0        3,384\n",
      "      Conv2d-34        (47, 2, 9)        (65, 2, 9)     27,495      494,910\n",
      " BatchNorm2d-35        (65, 2, 9)        (65, 2, 9)        130            0\n",
      "        ReLu-36        (65, 2, 9)        (65, 2, 9)          0        1,170\n",
      "      Conv2d-37        (65, 2, 9)        (90, 2, 9)     52,650      947,700\n",
      " BatchNorm2d-38        (90, 2, 9)        (90, 2, 9)        180            0\n",
      "        ReLu-39        (90, 2, 9)        (90, 2, 9)          0        1,620\n",
      "   MaxPool2d-40        (90, 2, 9)        (90, 1, 4)          0        1,440\n",
      "      Conv2d-41        (90, 1, 4)         (2, 1, 4)        180          720\n",
      " BatchNorm2d-42         (2, 1, 4)         (2, 1, 4)          4            0\n",
      "        ReLu-43         (2, 1, 4)         (2, 1, 4)          0            8\n",
      "   AvgPool2d-44         (2, 1, 4)         (2, 1, 1)          0            8\n",
      "     Flatten-45         (2, 1, 1)            (1, 2)          0            0\n",
      "      Linear-46            (1, 2)            (1, 2)          6            6\n",
      "     Softmax-47            (1, 2)            (1, 2)          0            2\n",
      "==============================================================================\n",
      "Total Params: 140,427\n",
      "Total FLOPs : 22,177,258\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 0.54\n",
      "Total size (MB) : 0.65\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([176, 2])\n",
      "Testing: Acc(top1) 95.45%\n",
      "Quantization process is started.....\n",
      "[5, 32, 9, 16, 23, 33, 29, 56, 47, 65, 90, 2]\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (5, 1, 15109)         45      679,905\n",
      "  BatchNorm2d-2     (5, 1, 15109)     (5, 1, 15109)         10            0\n",
      "         ReLu-3     (5, 1, 15109)     (5, 1, 15109)          0       75,545\n",
      "       Conv2d-4     (5, 1, 15109)     (32, 1, 7553)        800    6,042,400\n",
      "  BatchNorm2d-5     (32, 1, 7553)     (32, 1, 7553)         64            0\n",
      "         ReLu-6     (32, 1, 7553)     (32, 1, 7553)          0      241,696\n",
      "    MaxPool2d-7     (32, 1, 7553)      (32, 1, 151)          0      241,600\n",
      "      Permute-8      (32, 1, 151)      (1, 32, 151)          0            0\n",
      "       Conv2d-9      (1, 32, 151)      (9, 32, 151)         81      391,392\n",
      " BatchNorm2d-10      (9, 32, 151)      (9, 32, 151)         18            0\n",
      "        ReLu-11      (9, 32, 151)      (9, 32, 151)          0       43,488\n",
      "   MaxPool2d-12      (9, 32, 151)       (9, 16, 75)          0       43,200\n",
      "      Conv2d-13       (9, 16, 75)      (16, 16, 75)      1,296    1,555,200\n",
      " BatchNorm2d-14      (16, 16, 75)      (16, 16, 75)         32            0\n",
      "        ReLu-15      (16, 16, 75)      (16, 16, 75)          0       19,200\n",
      "      Conv2d-16      (16, 16, 75)      (23, 16, 75)      3,312    3,974,400\n",
      " BatchNorm2d-17      (23, 16, 75)      (23, 16, 75)         46            0\n",
      "        ReLu-18      (23, 16, 75)      (23, 16, 75)          0       27,600\n",
      "   MaxPool2d-19      (23, 16, 75)       (23, 8, 37)          0       27,232\n",
      "      Conv2d-20       (23, 8, 37)       (33, 8, 37)      6,831    2,021,976\n",
      " BatchNorm2d-21       (33, 8, 37)       (33, 8, 37)         66            0\n",
      "        ReLu-22       (33, 8, 37)       (33, 8, 37)          0        9,768\n",
      "      Conv2d-23       (33, 8, 37)       (29, 8, 37)      8,613    2,549,448\n",
      " BatchNorm2d-24       (29, 8, 37)       (29, 8, 37)         58            0\n",
      "        ReLu-25       (29, 8, 37)       (29, 8, 37)          0        8,584\n",
      "   MaxPool2d-26       (29, 8, 37)       (29, 4, 18)          0        8,352\n",
      "      Conv2d-27       (29, 4, 18)       (56, 4, 18)     14,616    1,052,352\n",
      " BatchNorm2d-28       (56, 4, 18)       (56, 4, 18)        112            0\n",
      "        ReLu-29       (56, 4, 18)       (56, 4, 18)          0        4,032\n",
      "      Conv2d-30       (56, 4, 18)       (47, 4, 18)     23,688    1,705,536\n",
      " BatchNorm2d-31       (47, 4, 18)       (47, 4, 18)         94            0\n",
      "        ReLu-32       (47, 4, 18)       (47, 4, 18)          0        3,384\n",
      "   MaxPool2d-33       (47, 4, 18)        (47, 2, 9)          0        3,384\n",
      "      Conv2d-34        (47, 2, 9)        (65, 2, 9)     27,495      494,910\n",
      " BatchNorm2d-35        (65, 2, 9)        (65, 2, 9)        130            0\n",
      "        ReLu-36        (65, 2, 9)        (65, 2, 9)          0        1,170\n",
      "      Conv2d-37        (65, 2, 9)        (90, 2, 9)     52,650      947,700\n",
      " BatchNorm2d-38        (90, 2, 9)        (90, 2, 9)        180            0\n",
      "        ReLu-39        (90, 2, 9)        (90, 2, 9)          0        1,620\n",
      "   MaxPool2d-40        (90, 2, 9)        (90, 1, 4)          0        1,440\n",
      "      Conv2d-41        (90, 1, 4)         (2, 1, 4)        180          720\n",
      " BatchNorm2d-42         (2, 1, 4)         (2, 1, 4)          4            0\n",
      "        ReLu-43         (2, 1, 4)         (2, 1, 4)          0            8\n",
      "   AvgPool2d-44         (2, 1, 4)         (2, 1, 1)          0            8\n",
      "     Flatten-45         (2, 1, 1)            (1, 2)          0            0\n",
      "      Linear-46            (1, 2)            (1, 2)          6            6\n",
      "     Softmax-47            (1, 2)            (1, 2)          0            2\n",
      "==============================================================================\n",
      "Total Params: 140,427\n",
      "Total FLOPs : 22,177,258\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 0.54\n",
      "Total size (MB) : 0.65\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=False){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "Preparing calibration dataset..\n",
      "Calibration dataset is ready\n",
      "calibrate accuracy is: 90.62\n",
      "Post Training Quantization: Convert done\n",
      "Size of model after quantization\n",
      "Size (MB): 0.158022\n",
      "torch.Size([176, 2])\n",
      "Testing: Acc(top1) 90.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (tinynn.converter.base) Generated model saved to ../th/quantized_models/quant_retrained_model_95.4_20240131220101.tflite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('sfeb.0.weight', tensor([[[[ 0.5615,  0.9188,  0.3573,  3.1649, -0.8678, -1.2251, -1.4803,\n",
      "           -0.4594, -0.2042]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8678,  0.1531,  0.0510, -1.2762, -0.2042, -1.1741,  0.8167,\n",
      "            0.1021, -2.0929]]],\n",
      "\n",
      "\n",
      "        [[[-1.4293, -1.6845,  0.0510,  1.6845,  0.2552,  0.8678, -0.7657,\n",
      "            0.0510,  0.2552]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7657,  0.8167, -1.6335,  2.5523, -5.7682,  4.6963,  0.7146,\n",
      "           -6.5339,  3.5222]]],\n",
      "\n",
      "\n",
      "        [[[-0.6126, -0.8678, -0.1531,  0.9188,  0.3573,  0.3063, -1.0720,\n",
      "            0.4084,  2.3481]]]], size=(5, 1, 1, 9), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.05104620382189751,\n",
      "       zero_point=0)), ('sfeb.0.bias', Parameter containing:\n",
      "tensor([0.0650, 0.0450, 0.5241, 0.6084, 0.0782])), ('sfeb.0.scale', tensor(0.0272)), ('sfeb.0.zero_point', tensor(0)), ('sfeb.3.weight', tensor([[[[-0.1940, -0.0617, -0.0970, -0.0088,  0.0088]],\n",
      "\n",
      "         [[ 0.0617,  0.1499,  0.2205,  0.4057,  0.3087]],\n",
      "\n",
      "         [[ 0.0794,  0.1499,  0.0970, -0.1235,  0.0176]],\n",
      "\n",
      "         [[-0.1058, -0.0529,  0.0000,  0.0000, -0.0353]],\n",
      "\n",
      "         [[-0.0882,  0.0088,  0.0088,  0.0529,  0.0882]]],\n",
      "\n",
      "\n",
      "        [[[-0.0617,  0.0000,  0.0706, -0.1147, -0.2470]],\n",
      "\n",
      "         [[-1.1201, -0.8379, -0.5468, -0.4145, -0.3528]],\n",
      "\n",
      "         [[-0.3616, -0.5557, -0.2117, -0.3704, -0.2558]],\n",
      "\n",
      "         [[-0.0265,  0.0000, -0.0176, -0.0265, -0.0176]],\n",
      "\n",
      "         [[ 0.4675,  0.5998,  0.5116,  0.6439,  0.7056]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0529, -0.1676, -0.1235, -0.0882]],\n",
      "\n",
      "         [[ 0.0441,  0.1411,  0.1147,  0.1499,  0.1588]],\n",
      "\n",
      "         [[-0.0353, -0.0088, -0.0882,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0441,  0.0353,  0.0000, -0.0617,  0.0176]],\n",
      "\n",
      "         [[ 0.0000, -0.0088, -0.0706, -0.0265, -0.0088]]],\n",
      "\n",
      "\n",
      "        [[[-0.1323, -0.3704, -0.5821, -0.3087, -0.2646]],\n",
      "\n",
      "         [[-0.2293,  0.0265, -0.2029, -0.0617,  0.2029]],\n",
      "\n",
      "         [[ 0.5733,  0.3263,  0.1676, -0.0441, -0.0970]],\n",
      "\n",
      "         [[-0.1676, -0.1235, -0.0176, -0.0088,  0.0617]],\n",
      "\n",
      "         [[-0.0353,  0.2381,  0.0970,  0.4586,  0.5998]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1676,  0.3704,  0.2470,  0.0617, -0.0088]],\n",
      "\n",
      "         [[-0.3175,  0.0176,  0.3352,  0.4675,  0.4410]],\n",
      "\n",
      "         [[-0.2558, -0.4234, -0.3616, -0.2734, -0.2029]],\n",
      "\n",
      "         [[-0.0353, -0.0529,  0.0353, -0.0265, -0.0441]],\n",
      "\n",
      "         [[ 0.0088, -0.1499, -0.2381, -0.2205, -0.2646]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0088, -0.0706, -0.0441, -0.0882, -0.0970]],\n",
      "\n",
      "         [[-0.0088, -0.0088, -0.0882, -0.1235, -0.1235]],\n",
      "\n",
      "         [[-0.1058, -0.0970,  0.0088,  0.0176, -0.0088]],\n",
      "\n",
      "         [[-0.0088, -0.0529, -0.0353,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0882, -0.0882, -0.0353,  0.0441,  0.1588]]],\n",
      "\n",
      "\n",
      "        [[[-0.2117, -0.1411, -0.0970, -0.1323,  0.0000]],\n",
      "\n",
      "         [[-0.1323, -0.1058, -0.0617,  0.0265,  0.2293]],\n",
      "\n",
      "         [[ 0.0706, -0.0176, -0.0441, -0.0088,  0.0000]],\n",
      "\n",
      "         [[ 0.0353,  0.0088,  0.0088,  0.0265, -0.0882]],\n",
      "\n",
      "         [[-0.0176, -0.0529, -0.0882, -0.1411, -0.1411]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0441,  0.0088, -0.0353, -0.0529, -0.0088]],\n",
      "\n",
      "         [[-0.0353, -0.0176,  0.0265,  0.0265, -0.0088]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0176, -0.0441, -0.0088]],\n",
      "\n",
      "         [[ 0.1058,  0.1147, -0.0353, -0.1058, -0.0617]],\n",
      "\n",
      "         [[ 0.0000,  0.0176, -0.0176,  0.0000,  0.0265]]],\n",
      "\n",
      "\n",
      "        [[[-0.2911, -0.0794, -0.1058,  0.2117,  0.1499]],\n",
      "\n",
      "         [[-0.0441,  0.0706, -0.0882, -0.1058, -0.0353]],\n",
      "\n",
      "         [[ 0.0088,  0.0970,  0.1499,  0.1676, -0.1499]],\n",
      "\n",
      "         [[-0.5292, -0.4763,  0.1588,  0.5027,  0.1588]],\n",
      "\n",
      "         [[-0.0088,  0.0441,  0.0970, -0.0088, -0.0088]]],\n",
      "\n",
      "\n",
      "        [[[-0.0176,  0.0265, -0.1940,  0.0617, -0.1235]],\n",
      "\n",
      "         [[-0.0882,  0.1147, -0.2734,  0.1852, -0.0529]],\n",
      "\n",
      "         [[ 0.1323, -0.3352,  0.2205, -0.1058, -0.1058]],\n",
      "\n",
      "         [[ 0.4498, -1.0760,  0.6174,  0.0176, -0.3352]],\n",
      "\n",
      "         [[-0.0176, -0.1940,  0.2734, -0.2293,  0.1499]]],\n",
      "\n",
      "\n",
      "        [[[-0.0706, -0.1058,  0.1058, -0.1499,  0.0882]],\n",
      "\n",
      "         [[-0.0882,  0.0176,  0.0176, -0.0441,  0.0265]],\n",
      "\n",
      "         [[-0.0088, -0.0794, -0.0441,  0.0176,  0.0265]],\n",
      "\n",
      "         [[-0.3087,  0.4675, -0.4057,  0.4763, -0.3352]],\n",
      "\n",
      "         [[ 0.0441, -0.1323, -0.0088, -0.0176, -0.0176]]],\n",
      "\n",
      "\n",
      "        [[[-0.1235,  0.1499, -0.2117,  0.2734, -0.0970]],\n",
      "\n",
      "         [[-0.0088, -0.0529,  0.1411, -0.2293,  0.1411]],\n",
      "\n",
      "         [[-0.1411,  0.2822, -0.3969,  0.3352, -0.2029]],\n",
      "\n",
      "         [[ 0.0529, -0.1147,  0.0088,  0.0000, -0.0794]],\n",
      "\n",
      "         [[-0.0882,  0.0970, -0.2205,  0.2470, -0.0970]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1235, -0.2911,  0.2117, -0.1411,  0.0088]],\n",
      "\n",
      "         [[ 0.0176,  0.1676, -0.2029,  0.1676, -0.1235]],\n",
      "\n",
      "         [[ 0.1411, -0.4586,  0.2999, -0.4410,  0.2117]],\n",
      "\n",
      "         [[-0.1147, -0.0265, -0.0088, -0.0088,  0.0353]],\n",
      "\n",
      "         [[ 0.1588, -0.2293,  0.1411, -0.2558,  0.2470]]],\n",
      "\n",
      "\n",
      "        [[[-0.0441,  0.1499, -0.1323, -0.0706,  0.1323]],\n",
      "\n",
      "         [[-0.0794, -0.1499, -0.0353,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.1852,  0.0529, -0.3175,  0.0088,  0.1235]],\n",
      "\n",
      "         [[-0.1852, -0.1499, -0.1499, -0.1940, -0.1499]],\n",
      "\n",
      "         [[-0.0176,  0.0441, -0.0176, -0.0882, -0.0088]]],\n",
      "\n",
      "\n",
      "        [[[-0.0617,  0.0176,  0.0617, -0.1147,  0.0529]],\n",
      "\n",
      "         [[ 0.0176,  0.0000, -0.0706, -0.0529, -0.0088]],\n",
      "\n",
      "         [[-0.0882, -0.1676, -0.1499, -0.0176,  0.0970]],\n",
      "\n",
      "         [[-0.0882,  0.0617, -0.1411, -0.0970, -0.2117]],\n",
      "\n",
      "         [[-0.1499, -0.0088,  0.0970, -0.0794, -0.1411]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1411, -0.0529, -0.2029,  0.0706,  0.4145]],\n",
      "\n",
      "         [[ 0.0617, -0.1588,  0.0088,  0.0353,  0.2029]],\n",
      "\n",
      "         [[-0.3175, -0.1940, -0.1147,  0.3352, -0.2117]],\n",
      "\n",
      "         [[ 0.0000,  0.0353, -0.1940, -0.1058, -0.0353]],\n",
      "\n",
      "         [[-0.1588, -0.0353, -0.1323, -0.1411,  0.0088]]],\n",
      "\n",
      "\n",
      "        [[[-0.2999,  0.0000,  0.2293,  0.0000, -0.1940]],\n",
      "\n",
      "         [[-0.0529,  0.1058,  0.1499, -0.0529, -0.0882]],\n",
      "\n",
      "         [[-0.2381,  0.7056,  0.0617, -0.4234, -0.1058]],\n",
      "\n",
      "         [[ 0.0353, -0.1764, -0.0970,  0.1147, -0.1147]],\n",
      "\n",
      "         [[-0.0176, -0.1676,  0.0088, -0.0265,  0.1058]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1499, -0.0176, -0.1323, -0.0882, -0.0882]],\n",
      "\n",
      "         [[ 0.0000,  0.0882,  0.0088,  0.0088, -0.0088]],\n",
      "\n",
      "         [[-0.0617, -0.0794, -0.0794,  0.0529,  0.0794]],\n",
      "\n",
      "         [[ 0.0176,  0.0706,  0.0088, -0.0265, -0.1058]],\n",
      "\n",
      "         [[-0.0176, -0.0706, -0.0088,  0.1147,  0.1058]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0088,  0.0000, -0.1147, -0.1058, -0.0265]],\n",
      "\n",
      "         [[-0.0617,  0.0088,  0.0441,  0.1058,  0.1676]],\n",
      "\n",
      "         [[-0.4057, -0.0353,  0.0617,  0.3793, -0.1499]],\n",
      "\n",
      "         [[ 0.1588,  0.0353, -0.0706, -0.1323, -0.2117]],\n",
      "\n",
      "         [[-0.0441,  0.0265,  0.0882,  0.0441, -0.0970]]],\n",
      "\n",
      "\n",
      "        [[[-0.0794, -0.0970, -0.1940, -0.1235,  0.0000]],\n",
      "\n",
      "         [[-0.1764, -0.1588, -0.0088, -0.0088,  0.0882]],\n",
      "\n",
      "         [[ 0.2911,  0.0353,  0.2029, -0.1147, -0.1058]],\n",
      "\n",
      "         [[-0.0441,  0.0000, -0.1058, -0.0441, -0.1058]],\n",
      "\n",
      "         [[-0.0706, -0.0794, -0.0441, -0.0970,  0.0088]]],\n",
      "\n",
      "\n",
      "        [[[-0.0265,  0.0353, -0.2029, -0.1411, -0.1058]],\n",
      "\n",
      "         [[-0.0441, -0.0617, -0.0353,  0.0000,  0.0353]],\n",
      "\n",
      "         [[-0.2029, -0.1676, -0.0882, -0.0441,  0.0706]],\n",
      "\n",
      "         [[-0.0353,  0.0000,  0.0441, -0.0441,  0.0176]],\n",
      "\n",
      "         [[-0.0353, -0.0882,  0.0088,  0.0970,  0.0882]]],\n",
      "\n",
      "\n",
      "        [[[-0.0970,  0.2999,  0.1852, -0.2646, -0.4498]],\n",
      "\n",
      "         [[-0.1588,  0.1676,  0.1058,  0.0176, -0.0529]],\n",
      "\n",
      "         [[ 0.0441, -0.3263, -0.0441, -0.1058,  0.0176]],\n",
      "\n",
      "         [[-0.1235, -0.0088,  0.0882,  0.0441, -0.0441]],\n",
      "\n",
      "         [[-0.3087, -0.2558, -0.1411,  0.0970,  0.3793]]],\n",
      "\n",
      "\n",
      "        [[[-0.2117, -0.2381, -0.1058, -0.1411, -0.3352]],\n",
      "\n",
      "         [[-0.0529, -0.0794, -0.1323,  0.0617, -0.0441]],\n",
      "\n",
      "         [[ 0.0794,  0.0000, -0.1235,  0.0353, -0.1058]],\n",
      "\n",
      "         [[-0.0529,  0.0088, -0.0970, -0.0088,  0.0706]],\n",
      "\n",
      "         [[ 0.0088,  0.0617, -0.0617,  0.0088,  0.1676]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3616,  0.1852,  0.1147, -0.3704, -0.3793]],\n",
      "\n",
      "         [[ 0.0176,  0.2381,  0.0970,  0.1764, -0.2205]],\n",
      "\n",
      "         [[-0.1764, -0.1852,  0.0617, -0.0176,  0.0706]],\n",
      "\n",
      "         [[-0.0265,  0.0441, -0.0176,  0.0000,  0.0617]],\n",
      "\n",
      "         [[-0.0265, -0.0970,  0.1235, -0.0176, -0.0706]]],\n",
      "\n",
      "\n",
      "        [[[-0.1676, -0.1940, -0.2205,  0.0265,  0.3881]],\n",
      "\n",
      "         [[ 0.0176, -0.1235, -0.0441,  0.0176, -0.0265]],\n",
      "\n",
      "         [[ 0.2822,  0.7232,  0.0970, -0.0794, -0.0265]],\n",
      "\n",
      "         [[ 0.0353, -0.1058, -0.0706, -0.0088, -0.0617]],\n",
      "\n",
      "         [[ 0.1235,  0.5292, -0.0441, -0.1323, -0.2999]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2558, -0.0265, -0.0970, -0.4675, -0.3704]],\n",
      "\n",
      "         [[ 0.3087,  0.3087,  0.0706, -0.0441, -0.3881]],\n",
      "\n",
      "         [[-0.4145, -0.2029, -0.0970,  0.2911,  0.0706]],\n",
      "\n",
      "         [[ 0.0088,  0.0000,  0.0088,  0.0176, -0.0529]],\n",
      "\n",
      "         [[-0.1940, -0.3263,  0.1676,  0.0706,  0.4322]]],\n",
      "\n",
      "\n",
      "        [[[-0.1147, -0.1147, -0.1411, -0.1499, -0.1411]],\n",
      "\n",
      "         [[-0.0529, -0.0529, -0.0706, -0.0706, -0.0706]],\n",
      "\n",
      "         [[ 0.0970,  0.1499,  0.0441,  0.0088,  0.0176]],\n",
      "\n",
      "         [[-0.0441,  0.0265,  0.0176,  0.0176,  0.0088]],\n",
      "\n",
      "         [[ 0.0000, -0.0529, -0.0529,  0.0088,  0.0970]]],\n",
      "\n",
      "\n",
      "        [[[-0.2558, -0.4234, -0.4322, -0.1852, -0.3263]],\n",
      "\n",
      "         [[ 0.0617,  0.1499, -0.1411, -0.2293, -0.1676]],\n",
      "\n",
      "         [[ 0.2470,  0.4586,  0.3263,  0.1235,  0.0529]],\n",
      "\n",
      "         [[-0.0617,  0.0088,  0.0000, -0.0176, -0.0441]],\n",
      "\n",
      "         [[ 0.1676,  0.1588,  0.0529,  0.3616,  0.5557]]],\n",
      "\n",
      "\n",
      "        [[[-0.2558, -0.1852, -0.1764, -0.0970, -0.1323]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0353,  0.1764,  0.2381]],\n",
      "\n",
      "         [[ 0.0529,  0.0617,  0.0353,  0.0617, -0.0794]],\n",
      "\n",
      "         [[ 0.1058,  0.0353, -0.0617,  0.0000, -0.0529]],\n",
      "\n",
      "         [[-0.0794, -0.1852, -0.1323, -0.1323, -0.1940]]],\n",
      "\n",
      "\n",
      "        [[[-0.1852, -0.2734, -0.1940, -0.1764, -0.1323]],\n",
      "\n",
      "         [[-0.0441, -0.0088, -0.1058,  0.0000, -0.1499]],\n",
      "\n",
      "         [[ 0.0441, -0.1323, -0.1676, -0.1852, -0.0794]],\n",
      "\n",
      "         [[-0.0353,  0.0088,  0.0441, -0.1852,  0.0617]],\n",
      "\n",
      "         [[ 0.2205,  0.2029,  0.0706,  0.1676,  0.2646]]],\n",
      "\n",
      "\n",
      "        [[[-0.1588, -0.2470, -0.3087, -0.1323, -0.1323]],\n",
      "\n",
      "         [[ 0.0088,  0.0000, -0.1411, -0.1588, -0.2117]],\n",
      "\n",
      "         [[ 0.4763,  0.0529,  0.1588, -0.0617,  0.0882]],\n",
      "\n",
      "         [[ 0.0088, -0.1323, -0.0088, -0.0617,  0.0000]],\n",
      "\n",
      "         [[-0.0882, -0.1588, -0.0882, -0.1588,  0.1676]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0441, -0.0970, -0.0617,  0.0000, -0.1411]],\n",
      "\n",
      "         [[-0.0088,  0.0882, -0.1499, -0.1588,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0794,  0.1676,  0.0000,  0.0882]],\n",
      "\n",
      "         [[ 0.0000, -0.0617,  0.0000, -0.0882,  0.0000]]]], size=(32, 5, 1, 5),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.008819940499961376, zero_point=0)), ('sfeb.3.bias', Parameter containing:\n",
      "tensor([-0.1801,  1.0522,  0.0839, -0.2480,  0.7322,  0.2349,  0.1141,  0.0984,\n",
      "        -0.0870,  0.3796,  0.1600,  0.1930,  0.2420,  0.5375,  0.5880,  0.4843,\n",
      "         0.2208,  0.0906,  0.1847,  0.1691,  0.3118,  0.2928,  0.2060,  0.1351,\n",
      "        -0.3605,  0.2974, -0.0747, -0.4979,  0.1120,  0.3800,  0.0029,  0.1441])), ('sfeb.3.scale', tensor(0.0292)), ('sfeb.3.zero_point', tensor(0)), ('tfeb.0.weight', tensor([[[[-0.0052, -0.0052, -0.0105],\n",
      "          [-0.0157, -0.0105, -0.0157],\n",
      "          [-0.0052,  0.0837, -0.0052]]],\n",
      "\n",
      "\n",
      "        [[[-0.4079,  0.0837,  0.1621],\n",
      "          [-0.4654,  0.0994,  0.3137],\n",
      "          [-0.1882,  0.0523,  0.1255]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0052,  0.0052,  0.0000],\n",
      "          [-0.0052, -0.0052, -0.0052],\n",
      "          [ 0.0314,  0.0575,  0.0366]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2249,  0.1307,  0.0889],\n",
      "          [-0.0627, -0.1987, -0.0941],\n",
      "          [-0.2562, -0.1673, -0.0052]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0366, -0.0052],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0314,  0.0471,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0261,  0.0941,  0.0680],\n",
      "          [-0.0575,  0.2249,  0.0157],\n",
      "          [-0.0627,  0.0994,  0.0052]]],\n",
      "\n",
      "\n",
      "        [[[-0.0575, -0.0471, -0.0732],\n",
      "          [ 0.0209, -0.0471,  0.0941],\n",
      "          [-0.0471, -0.0627, -0.0366]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4131,  0.0000, -0.6693],\n",
      "          [ 0.5072,  0.0052, -0.5752],\n",
      "          [-0.5700,  0.1098,  0.3713]]],\n",
      "\n",
      "\n",
      "        [[[-0.0784, -0.0575,  0.1412],\n",
      "          [ 0.0052,  0.0000,  0.0052],\n",
      "          [ 0.0052,  0.0000,  0.0052]]]], size=(9, 1, 3, 3), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.005229143891483545,\n",
      "       zero_point=0)), ('tfeb.0.bias', Parameter containing:\n",
      "tensor([-0.0602,  0.2706, -0.1182,  0.1396, -0.1268,  0.0042,  0.2444,  0.1174,\n",
      "        -0.0382])), ('tfeb.0.scale', tensor(0.0139)), ('tfeb.0.zero_point', tensor(0)), ('tfeb.4.weight', tensor([[[[-0.0052, -0.0206, -0.0155],\n",
      "          [-0.0052, -0.0206, -0.0103],\n",
      "          [-0.0103, -0.0155, -0.0155]],\n",
      "\n",
      "         [[-0.0877,  0.1186, -0.0774],\n",
      "          [-0.0670,  0.1960, -0.3352],\n",
      "          [-0.3352,  0.0928, -0.0722]],\n",
      "\n",
      "         [[-0.0052, -0.0206, -0.0206],\n",
      "          [-0.0103, -0.0206, -0.0155],\n",
      "          [-0.0155, -0.0206, -0.0155]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0877, -0.1341,  0.0052],\n",
      "          [-0.0670, -0.2217, -0.0464],\n",
      "          [ 0.0825,  0.0000,  0.1289]],\n",
      "\n",
      "         [[-0.0206, -0.0928, -0.3352],\n",
      "          [-0.0774, -0.1753,  0.0670],\n",
      "          [-0.1856, -0.0670, -0.1908]],\n",
      "\n",
      "         [[-0.0206, -0.0464, -0.0155],\n",
      "          [-0.0155, -0.0567, -0.0103],\n",
      "          [-0.0052, -0.0361,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0103,  0.0103,  0.0052],\n",
      "          [ 0.0052,  0.0052,  0.0052],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0052,  0.0052, -0.0567],\n",
      "          [ 0.1495,  0.1341,  0.0000],\n",
      "          [ 0.0000,  0.0052,  0.0000]],\n",
      "\n",
      "         [[ 0.0155,  0.0155,  0.0155],\n",
      "          [ 0.0103,  0.0103,  0.0103],\n",
      "          [ 0.0052,  0.0052,  0.0052]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0774,  0.0825,  0.0825],\n",
      "          [ 0.0980,  0.1134,  0.0928],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0670],\n",
      "          [ 0.0000,  0.0052,  0.0052],\n",
      "          [ 0.0000,  0.0052,  0.0103]],\n",
      "\n",
      "         [[ 0.0000,  0.0052,  0.0000],\n",
      "          [ 0.0052,  0.0052,  0.0000],\n",
      "          [ 0.0000,  0.0052,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0052,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0052,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0052]],\n",
      "\n",
      "         [[-0.6601, -0.0052,  0.2888],\n",
      "          [-0.2424, -0.0877, -0.1341],\n",
      "          [ 0.3455, -0.4641,  0.2475]],\n",
      "\n",
      "         [[-0.0052,  0.0980, -0.0052],\n",
      "          [-0.0052, -0.0103,  0.0000],\n",
      "          [ 0.0000, -0.0052,  0.0052]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1805,  0.3249, -0.0103],\n",
      "          [ 0.0000,  0.0155, -0.1599],\n",
      "          [-0.0155,  0.2063,  0.2063]],\n",
      "\n",
      "         [[ 0.0774,  0.1289, -0.0052],\n",
      "          [ 0.4177, -0.3816, -0.1444],\n",
      "          [-0.1186,  0.4332,  0.0206]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0103],\n",
      "          [ 0.0000,  0.0000,  0.0052],\n",
      "          [-0.0052,  0.0000,  0.0103]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0052,  0.0052,  0.0052],\n",
      "          [ 0.0052,  0.0103,  0.0052],\n",
      "          [ 0.0052,  0.0052,  0.0000]],\n",
      "\n",
      "         [[ 0.0155,  0.0103, -0.1392],\n",
      "          [ 0.0206,  0.0206, -0.1031],\n",
      "          [ 0.0155,  0.0052,  0.0052]],\n",
      "\n",
      "         [[ 0.0103,  0.0103,  0.0052],\n",
      "          [ 0.0103,  0.0103,  0.0103],\n",
      "          [ 0.0052,  0.0052,  0.0052]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0774,  0.1031,  0.0774],\n",
      "          [ 0.1083,  0.1289,  0.0980],\n",
      "          [ 0.0103,  0.0980,  0.0052]],\n",
      "\n",
      "         [[ 0.0103,  0.0206,  0.0052],\n",
      "          [ 0.0103,  0.0155,  0.0000],\n",
      "          [ 0.0052,  0.0103,  0.0052]],\n",
      "\n",
      "         [[ 0.0052,  0.0052,  0.0000],\n",
      "          [ 0.0052,  0.0052,  0.0000],\n",
      "          [ 0.0052,  0.0052,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0052],\n",
      "          [ 0.0052,  0.0000,  0.0052],\n",
      "          [ 0.0000,  0.0000,  0.0052]],\n",
      "\n",
      "         [[-0.3868, -0.1650, -0.2063],\n",
      "          [-0.1083, -0.1495, -0.0258],\n",
      "          [ 0.0980, -0.0619,  0.1134]],\n",
      "\n",
      "         [[ 0.0052,  0.0052,  0.0052],\n",
      "          [ 0.0103,  0.0052,  0.0103],\n",
      "          [ 0.0052,  0.0000,  0.0052]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0206, -0.0258,  0.0722],\n",
      "          [-0.0103, -0.0052,  0.0000],\n",
      "          [-0.0103, -0.0103,  0.0722]],\n",
      "\n",
      "         [[-0.0413, -0.3094,  0.0980],\n",
      "          [-0.1650, -0.0155,  0.0052],\n",
      "          [-0.0258, -0.0052,  0.1908]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0103,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0103, -0.0103, -0.0103],\n",
      "          [-0.0052, -0.0052, -0.0052],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0309, -0.0361, -0.0258],\n",
      "          [-0.0516,  0.0206, -0.0361],\n",
      "          [-0.0309,  0.0361, -0.0206]],\n",
      "\n",
      "         [[-0.0155, -0.0155, -0.0155],\n",
      "          [-0.0103, -0.0103, -0.0103],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0052,  0.0052,  0.0052],\n",
      "          [ 0.0103, -0.1031,  0.0103],\n",
      "          [ 0.0206,  0.0206,  0.0206]],\n",
      "\n",
      "         [[-0.0103, -0.0206, -0.0052],\n",
      "          [ 0.0052, -0.0206,  0.0052],\n",
      "          [-0.0052, -0.0103, -0.0103]],\n",
      "\n",
      "         [[-0.0052, -0.0103, -0.0052],\n",
      "          [-0.0103, -0.0103, -0.0052],\n",
      "          [-0.0052, -0.0103, -0.0052]]]], size=(16, 9, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.005156795959919691, zero_point=0)), ('tfeb.4.bias', Parameter containing:\n",
      "tensor([ 0.2976, -0.3875,  0.2386,  0.1832,  0.2653,  0.2009,  0.2595,  0.0246,\n",
      "         0.1733,  0.0153,  0.1736,  0.1417,  0.4039, -0.4383,  0.3032,  0.0782])), ('tfeb.4.scale', tensor(0.0095)), ('tfeb.4.zero_point', tensor(0)), ('tfeb.7.weight', tensor([[[[ 0.0067,  0.0067,  0.0665],\n",
      "          [-0.1729,  0.0599,  0.2461],\n",
      "          [-0.1330,  0.0266,  0.0200]],\n",
      "\n",
      "         [[ 0.0067,  0.0000,  0.0133],\n",
      "          [ 0.0000,  0.0133,  0.0466],\n",
      "          [-0.0133,  0.0067,  0.0200]],\n",
      "\n",
      "         [[ 0.0333,  0.0333,  0.0333],\n",
      "          [ 0.0200,  0.0399,  0.1330],\n",
      "          [ 0.0333, -0.1397,  0.0200]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0266, -0.0133, -0.0133],\n",
      "          [-0.0200, -0.0067, -0.0067],\n",
      "          [-0.0067, -0.0067, -0.0067]],\n",
      "\n",
      "         [[ 0.2461,  0.1264,  0.0067],\n",
      "          [ 0.2328,  0.1330,  0.1663],\n",
      "          [ 0.0067,  0.0000,  0.0067]],\n",
      "\n",
      "         [[-0.0266, -0.0133,  0.0133],\n",
      "          [-0.0333, -0.0067,  0.0067],\n",
      "          [-0.0466, -0.0333, -0.0266]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1131,  0.1397,  0.0000],\n",
      "          [-0.0200, -0.0466,  0.0333],\n",
      "          [ 0.1197, -0.0266, -0.1197]],\n",
      "\n",
      "         [[ 0.0133,  0.0133,  0.0000],\n",
      "          [ 0.0000, -0.0067,  0.0000],\n",
      "          [ 0.0067,  0.0000,  0.0067]],\n",
      "\n",
      "         [[ 0.1862,  0.1596,  0.2394],\n",
      "          [ 0.0333,  0.0000, -0.0067],\n",
      "          [ 0.0266,  0.0399,  0.0133]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0067,  0.0000,  0.0067],\n",
      "          [-0.0133, -0.0067, -0.0067],\n",
      "          [ 0.0067,  0.0067,  0.0133]],\n",
      "\n",
      "         [[-0.0266, -0.0200, -0.0133],\n",
      "          [ 0.1330, -0.0133, -0.0067],\n",
      "          [ 0.0067,  0.0067,  0.0067]],\n",
      "\n",
      "         [[-0.0067,  0.0000,  0.0266],\n",
      "          [-0.0200, -0.0133,  0.0067],\n",
      "          [-0.0133, -0.0067,  0.0133]]],\n",
      "\n",
      "\n",
      "        [[[-0.3325, -0.0399, -0.1264],\n",
      "          [ 0.0133, -0.0466, -0.2594],\n",
      "          [-0.0133,  0.0067,  0.0067]],\n",
      "\n",
      "         [[ 0.0067,  0.0000, -0.1729],\n",
      "          [ 0.0266,  0.0266,  0.0200],\n",
      "          [ 0.0133,  0.0200,  0.0067]],\n",
      "\n",
      "         [[-0.1530, -0.0133,  0.0133],\n",
      "          [-0.1596, -0.5719, -0.1995],\n",
      "          [-0.0266,  0.3724,  0.2926]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0399,  0.0466,  0.0399],\n",
      "          [ 0.0200,  0.0266,  0.0133],\n",
      "          [ 0.0133,  0.0266,  0.0133]],\n",
      "\n",
      "         [[ 0.0266,  0.0200,  0.0333],\n",
      "          [-0.0067, -0.0266, -0.0266],\n",
      "          [-0.0133, -0.2261, -0.3059]],\n",
      "\n",
      "         [[ 0.0399,  0.0399,  0.0333],\n",
      "          [ 0.0133,  0.0133,  0.0200],\n",
      "          [ 0.0067,  0.0000,  0.0133]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.2328,  0.0732,  0.0466],\n",
      "          [ 0.0466,  0.0798, -0.4123],\n",
      "          [-0.1862,  0.0865, -0.0333]],\n",
      "\n",
      "         [[-0.0133,  0.0000,  0.0133],\n",
      "          [ 0.0200,  0.0133,  0.0200],\n",
      "          [ 0.0000,  0.0200,  0.0266]],\n",
      "\n",
      "         [[-0.1929,  0.0067,  0.1330],\n",
      "          [-0.0865, -0.0266, -0.3924],\n",
      "          [-0.2461, -0.0200,  0.3259]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0133,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0200],\n",
      "          [-0.1729, -0.0067,  0.0000]],\n",
      "\n",
      "         [[-0.0067, -0.0133, -0.0399],\n",
      "          [-0.0200, -0.0133, -0.0266],\n",
      "          [-0.0266,  0.1131, -0.0266]],\n",
      "\n",
      "         [[-0.0200, -0.0067,  0.0067],\n",
      "          [-0.0200,  0.0133,  0.0067],\n",
      "          [-0.0067,  0.0067,  0.0200]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0067,  0.2793,  0.0466],\n",
      "          [ 0.0599, -0.0333,  0.0732],\n",
      "          [-0.0399,  0.2261,  0.0266]],\n",
      "\n",
      "         [[-0.0399, -0.0333, -0.0266],\n",
      "          [-0.0266, -0.0200, -0.0067],\n",
      "          [-0.0266, -0.0133, -0.0067]],\n",
      "\n",
      "         [[-0.0266,  0.0998, -0.0067],\n",
      "          [-0.1264, -0.3791, -0.0200],\n",
      "          [ 0.1330,  0.3791,  0.1197]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -0.1197, -0.1131],\n",
      "          [-0.0133, -0.1463, -0.1796],\n",
      "          [ 0.0000,  0.0067, -0.1131]],\n",
      "\n",
      "         [[-0.0266, -0.0067,  0.0067],\n",
      "          [ 0.0133,  0.1463,  0.0466],\n",
      "          [ 0.0067, -0.1929, -0.1463]],\n",
      "\n",
      "         [[ 0.0333,  0.0532,  0.1862],\n",
      "          [ 0.0333,  0.0532,  0.0532],\n",
      "          [ 0.0200,  0.0266,  0.0266]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0466,  0.1530,  0.0333],\n",
      "          [ 0.0665,  0.2660,  0.0200],\n",
      "          [-0.3259, -0.2926, -0.3392]],\n",
      "\n",
      "         [[ 0.0466,  0.0133,  0.0067],\n",
      "          [ 0.0599,  0.0466,  0.0266],\n",
      "          [ 0.0466,  0.0200,  0.0133]],\n",
      "\n",
      "         [[-0.0067, -0.2062,  0.0998],\n",
      "          [ 0.4655,  0.4323,  0.3392],\n",
      "          [-0.4655, -0.5254, -0.1064]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0200,  0.0200, -0.1064],\n",
      "          [ 0.0399,  0.0333,  0.0333],\n",
      "          [ 0.0200,  0.0133,  0.0067]],\n",
      "\n",
      "         [[ 0.0333,  0.2128,  0.2328],\n",
      "          [-0.1397,  0.0067,  0.0067],\n",
      "          [-0.0067, -0.0067,  0.0000]],\n",
      "\n",
      "         [[ 0.0466,  0.0333,  0.0200],\n",
      "          [ 0.0532,  0.0399,  0.0266],\n",
      "          [ 0.0399,  0.0266,  0.0133]]]], size=(23, 16, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.006650029681622982, zero_point=0)), ('tfeb.7.bias', Parameter containing:\n",
      "tensor([-0.4302, -0.0775,  0.1125,  0.0767,  0.0521,  0.0788,  0.0033,  0.0269,\n",
      "         0.0078,  0.1253,  0.0768,  0.0264,  0.0329,  0.1374, -0.1450, -0.0068,\n",
      "         0.2700, -0.1006,  0.2737, -0.1264,  0.2207, -0.0409,  0.0098])), ('tfeb.7.scale', tensor(0.0072)), ('tfeb.7.zero_point', tensor(0)), ('tfeb.11.weight', tensor([[[[ 0.0173,  0.0087,  0.0087],\n",
      "          [ 0.0043,  0.0043,  0.0043],\n",
      "          [ 0.0130,  0.0043,  0.0087]],\n",
      "\n",
      "         [[-0.0217, -0.1126, -0.2383],\n",
      "          [-0.0347, -0.0087, -0.1906],\n",
      "          [-0.0433, -0.0173, -0.0173]],\n",
      "\n",
      "         [[ 0.0000, -0.1256,  0.0043],\n",
      "          [-0.1776, -0.1820,  0.1603],\n",
      "          [ 0.0043,  0.0043,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0043,  0.0130, -0.0217],\n",
      "          [-0.0130,  0.1430, -0.0217],\n",
      "          [-0.0130,  0.0087, -0.1256]],\n",
      "\n",
      "         [[-0.0260,  0.0043,  0.1170],\n",
      "          [-0.0390, -0.0173,  0.0996],\n",
      "          [-0.0477,  0.1170,  0.0823]],\n",
      "\n",
      "         [[ 0.0000, -0.1300, -0.0130],\n",
      "          [-0.1343, -0.1170,  0.0000],\n",
      "          [-0.0217, -0.0087, -0.0087]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0087,  0.0173,  0.0130],\n",
      "          [ 0.0087,  0.0217,  0.0260],\n",
      "          [ 0.0043,  0.0087,  0.0087]],\n",
      "\n",
      "         [[-0.0043, -0.0173,  0.0173],\n",
      "          [-0.0173, -0.0217,  0.0173],\n",
      "          [-0.0087, -0.0043,  0.0173]],\n",
      "\n",
      "         [[ 0.0130, -0.1300, -0.1516],\n",
      "          [ 0.0043,  0.0130, -0.0087],\n",
      "          [-0.0260,  0.1690, -0.0303]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.1603],\n",
      "          [ 0.0000,  0.0043,  0.1473],\n",
      "          [-0.0130, -0.0130,  0.0043]],\n",
      "\n",
      "         [[ 0.0000, -0.0087,  0.0043],\n",
      "          [-0.0043, -0.1256, -0.1040],\n",
      "          [-0.1256, -0.1213, -0.0953]],\n",
      "\n",
      "         [[-0.0087,  0.0000, -0.0087],\n",
      "          [ 0.0087,  0.0217,  0.0217],\n",
      "          [-0.1386, -0.0130, -0.0087]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0087,  0.0130,  0.0000],\n",
      "          [ 0.0087,  0.0130,  0.0043],\n",
      "          [-0.0823,  0.0087,  0.0043]],\n",
      "\n",
      "         [[-0.0087, -0.0087, -0.1083],\n",
      "          [-0.0173, -0.0217, -0.1776],\n",
      "          [-0.1083, -0.1560, -0.1560]],\n",
      "\n",
      "         [[ 0.0953,  0.0087, -0.1863],\n",
      "          [ 0.0260, -0.1516, -0.2599],\n",
      "          [ 0.0000,  0.0000,  0.1040]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0130, -0.1256, -0.0087],\n",
      "          [-0.0043, -0.0043,  0.0000],\n",
      "          [-0.0087, -0.0043,  0.0823]],\n",
      "\n",
      "         [[ 0.0130,  0.0780, -0.0866],\n",
      "          [ 0.0043, -0.0087, -0.1256],\n",
      "          [-0.0087, -0.0217, -0.0260]],\n",
      "\n",
      "         [[ 0.0996,  0.0000, -0.2296],\n",
      "          [ 0.0087,  0.0043, -0.1170],\n",
      "          [ 0.0087,  0.0087,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0347, -0.0390, -0.0303],\n",
      "          [-0.0173, -0.0260,  0.1170],\n",
      "          [ 0.0043, -0.0087,  0.0910]],\n",
      "\n",
      "         [[-0.0173, -0.0043, -0.0866],\n",
      "          [-0.0173, -0.0043, -0.0996],\n",
      "          [-0.0087, -0.0087, -0.0866]],\n",
      "\n",
      "         [[ 0.1040,  0.1300, -0.1386],\n",
      "          [-0.0260,  0.1170, -0.1430],\n",
      "          [ 0.0043, -0.0043,  0.0043]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0910, -0.0043, -0.0087],\n",
      "          [-0.0866,  0.1126, -0.0866],\n",
      "          [-0.0173,  0.1603, -0.0043]],\n",
      "\n",
      "         [[-0.0217,  0.0000,  0.0043],\n",
      "          [ 0.0000,  0.0217,  0.0260],\n",
      "          [ 0.0087,  0.0173,  0.0130]],\n",
      "\n",
      "         [[ 0.0390, -0.0563, -0.0347],\n",
      "          [ 0.1126, -0.0347, -0.0130],\n",
      "          [-0.0043, -0.1126, -0.1170]]],\n",
      "\n",
      "\n",
      "        [[[-0.0043, -0.0043,  0.0000],\n",
      "          [ 0.0130,  0.0217,  0.0217],\n",
      "          [ 0.0000,  0.0043,  0.0043]],\n",
      "\n",
      "         [[ 0.0000, -0.0607, -0.0737],\n",
      "          [ 0.0000, -0.0563, -0.0823],\n",
      "          [ 0.0000,  0.0000, -0.0520]],\n",
      "\n",
      "         [[-0.0130, -0.0043,  0.0000],\n",
      "          [ 0.0000,  0.0043,  0.0087],\n",
      "          [ 0.0000,  0.0043,  0.0043]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0087,  0.0000, -0.0043],\n",
      "          [-0.0043, -0.0607, -0.0043],\n",
      "          [ 0.0000,  0.0043,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0043],\n",
      "          [ 0.0866,  0.0737,  0.0823],\n",
      "          [ 0.0043,  0.0043,  0.0087]],\n",
      "\n",
      "         [[-0.0477, -0.0477,  0.0043],\n",
      "          [-0.0043, -0.0043, -0.0043],\n",
      "          [-0.0390,  0.0043,  0.0043]]],\n",
      "\n",
      "\n",
      "        [[[-0.0087, -0.0130, -0.2123],\n",
      "          [-0.0087, -0.0260, -0.1906],\n",
      "          [-0.0043, -0.0130, -0.0087]],\n",
      "\n",
      "         [[-0.1386, -0.1170, -0.1170],\n",
      "          [ 0.0130, -0.1300, -0.1300],\n",
      "          [ 0.0260,  0.0087,  0.0043]],\n",
      "\n",
      "         [[ 0.0173,  0.0217,  0.0000],\n",
      "          [ 0.0173,  0.0217, -0.1386],\n",
      "          [ 0.0260,  0.0173,  0.0043]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0303,  0.1300,  0.0260],\n",
      "          [ 0.0390,  0.0173,  0.0260],\n",
      "          [ 0.0303,  0.0130,  0.0260]],\n",
      "\n",
      "         [[-0.0043, -0.0043, -0.0043],\n",
      "          [ 0.0173,  0.0130,  0.0130],\n",
      "          [ 0.0130,  0.0087,  0.0043]],\n",
      "\n",
      "         [[ 0.0173,  0.0217,  0.0260],\n",
      "          [ 0.0087,  0.0173,  0.0130],\n",
      "          [ 0.0260,  0.0217,  0.0260]]]], size=(33, 23, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.004332436248660088, zero_point=0)), ('tfeb.11.bias', Parameter containing:\n",
      "tensor([ 0.0203,  0.1575,  0.3863, -0.0253, -0.0534, -0.2511, -0.4326,  0.0155,\n",
      "         0.1605, -0.0792, -0.3268, -0.0343, -0.1770, -0.0649, -0.2841,  0.1525,\n",
      "         0.0192,  0.1373,  0.1624, -0.1095,  0.1712,  0.0012,  0.0298,  0.0831,\n",
      "        -0.0976,  0.1098,  0.2655,  0.0784, -0.4202,  0.1213,  0.1330,  0.1700,\n",
      "        -0.0411])), ('tfeb.11.scale', tensor(0.0046)), ('tfeb.11.zero_point', tensor(0)), ('tfeb.14.weight', tensor([[[[-0.0287, -0.0215, -0.0215],\n",
      "          [-0.0143, -0.0215,  0.2222],\n",
      "          [-0.0072, -0.0215, -0.0143]],\n",
      "\n",
      "         [[ 0.3369, -0.0072, -0.3440],\n",
      "          [ 0.0215, -0.3154, -0.2079],\n",
      "          [ 0.0358,  0.2652,  0.0502]],\n",
      "\n",
      "         [[ 0.0358, -0.1218, -0.2365],\n",
      "          [ 0.0215,  0.3942,  0.2222],\n",
      "          [ 0.0430,  0.3512,  0.1935]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3440,  0.0215,  0.0215],\n",
      "          [-0.0072,  0.2294, -0.0072],\n",
      "          [-0.0143,  0.0072, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0358, -0.0143],\n",
      "          [-0.0215, -0.0502, -0.0287],\n",
      "          [ 0.0143, -0.0215, -0.0143]],\n",
      "\n",
      "         [[ 0.0287,  0.0358,  0.0143],\n",
      "          [-0.0072,  0.1648,  0.0143],\n",
      "          [ 0.0143,  0.0358,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0143,  0.1003,  0.1075],\n",
      "          [ 0.2580,  0.2939,  0.1648],\n",
      "          [ 0.3225,  0.3154, -0.0143]],\n",
      "\n",
      "         [[-0.0072, -0.0072,  0.1290],\n",
      "          [ 0.0000, -0.1290,  0.0000],\n",
      "          [-0.0358, -0.0358, -0.0358]],\n",
      "\n",
      "         [[ 0.0000,  0.0143,  0.1433],\n",
      "          [-0.0143,  0.0072,  0.0143],\n",
      "          [-0.1792,  0.0000,  0.0072]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0358,  0.0215, -0.0932],\n",
      "          [-0.1147,  0.1864,  0.0143],\n",
      "          [ 0.0287,  0.2795,  0.0287]],\n",
      "\n",
      "         [[ 0.0143,  0.0215,  0.0287],\n",
      "          [-0.1290,  0.0072,  0.0287],\n",
      "          [-0.0072,  0.0000, -0.1218]],\n",
      "\n",
      "         [[ 0.0000,  0.0072,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.1362],\n",
      "          [-0.0143, -0.0143, -0.1577]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0072,  0.0072],\n",
      "          [ 0.0143,  0.0143,  0.0143],\n",
      "          [ 0.0000,  0.0072,  0.0072]],\n",
      "\n",
      "         [[ 0.0215,  0.0143,  0.1505],\n",
      "          [ 0.0143,  0.0215, -0.0072],\n",
      "          [ 0.1792,  0.0502,  0.0287]],\n",
      "\n",
      "         [[ 0.0287,  0.0072, -0.1505],\n",
      "          [ 0.0215,  0.0000, -0.2365],\n",
      "          [ 0.0215,  0.0000, -0.1648]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1147, -0.0215, -0.3154],\n",
      "          [ 0.0000, -0.0143, -0.2580],\n",
      "          [ 0.1218, -0.0358, -0.2437]],\n",
      "\n",
      "         [[ 0.0143,  0.0143,  0.0000],\n",
      "          [-0.0143, -0.1864, -0.2365],\n",
      "          [-0.0072,  0.0000, -0.1935]],\n",
      "\n",
      "         [[ 0.0215,  0.0143,  0.0143],\n",
      "          [ 0.0143,  0.0143,  0.0072],\n",
      "          [-0.1290,  0.0215,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0072],\n",
      "          [ 0.0000, -0.0072, -0.0072],\n",
      "          [-0.0072, -0.0072,  0.0000]],\n",
      "\n",
      "         [[-0.0072, -0.0072,  0.0000],\n",
      "          [-0.0717,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0788,  0.0143]],\n",
      "\n",
      "         [[ 0.0072,  0.0143,  0.0143],\n",
      "          [-0.0072,  0.0000,  0.0000],\n",
      "          [-0.0645,  0.0072, -0.0932]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0072, -0.1003,  0.0573],\n",
      "          [ 0.1075,  0.0143,  0.1147],\n",
      "          [ 0.0000,  0.0000,  0.0072]],\n",
      "\n",
      "         [[ 0.0215,  0.0215,  0.0215],\n",
      "          [-0.0573,  0.0072,  0.0072],\n",
      "          [-0.1147, -0.1218, -0.1003]],\n",
      "\n",
      "         [[ 0.0072,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0072],\n",
      "          [-0.0143, -0.0287, -0.0287]]],\n",
      "\n",
      "\n",
      "        [[[-0.0072,  0.1218, -0.0072],\n",
      "          [ 0.0000, -0.0072, -0.0072],\n",
      "          [-0.1362,  0.1648, -0.0072]],\n",
      "\n",
      "         [[ 0.0072,  0.0072,  0.0072],\n",
      "          [ 0.0143,  0.0072,  0.0072],\n",
      "          [ 0.1505,  0.0215,  0.0072]],\n",
      "\n",
      "         [[-0.1362, -0.0072,  0.0000],\n",
      "          [-0.0143, -0.0143, -0.0072],\n",
      "          [ 0.0000,  0.0143,  0.0143]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -0.1218,  0.0072],\n",
      "          [ 0.0000,  0.0143,  0.0000],\n",
      "          [-0.0072,  0.0072, -0.0072]],\n",
      "\n",
      "         [[-0.0072,  0.0072,  0.0000],\n",
      "          [-0.0143, -0.0072, -0.0143],\n",
      "          [-0.0143, -0.0072, -0.0072]],\n",
      "\n",
      "         [[ 0.0143,  0.0072,  0.0215],\n",
      "          [ 0.0143,  0.0072,  0.0143],\n",
      "          [ 0.0072,  0.0072,  0.0287]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0072],\n",
      "          [ 0.1720, -0.0072, -0.0072],\n",
      "          [ 0.1792, -0.0072, -0.0072]],\n",
      "\n",
      "         [[-0.2007, -0.0143, -0.0143],\n",
      "          [ 0.0072,  0.0143, -0.1218],\n",
      "          [ 0.0072,  0.2509,  0.0215]],\n",
      "\n",
      "         [[ 0.0072, -0.2150, -0.0215],\n",
      "          [ 0.0072, -0.1433,  0.1505],\n",
      "          [-0.1147, -0.1577, -0.0072]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0143,  0.1218, -0.0072],\n",
      "          [ 0.2365,  0.2724, -0.2079],\n",
      "          [ 0.2437,  0.1792, -0.3727]],\n",
      "\n",
      "         [[-0.1218, -0.1362,  0.0143],\n",
      "          [-0.1577,  0.0358,  0.0502],\n",
      "          [ 0.0143,  0.0143,  0.1505]],\n",
      "\n",
      "         [[ 0.1433,  0.1577,  0.0072],\n",
      "          [-0.0215,  0.1720,  0.0000],\n",
      "          [-0.0143, -0.1792, -0.0072]]]], size=(29, 33, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.007167343515902758, zero_point=0)), ('tfeb.14.bias', Parameter containing:\n",
      "tensor([-0.0882,  0.0065,  0.0135,  0.1001, -0.0825,  0.0905, -0.1186, -0.0254,\n",
      "        -0.0283,  0.1454,  0.0127, -0.0280,  0.1666,  0.0005, -0.0459, -0.0340,\n",
      "        -0.1149,  0.0994,  0.0083,  0.0776, -0.1100, -0.0695,  0.0028, -0.0021,\n",
      "         0.0569, -0.0453, -0.0073,  0.0178, -0.0453])), ('tfeb.14.scale', tensor(0.0069)), ('tfeb.14.zero_point', tensor(0)), ('tfeb.18.weight', tensor([[[[ 0.0000, -0.1207, -0.0695],\n",
      "          [ 0.0000, -0.0585, -0.1463],\n",
      "          [-0.0037,  0.0000, -0.0037]],\n",
      "\n",
      "         [[-0.0914, -0.1134, -0.0878],\n",
      "          [-0.0037, -0.1280, -0.0988],\n",
      "          [ 0.0000, -0.1317,  0.0000]],\n",
      "\n",
      "         [[-0.1536, -0.1207, -0.1061],\n",
      "          [-0.1756, -0.1865, -0.2377],\n",
      "          [-0.1646, -0.1061, -0.1024]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0037,  0.0000,  0.0037],\n",
      "          [-0.0110, -0.0073, -0.0037],\n",
      "          [-0.0219, -0.0183, -0.1134]],\n",
      "\n",
      "         [[ 0.0037,  0.0037,  0.0037],\n",
      "          [ 0.0000, -0.0037,  0.0622],\n",
      "          [-0.0805,  0.0000, -0.0037]],\n",
      "\n",
      "         [[ 0.0037, -0.1426, -0.1975],\n",
      "          [-0.0037,  0.0914, -0.0988],\n",
      "          [ 0.0000,  0.0622, -0.0073]]],\n",
      "\n",
      "\n",
      "        [[[-0.0073, -0.0146, -0.0110],\n",
      "          [ 0.0000, -0.1134, -0.0110],\n",
      "          [-0.0037, -0.1061,  0.0183]],\n",
      "\n",
      "         [[-0.1609,  0.0073, -0.0037],\n",
      "          [-0.1061,  0.0073, -0.1134],\n",
      "          [-0.0037, -0.0037, -0.0110]],\n",
      "\n",
      "         [[ 0.0256,  0.1463, -0.0146],\n",
      "          [-0.0622, -0.0402, -0.0183],\n",
      "          [ 0.0073,  0.0402,  0.0073]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0073,  0.1683,  0.1463],\n",
      "          [-0.0256, -0.0110, -0.0073],\n",
      "          [-0.0073,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0073, -0.0110, -0.0110],\n",
      "          [-0.0037, -0.0110, -0.0037],\n",
      "          [-0.0073, -0.0037, -0.0037]],\n",
      "\n",
      "         [[-0.0037,  0.1317,  0.1463],\n",
      "          [ 0.0000, -0.0146, -0.0146],\n",
      "          [-0.0037, -0.1280,  0.0219]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0805, -0.0073, -0.0110],\n",
      "          [-0.0037, -0.2012, -0.1426],\n",
      "          [ 0.0037, -0.0037, -0.0037]],\n",
      "\n",
      "         [[ 0.0183,  0.0183, -0.0073],\n",
      "          [ 0.1097, -0.0037, -0.0110],\n",
      "          [-0.0073, -0.0110,  0.0768]],\n",
      "\n",
      "         [[-0.0073,  0.0658,  0.1134],\n",
      "          [-0.0914, -0.0183, -0.0293],\n",
      "          [-0.0183, -0.0073,  0.0878]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0146,  0.0073, -0.0037],\n",
      "          [-0.0146, -0.0073, -0.0183],\n",
      "          [-0.0146, -0.0073, -0.0183]],\n",
      "\n",
      "         [[-0.0805, -0.0988,  0.0037],\n",
      "          [-0.0073, -0.0073, -0.0073],\n",
      "          [-0.0073, -0.0073, -0.0073]],\n",
      "\n",
      "         [[-0.1061, -0.0073, -0.0073],\n",
      "          [-0.0914, -0.0073, -0.0037],\n",
      "          [ 0.0037, -0.0073, -0.0110]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0037,  0.0000, -0.0037],\n",
      "          [ 0.0000,  0.0000,  0.1500],\n",
      "          [-0.0037,  0.0000,  0.1207]],\n",
      "\n",
      "         [[ 0.1134,  0.0000, -0.0073],\n",
      "          [-0.0110,  0.0037,  0.1426],\n",
      "          [-0.0037, -0.0073, -0.0110]],\n",
      "\n",
      "         [[-0.1792, -0.1280, -0.0037],\n",
      "          [-0.0914, -0.0073,  0.1061],\n",
      "          [-0.0219,  0.0768,  0.0841]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0110,  0.0073,  0.0110],\n",
      "          [ 0.1463,  0.1463,  0.1244],\n",
      "          [ 0.1463,  0.1865,  0.1390]],\n",
      "\n",
      "         [[-0.0110, -0.0110, -0.0073],\n",
      "          [-0.0037, -0.0037,  0.0000],\n",
      "          [-0.0037, -0.0037,  0.0000]],\n",
      "\n",
      "         [[-0.0073, -0.0073,  0.0000],\n",
      "          [ 0.0951, -0.0073, -0.0073],\n",
      "          [ 0.0000,  0.0000, -0.0073]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0073,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0037,  0.0000],\n",
      "          [ 0.0073,  0.0110,  0.0146]],\n",
      "\n",
      "         [[ 0.0000,  0.0073,  0.0110],\n",
      "          [-0.0256, -0.0146, -0.0183],\n",
      "          [ 0.0110,  0.0146,  0.1646]],\n",
      "\n",
      "         [[ 0.0146,  0.0000,  0.0183],\n",
      "          [ 0.0402,  0.0146,  0.0000],\n",
      "          [ 0.0183, -0.0219, -0.0146]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0073,  0.0037, -0.0146],\n",
      "          [-0.0037,  0.0256,  0.0402],\n",
      "          [ 0.0000,  0.0256,  0.0293]],\n",
      "\n",
      "         [[-0.0037,  0.0000, -0.0110],\n",
      "          [ 0.0000,  0.0000, -0.0037],\n",
      "          [ 0.0073,  0.1390,  0.1865]],\n",
      "\n",
      "         [[ 0.0000, -0.0110,  0.0110],\n",
      "          [-0.0183, -0.0037,  0.0110],\n",
      "          [-0.0146,  0.0073,  0.0037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0073,  0.0037],\n",
      "          [ 0.1975,  0.1426,  0.0073],\n",
      "          [ 0.0000,  0.0037,  0.0183]],\n",
      "\n",
      "         [[ 0.0073,  0.0146, -0.0768],\n",
      "          [-0.0658, -0.0878, -0.0037],\n",
      "          [ 0.0110,  0.0951,  0.1500]],\n",
      "\n",
      "         [[ 0.1573,  0.0110, -0.0183],\n",
      "          [ 0.1244,  0.0183, -0.0110],\n",
      "          [ 0.0293,  0.0878, -0.0183]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.1280, -0.0037],\n",
      "          [-0.1024, -0.0183, -0.0183],\n",
      "          [ 0.0000, -0.0110, -0.0110]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0037],\n",
      "          [ 0.0000,  0.0000, -0.0037],\n",
      "          [ 0.0000,  0.0914,  0.0037]],\n",
      "\n",
      "         [[-0.0037, -0.0037, -0.0695],\n",
      "          [-0.0073,  0.1536,  0.0000],\n",
      "          [-0.0037,  0.0146,  0.0110]]]], size=(56, 29, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.0036576741840690374, zero_point=0)), ('tfeb.18.bias', Parameter containing:\n",
      "tensor([ 0.1867,  0.1142, -0.2867, -0.0374, -0.0878, -0.2766,  0.0307, -0.0065,\n",
      "         0.0278,  0.0386, -0.1893,  0.0728, -0.1148,  0.0826, -0.0646,  0.0950,\n",
      "         0.1172,  0.1180,  0.0986,  0.0193, -0.0069, -0.1915, -0.0107,  0.0393,\n",
      "        -0.2417,  0.0260,  0.1203, -0.0535, -0.0513, -0.0054,  0.2462, -0.0792,\n",
      "        -0.0029, -0.1480,  0.1064,  0.1565,  0.0514, -0.0136, -0.3526, -0.1840,\n",
      "         0.0214, -0.0849, -0.1616,  0.0241,  0.0779, -0.0563, -0.1643, -0.0046,\n",
      "        -0.0644,  0.1763,  0.1828, -0.0191,  0.0048, -0.1920, -0.1437,  0.0010])), ('tfeb.18.scale', tensor(0.0050)), ('tfeb.18.zero_point', tensor(0)), ('tfeb.21.weight', tensor([[[[ 0.0000,  0.0000, -0.1610],\n",
      "          [ 0.0129,  0.0129,  0.0000],\n",
      "          [ 0.0258,  0.0193,  0.0129]],\n",
      "\n",
      "         [[ 0.0064,  0.0193,  0.0258],\n",
      "          [-0.0129,  0.1417, -0.0193],\n",
      "          [ 0.0193,  0.0193,  0.0322]],\n",
      "\n",
      "         [[ 0.0322, -0.0193, -0.0258],\n",
      "          [-0.0129, -0.0193, -0.0064],\n",
      "          [-0.0322, -0.0129,  0.0064]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3478,  0.2962,  0.3671],\n",
      "          [-0.0064,  0.0000,  0.0064],\n",
      "          [-0.0129, -0.0064, -0.0129]],\n",
      "\n",
      "         [[ 0.0000, -0.0129, -0.0258],\n",
      "          [ 0.0129,  0.0064,  0.0129],\n",
      "          [ 0.0064,  0.0000,  0.0064]],\n",
      "\n",
      "         [[-0.1932, -0.0064, -0.0064],\n",
      "          [-0.2383, -0.0129, -0.0193],\n",
      "          [-0.0064, -0.0064, -0.0064]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2898,  0.2125,  0.1674],\n",
      "          [-0.0064, -0.0129, -0.0193],\n",
      "          [ 0.0064,  0.0000, -0.0064]],\n",
      "\n",
      "         [[ 0.0193,  0.0129, -0.0129],\n",
      "          [ 0.0193,  0.0193,  0.0129],\n",
      "          [ 0.0000,  0.2383,  0.1674]],\n",
      "\n",
      "         [[ 0.0000,  0.0064,  0.3091],\n",
      "          [ 0.0258,  0.0129,  0.0000],\n",
      "          [ 0.1996,  0.0000,  0.0064]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0064, -0.0064, -0.0064],\n",
      "          [-0.0064,  0.0064,  0.0129],\n",
      "          [ 0.0193,  0.0258,  0.0386]],\n",
      "\n",
      "         [[ 0.0129,  0.0129,  0.2640],\n",
      "          [ 0.0129,  0.0000,  0.0064],\n",
      "          [ 0.0129, -0.1352,  0.0129]],\n",
      "\n",
      "         [[-0.1417,  0.0193,  0.0322],\n",
      "          [ 0.0322,  0.1803,  0.0129],\n",
      "          [ 0.0258,  0.0000,  0.0193]]],\n",
      "\n",
      "\n",
      "        [[[-0.0580, -0.0386, -0.0129],\n",
      "          [-0.3156, -0.3284,  0.0000],\n",
      "          [ 0.0064,  0.0064,  0.0193]],\n",
      "\n",
      "         [[-0.0580, -0.0515,  0.0000],\n",
      "          [ 0.0580,  0.4959,  0.1159],\n",
      "          [ 0.0193,  0.0322,  0.0129]],\n",
      "\n",
      "         [[-0.0193,  0.0129,  0.0386],\n",
      "          [ 0.0902,  0.1159,  0.1030],\n",
      "          [ 0.0064,  0.0386,  0.0322]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0064,  0.2125, -0.0129],\n",
      "          [-0.0322,  0.3542,  0.4637],\n",
      "          [-0.0258, -0.0193, -0.0129]],\n",
      "\n",
      "         [[ 0.0000,  0.0129, -0.2962],\n",
      "          [ 0.0515,  0.0515,  0.0322],\n",
      "          [-0.0644, -0.0258, -0.0129]],\n",
      "\n",
      "         [[ 0.4572,  0.0580,  0.0451],\n",
      "          [ 0.0515,  0.1159,  0.0708],\n",
      "          [ 0.2898,  0.0386,  0.0129]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.1095,  0.1288],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0064, -0.1417,  0.0064]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.2512, -0.2190,  0.0000],\n",
      "          [ 0.0064,  0.0064,  0.0129]],\n",
      "\n",
      "         [[-0.0064, -0.0064,  0.0000],\n",
      "          [-0.1803, -0.1674, -0.1546],\n",
      "          [-0.0064, -0.0129, -0.0129]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0064,  0.0000],\n",
      "          [-0.0064, -0.0129,  0.0000],\n",
      "          [-0.0064, -0.0064, -0.0064]],\n",
      "\n",
      "         [[ 0.0064,  0.0000,  0.0000],\n",
      "          [-0.1739, -0.1095,  0.1095],\n",
      "          [ 0.0000,  0.0000,  0.1674]],\n",
      "\n",
      "         [[ 0.1932,  0.2125,  0.1159],\n",
      "          [ 0.1417,  0.3220, -0.0064],\n",
      "          [ 0.0064,  0.0064,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0129, -0.1674,  0.0515],\n",
      "          [-0.0258, -0.0322, -0.0322],\n",
      "          [ 0.0258,  0.0515,  0.0515]],\n",
      "\n",
      "         [[-0.0515, -0.0515, -0.0386],\n",
      "          [-0.0258, -0.0193, -0.0258],\n",
      "          [-0.0064,  0.0258,  0.2061]],\n",
      "\n",
      "         [[ 0.1224,  0.0708,  0.0837],\n",
      "          [-0.0515, -0.0644, -0.0258],\n",
      "          [-0.0515, -0.0515, -0.0193]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0515, -0.0515, -0.0322],\n",
      "          [ 0.1546, -0.0515, -0.0322],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0129, -0.0064],\n",
      "          [-0.0129, -0.0129, -0.0064],\n",
      "          [-0.0193, -0.0129, -0.0129]],\n",
      "\n",
      "         [[ 0.0129,  0.0064,  0.0129],\n",
      "          [ 0.1610,  0.0064,  0.0064],\n",
      "          [-0.0129, -0.0193, -0.0064]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.1481,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.1546],\n",
      "          [-0.0064, -0.0064, -0.1224]],\n",
      "\n",
      "         [[-0.0129, -0.0064,  0.0000],\n",
      "          [-0.0193,  0.1546, -0.0064],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0193, -0.0129, -0.0193],\n",
      "          [-0.0193, -0.0129, -0.0258],\n",
      "          [-0.1224, -0.1288,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0129, -0.0129, -0.0129],\n",
      "          [ 0.0258,  0.0193,  0.0064],\n",
      "          [ 0.0064,  0.0064,  0.0064]],\n",
      "\n",
      "         [[-0.0064, -0.0129, -0.0064],\n",
      "          [ 0.0902,  0.0000,  0.0000],\n",
      "          [-0.1868, -0.1481, -0.1803]],\n",
      "\n",
      "         [[-0.0064, -0.0258, -0.0129],\n",
      "          [-0.0129, -0.0258,  0.1932],\n",
      "          [ 0.0000, -0.0064, -0.0129]]]], size=(47, 56, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.006439938675612211, zero_point=0)), ('tfeb.21.bias', Parameter containing:\n",
      "tensor([-0.1576, -0.1707, -0.1928, -0.1296, -0.1575, -0.0662, -0.0621, -0.1432,\n",
      "        -0.1725,  0.0597, -0.1562, -0.1955, -0.1464, -0.0545, -0.0283, -0.2377,\n",
      "        -0.0920,  0.0192, -0.0899, -0.0211, -0.0469,  0.0279, -0.2655, -0.2473,\n",
      "        -0.1317, -0.0034, -0.1798, -0.1405, -0.1367, -0.0161, -0.2571, -0.1960,\n",
      "        -0.1458, -0.0893,  0.0193, -0.1760, -0.1270, -0.0520,  0.0018, -0.2923,\n",
      "         0.0333, -0.1915, -0.2937, -0.2271, -0.0436, -0.1104, -0.0184])), ('tfeb.21.scale', tensor(0.0073)), ('tfeb.21.zero_point', tensor(0)), ('tfeb.25.weight', tensor([[[[-0.0099,  0.0000, -0.0561],\n",
      "          [ 0.0000,  0.0033,  0.0099],\n",
      "          [ 0.0000, -0.0066, -0.0066]],\n",
      "\n",
      "         [[ 0.0000,  0.0066,  0.0956],\n",
      "          [ 0.0033,  0.0000, -0.0099],\n",
      "          [-0.0033,  0.0000, -0.0033]],\n",
      "\n",
      "         [[-0.0561, -0.0627,  0.0033],\n",
      "          [-0.0033, -0.0033, -0.0033],\n",
      "          [-0.0033, -0.0033, -0.0726]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0033,  0.0000,  0.0726],\n",
      "          [ 0.0825,  0.0033,  0.0989]],\n",
      "\n",
      "         [[ 0.0033,  0.0066,  0.0066],\n",
      "          [-0.0033, -0.0033,  0.0000],\n",
      "          [-0.0660,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0033, -0.0066,  0.0792],\n",
      "          [-0.0726,  0.0033,  0.0033],\n",
      "          [ 0.0000, -0.0033,  0.0627]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0033,  0.0033,  0.0033],\n",
      "          [ 0.0000,  0.0033,  0.0231],\n",
      "          [-0.0989,  0.0033,  0.0198]],\n",
      "\n",
      "         [[-0.0033,  0.0000, -0.0132],\n",
      "          [-0.0165,  0.0396,  0.0132],\n",
      "          [ 0.1154,  0.0066,  0.0033]],\n",
      "\n",
      "         [[-0.1055,  0.0033,  0.0066],\n",
      "          [ 0.0000,  0.0989, -0.0066],\n",
      "          [-0.2210,  0.0033,  0.0033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0923, -0.0066,  0.0033],\n",
      "          [-0.0956, -0.0033,  0.0000]],\n",
      "\n",
      "         [[ 0.0033,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1154,  0.0066],\n",
      "          [ 0.0033, -0.2111, -0.1880]],\n",
      "\n",
      "         [[ 0.1220,  0.1253,  0.0033],\n",
      "          [ 0.0000, -0.1220,  0.0033],\n",
      "          [-0.0033, -0.0033, -0.0956]]],\n",
      "\n",
      "\n",
      "        [[[-0.0198,  0.0165,  0.0165],\n",
      "          [ 0.0759,  0.0198,  0.0198],\n",
      "          [ 0.0000, -0.0099, -0.0132]],\n",
      "\n",
      "         [[ 0.0000,  0.0330,  0.1319],\n",
      "          [ 0.0033,  0.0099,  0.0066],\n",
      "          [-0.0132, -0.0132,  0.1121]],\n",
      "\n",
      "         [[-0.0956, -0.1022,  0.0000],\n",
      "          [-0.0099, -0.0858,  0.0066],\n",
      "          [ 0.0264,  0.0198,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0033, -0.0033,  0.0923],\n",
      "          [-0.0033,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0890,  0.0000]],\n",
      "\n",
      "         [[ 0.0033,  0.0033,  0.0066],\n",
      "          [ 0.0792, -0.0033,  0.0066],\n",
      "          [-0.0099, -0.0099, -0.0099]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0099, -0.0066, -0.1253],\n",
      "          [ 0.1253,  0.0989,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0066, -0.0132],\n",
      "          [ 0.0066,  0.0033,  0.0000],\n",
      "          [-0.0066, -0.0792,  0.0066]],\n",
      "\n",
      "         [[ 0.0066, -0.1022,  0.0000],\n",
      "          [-0.0033,  0.0264,  0.0363],\n",
      "          [-0.0231, -0.0099,  0.0000]],\n",
      "\n",
      "         [[-0.1088,  0.0000, -0.0033],\n",
      "          [ 0.0132, -0.0033, -0.0099],\n",
      "          [ 0.0000, -0.0066, -0.0033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1022,  0.0033,  0.0066],\n",
      "          [-0.0066, -0.0066,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0066]],\n",
      "\n",
      "         [[-0.0033, -0.0066, -0.0099],\n",
      "          [-0.0033, -0.0033, -0.2177],\n",
      "          [-0.1055, -0.0033, -0.0858]],\n",
      "\n",
      "         [[-0.0759,  0.0231,  0.0198],\n",
      "          [ 0.0033,  0.0033,  0.0033],\n",
      "          [ 0.0000,  0.0033,  0.1418]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0165,  0.0165,  0.0132],\n",
      "          [ 0.0033,  0.0033,  0.0132],\n",
      "          [ 0.0264,  0.0264,  0.0264]],\n",
      "\n",
      "         [[ 0.0066,  0.0132,  0.0000],\n",
      "          [-0.0165, -0.1352, -0.0297],\n",
      "          [ 0.0165,  0.0132, -0.0165]],\n",
      "\n",
      "         [[ 0.0264, -0.0165, -0.0165],\n",
      "          [ 0.0792,  0.0165,  0.0066],\n",
      "          [ 0.0033, -0.0165, -0.0099]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0198, -0.0297, -0.0297],\n",
      "          [ 0.1352,  0.1022, -0.0066],\n",
      "          [ 0.1220,  0.1748,  0.1616]],\n",
      "\n",
      "         [[-0.1352, -0.1550, -0.1781],\n",
      "          [-0.1517, -0.1451, -0.1253],\n",
      "          [-0.1022, -0.1418, -0.0594]],\n",
      "\n",
      "         [[-0.0330, -0.0561, -0.0363],\n",
      "          [ 0.0066,  0.0000, -0.0099],\n",
      "          [ 0.0396,  0.0198,  0.0099]]],\n",
      "\n",
      "\n",
      "        [[[-0.0231, -0.0923, -0.0759],\n",
      "          [-0.0066,  0.0198,  0.0264],\n",
      "          [-0.0066,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0396, -0.0923,  0.0099],\n",
      "          [-0.0264,  0.0099,  0.0198],\n",
      "          [-0.0033,  0.0066, -0.0033]],\n",
      "\n",
      "         [[-0.0890,  0.0099,  0.0066],\n",
      "          [ 0.0000,  0.0000, -0.0033],\n",
      "          [-0.0033, -0.0099, -0.0033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0066, -0.0033,  0.0000],\n",
      "          [ 0.0033,  0.0099,  0.0033],\n",
      "          [ 0.0033,  0.0923, -0.0033]],\n",
      "\n",
      "         [[-0.0033,  0.0033,  0.0066],\n",
      "          [-0.0099,  0.0033,  0.0033],\n",
      "          [-0.0165, -0.0693,  0.0099]],\n",
      "\n",
      "         [[ 0.0890,  0.1154,  0.0066],\n",
      "          [ 0.0033, -0.0033, -0.0033],\n",
      "          [ 0.0033,  0.0033, -0.0066]]]], size=(65, 47, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.0032980830874294043, zero_point=0)), ('tfeb.25.bias', Parameter containing:\n",
      "tensor([-1.2290e-01, -1.6164e-01, -2.0165e-01, -1.7095e-01, -7.2159e-02,\n",
      "         4.6639e-02, -1.9102e-01, -1.9033e-01,  7.6531e-02,  7.2266e-02,\n",
      "        -8.8504e-02, -1.0377e-01, -8.3666e-02, -4.9360e-02, -3.9037e-02,\n",
      "        -1.8179e-06, -4.7389e-02, -4.7427e-02, -1.8383e-01, -1.6353e-01,\n",
      "         1.3772e-02, -1.3222e-01, -3.6612e-02, -5.1602e-02, -7.5321e-02,\n",
      "        -2.2531e-01,  4.5798e-03, -2.5505e-01, -3.6482e-02, -1.3350e-01,\n",
      "        -4.9540e-02, -1.7245e-01, -2.8054e-01, -1.8702e-01, -5.4695e-02,\n",
      "        -5.9713e-02, -7.0877e-02,  1.1592e-01, -2.2345e-01, -1.9162e-01,\n",
      "        -7.4717e-02, -2.6080e-01, -1.1520e-01, -8.9174e-02, -2.2006e-01,\n",
      "        -2.0009e-01, -8.5365e-02, -1.7993e-01, -1.7135e-01, -7.2273e-02,\n",
      "        -2.6555e-02, -4.6442e-02,  6.5069e-03, -8.9540e-02, -1.1876e-01,\n",
      "        -5.2522e-02, -1.6364e-01, -1.5566e-01, -2.0181e-02, -2.4741e-02,\n",
      "         6.2399e-03, -2.1699e-01, -1.0973e-01,  5.1683e-02, -2.3602e-01])), ('tfeb.25.scale', tensor(0.0037)), ('tfeb.25.zero_point', tensor(0)), ('tfeb.28.weight', tensor([[[[-0.0038,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0038,  0.0038],\n",
      "          [-0.0038,  0.0038,  0.0153]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0038],\n",
      "          [-0.0115,  0.0230,  0.0268],\n",
      "          [-0.0077,  0.0000,  0.0077]],\n",
      "\n",
      "         [[-0.0345,  0.0000,  0.0000],\n",
      "          [-0.0077,  0.0077,  0.0115],\n",
      "          [-0.0153,  0.0345,  0.0613]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0038,  0.0115],\n",
      "          [-0.0115,  0.0268,  0.0230],\n",
      "          [-0.0115,  0.0000, -0.0077]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0038, -0.0460, -0.0038],\n",
      "          [-0.0038, -0.0077, -0.0077]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0115,  0.0038,  0.0115],\n",
      "          [-0.0115,  0.0115,  0.0077]]],\n",
      "\n",
      "\n",
      "        [[[-0.0996,  0.0000,  0.0000],\n",
      "          [-0.1150, -0.0077, -0.0268],\n",
      "          [ 0.0038,  0.0000, -0.0153]],\n",
      "\n",
      "         [[-0.0192, -0.0153, -0.0077],\n",
      "          [ 0.0575, -0.0422,  0.0996],\n",
      "          [ 0.0307,  0.0000, -0.0077]],\n",
      "\n",
      "         [[ 0.0000,  0.0077,  0.0038],\n",
      "          [-0.0153, -0.0192,  0.0920],\n",
      "          [-0.0077,  0.0077, -0.0230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0230, -0.0268, -0.0192],\n",
      "          [ 0.0153, -0.0230, -0.0268],\n",
      "          [ 0.0115, -0.0038, -0.0038]],\n",
      "\n",
      "         [[-0.0038, -0.0038, -0.0038],\n",
      "          [ 0.0000,  0.0000, -0.1226],\n",
      "          [ 0.0038,  0.0153,  0.0077]],\n",
      "\n",
      "         [[-0.0077, -0.0038, -0.0038],\n",
      "          [-0.0115, -0.0307, -0.0153],\n",
      "          [ 0.1686, -0.1303,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0038,  0.0000,  0.0077],\n",
      "          [-0.0038, -0.0766,  0.0000],\n",
      "          [-0.0038,  0.0077,  0.0038]],\n",
      "\n",
      "         [[-0.0077, -0.0038, -0.0077],\n",
      "          [-0.0038,  0.0077,  0.0038],\n",
      "          [-0.0038,  0.0000,  0.0077]],\n",
      "\n",
      "         [[ 0.0038, -0.0038, -0.0153],\n",
      "          [-0.0077,  0.0000, -0.0038],\n",
      "          [-0.0192, -0.0038,  0.0192]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0077,  0.0000,  0.0000],\n",
      "          [-0.0153,  0.0192, -0.0115],\n",
      "          [ 0.0000,  0.0000, -0.0077]],\n",
      "\n",
      "         [[ 0.0038,  0.0000,  0.0077],\n",
      "          [ 0.0115, -0.0077, -0.0115],\n",
      "          [-0.0115, -0.0115, -0.0115]],\n",
      "\n",
      "         [[-0.0038,  0.0000, -0.0038],\n",
      "          [ 0.0115,  0.0077,  0.0153],\n",
      "          [-0.0115,  0.0000, -0.0690]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0192, -0.0077, -0.0038],\n",
      "          [-0.0307, -0.0230, -0.0115],\n",
      "          [-0.0153, -0.0153, -0.0115]],\n",
      "\n",
      "         [[-0.1226, -0.0038, -0.0038],\n",
      "          [-0.0192, -0.0192, -0.0077],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0038,  0.0000,  0.0920],\n",
      "          [-0.0307, -0.0268, -0.0115],\n",
      "          [-0.0153, -0.0192, -0.0153]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0268, -0.0115, -0.0077],\n",
      "          [-0.0268, -0.0268, -0.0307],\n",
      "          [-0.0843,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0038,  0.0077,  0.0000],\n",
      "          [ 0.0000,  0.0038,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0077,  0.0077],\n",
      "          [-0.0230, -0.0077,  0.0077],\n",
      "          [-0.1226, -0.1150, -0.0153]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0805, -0.0077,  0.0038],\n",
      "          [ 0.0115, -0.0115, -0.0038],\n",
      "          [ 0.0192,  0.0115,  0.0115]],\n",
      "\n",
      "         [[-0.0115, -0.0422,  0.0038],\n",
      "          [ 0.0422,  0.0038,  0.1188],\n",
      "          [ 0.0192,  0.0077,  0.0996]],\n",
      "\n",
      "         [[-0.0192, -0.0230, -0.0077],\n",
      "          [ 0.0958, -0.0077, -0.0153],\n",
      "          [ 0.0192,  0.0268,  0.0268]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0038, -0.0307, -0.0077],\n",
      "          [ 0.1724,  0.2069,  0.0345],\n",
      "          [ 0.1035,  0.1763,  0.0000]],\n",
      "\n",
      "         [[-0.0077, -0.0115, -0.0077],\n",
      "          [-0.0153, -0.0307, -0.0230],\n",
      "          [ 0.0038,  0.0000, -0.0077]],\n",
      "\n",
      "         [[-0.0115, -0.0038,  0.0000],\n",
      "          [-0.0038, -0.0345, -0.0230],\n",
      "          [ 0.0230,  0.0115,  0.0230]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0038, -0.0038],\n",
      "          [-0.0077, -0.0153, -0.0115],\n",
      "          [-0.0115, -0.0115, -0.0115]],\n",
      "\n",
      "         [[-0.2031, -0.0038,  0.0000],\n",
      "          [-0.0153, -0.0038,  0.0038],\n",
      "          [-0.0077, -0.0038, -0.0038]],\n",
      "\n",
      "         [[ 0.2414,  0.0000,  0.0000],\n",
      "          [-0.0038, -0.0077,  0.0038],\n",
      "          [-0.0077, -0.0115, -0.0038]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -0.0038, -0.0038],\n",
      "          [-0.0038,  0.0038,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0038,  0.0000,  0.0000],\n",
      "          [ 0.0077,  0.0000,  0.0038],\n",
      "          [ 0.0000, -0.0077, -0.0038]],\n",
      "\n",
      "         [[-0.0038, -0.0038, -0.0038],\n",
      "          [-0.0153, -0.0077, -0.0038],\n",
      "          [ 0.3794, -0.0077, -0.0077]]]], size=(90, 65, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.0038318783044815063, zero_point=0)), ('tfeb.28.bias', Parameter containing:\n",
      "tensor([-0.1166, -0.0483, -0.0417, -0.0251, -0.0229, -0.3006, -0.1105, -0.1506,\n",
      "        -0.1102, -0.1512, -0.0855, -0.0133, -0.0156, -0.1157, -0.1415, -0.0888,\n",
      "        -0.0523, -0.0308, -0.0408, -0.0354, -0.1122, -0.0328, -0.0724, -0.0845,\n",
      "        -0.0506, -0.1360, -0.1474, -0.1767, -0.0598, -0.0698, -0.0388, -0.0691,\n",
      "        -0.0581, -0.0881, -0.0813, -0.1525, -0.0340, -0.0896, -0.0055, -0.0651,\n",
      "        -0.2275,  0.0645, -0.1123, -0.1098, -0.1315, -0.1564, -0.1504, -0.0277,\n",
      "        -0.1775, -0.1139, -0.1510, -0.1166, -0.0460, -0.0429, -0.1134, -0.1460,\n",
      "        -0.0773, -0.0556, -0.0573, -0.1804, -0.1449, -0.1130, -0.0615, -0.1078,\n",
      "        -0.1326, -0.0951, -0.0543,  0.0799, -0.0423, -0.0684, -0.0989, -0.0514,\n",
      "        -0.0603, -0.0750, -0.1140, -0.0400, -0.1857, -0.0582, -0.0304, -0.0916,\n",
      "        -0.1391, -0.0570,  0.0364,  0.0030, -0.0619, -0.1271,  0.0078, -0.1206,\n",
      "        -0.1063, -0.0645])), ('tfeb.28.scale', tensor(0.0037)), ('tfeb.28.zero_point', tensor(0)), ('tfeb.33.weight', tensor([[[[-0.7030]],\n",
      "\n",
      "         [[ 0.2125]],\n",
      "\n",
      "         [[-0.4741]],\n",
      "\n",
      "         [[ 0.1880]],\n",
      "\n",
      "         [[ 0.0327]],\n",
      "\n",
      "         [[ 0.2779]],\n",
      "\n",
      "         [[ 0.0163]],\n",
      "\n",
      "         [[ 0.5150]],\n",
      "\n",
      "         [[ 0.0409]],\n",
      "\n",
      "         [[ 0.2044]],\n",
      "\n",
      "         [[ 0.3924]],\n",
      "\n",
      "         [[ 0.2698]],\n",
      "\n",
      "         [[ 0.2534]],\n",
      "\n",
      "         [[ 0.3270]],\n",
      "\n",
      "         [[ 0.4332]],\n",
      "\n",
      "         [[-0.3515]],\n",
      "\n",
      "         [[-0.4087]],\n",
      "\n",
      "         [[-0.4414]],\n",
      "\n",
      "         [[ 0.1063]],\n",
      "\n",
      "         [[-0.2044]],\n",
      "\n",
      "         [[-0.3842]],\n",
      "\n",
      "         [[ 0.8583]],\n",
      "\n",
      "         [[ 0.2943]],\n",
      "\n",
      "         [[ 0.0572]],\n",
      "\n",
      "         [[ 0.1880]],\n",
      "\n",
      "         [[ 0.0163]],\n",
      "\n",
      "         [[-0.0490]],\n",
      "\n",
      "         [[ 0.2371]],\n",
      "\n",
      "         [[ 0.5885]],\n",
      "\n",
      "         [[ 0.5395]],\n",
      "\n",
      "         [[-0.3924]],\n",
      "\n",
      "         [[-0.0163]],\n",
      "\n",
      "         [[-0.4823]],\n",
      "\n",
      "         [[-0.0654]],\n",
      "\n",
      "         [[-0.2044]],\n",
      "\n",
      "         [[-0.4496]],\n",
      "\n",
      "         [[ 0.6785]],\n",
      "\n",
      "         [[ 0.0736]],\n",
      "\n",
      "         [[ 0.1880]],\n",
      "\n",
      "         [[-0.0327]],\n",
      "\n",
      "         [[ 0.2943]],\n",
      "\n",
      "         [[ 0.4986]],\n",
      "\n",
      "         [[-0.6703]],\n",
      "\n",
      "         [[-0.1471]],\n",
      "\n",
      "         [[ 0.4414]],\n",
      "\n",
      "         [[-0.0163]],\n",
      "\n",
      "         [[ 0.0572]],\n",
      "\n",
      "         [[-0.1308]],\n",
      "\n",
      "         [[ 0.4414]],\n",
      "\n",
      "         [[ 0.4659]],\n",
      "\n",
      "         [[ 0.2698]],\n",
      "\n",
      "         [[ 0.4169]],\n",
      "\n",
      "         [[ 0.6866]],\n",
      "\n",
      "         [[ 0.3024]],\n",
      "\n",
      "         [[ 0.1798]],\n",
      "\n",
      "         [[ 0.5477]],\n",
      "\n",
      "         [[ 0.0736]],\n",
      "\n",
      "         [[ 0.4251]],\n",
      "\n",
      "         [[ 0.5068]],\n",
      "\n",
      "         [[ 0.4905]],\n",
      "\n",
      "         [[ 0.2861]],\n",
      "\n",
      "         [[ 0.4332]],\n",
      "\n",
      "         [[-0.0490]],\n",
      "\n",
      "         [[ 0.3270]],\n",
      "\n",
      "         [[ 0.0490]],\n",
      "\n",
      "         [[-0.3351]],\n",
      "\n",
      "         [[-0.5395]],\n",
      "\n",
      "         [[ 1.0381]],\n",
      "\n",
      "         [[ 0.3433]],\n",
      "\n",
      "         [[ 0.0899]],\n",
      "\n",
      "         [[-0.1798]],\n",
      "\n",
      "         [[ 0.3678]],\n",
      "\n",
      "         [[ 0.1798]],\n",
      "\n",
      "         [[-0.4169]],\n",
      "\n",
      "         [[-0.2207]],\n",
      "\n",
      "         [[-0.4823]],\n",
      "\n",
      "         [[ 0.3188]],\n",
      "\n",
      "         [[-0.0327]],\n",
      "\n",
      "         [[-0.2698]],\n",
      "\n",
      "         [[ 0.2943]],\n",
      "\n",
      "         [[ 0.4986]],\n",
      "\n",
      "         [[-0.6049]],\n",
      "\n",
      "         [[ 0.4005]],\n",
      "\n",
      "         [[ 0.5150]],\n",
      "\n",
      "         [[ 0.0245]],\n",
      "\n",
      "         [[ 0.2616]],\n",
      "\n",
      "         [[ 0.3760]],\n",
      "\n",
      "         [[-0.1962]],\n",
      "\n",
      "         [[ 0.6539]],\n",
      "\n",
      "         [[ 0.1226]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7439]],\n",
      "\n",
      "         [[-0.2125]],\n",
      "\n",
      "         [[ 0.2698]],\n",
      "\n",
      "         [[-0.3270]],\n",
      "\n",
      "         [[ 0.0409]],\n",
      "\n",
      "         [[ 0.3597]],\n",
      "\n",
      "         [[-0.2943]],\n",
      "\n",
      "         [[ 0.4087]],\n",
      "\n",
      "         [[ 0.6376]],\n",
      "\n",
      "         [[ 0.0327]],\n",
      "\n",
      "         [[ 0.4905]],\n",
      "\n",
      "         [[-0.3597]],\n",
      "\n",
      "         [[-0.2289]],\n",
      "\n",
      "         [[ 0.0163]],\n",
      "\n",
      "         [[-0.2207]],\n",
      "\n",
      "         [[ 0.4414]],\n",
      "\n",
      "         [[ 0.3760]],\n",
      "\n",
      "         [[ 0.8419]],\n",
      "\n",
      "         [[-0.4332]],\n",
      "\n",
      "         [[ 0.3842]],\n",
      "\n",
      "         [[ 0.5395]],\n",
      "\n",
      "         [[-0.5395]],\n",
      "\n",
      "         [[-0.0654]],\n",
      "\n",
      "         [[-0.2044]],\n",
      "\n",
      "         [[ 0.4251]],\n",
      "\n",
      "         [[ 0.5558]],\n",
      "\n",
      "         [[ 0.3760]],\n",
      "\n",
      "         [[ 0.3351]],\n",
      "\n",
      "         [[-0.1553]],\n",
      "\n",
      "         [[-0.0899]],\n",
      "\n",
      "         [[ 0.4169]],\n",
      "\n",
      "         [[ 0.1962]],\n",
      "\n",
      "         [[ 0.3270]],\n",
      "\n",
      "         [[ 0.5395]],\n",
      "\n",
      "         [[ 0.0409]],\n",
      "\n",
      "         [[ 0.2452]],\n",
      "\n",
      "         [[-0.3924]],\n",
      "\n",
      "         [[-0.0654]],\n",
      "\n",
      "         [[-0.3106]],\n",
      "\n",
      "         [[ 0.2289]],\n",
      "\n",
      "         [[ 0.6621]],\n",
      "\n",
      "         [[-0.4496]],\n",
      "\n",
      "         [[ 0.5068]],\n",
      "\n",
      "         [[ 0.5558]],\n",
      "\n",
      "         [[ 0.4741]],\n",
      "\n",
      "         [[ 0.2779]],\n",
      "\n",
      "         [[ 0.3351]],\n",
      "\n",
      "         [[ 0.2371]],\n",
      "\n",
      "         [[ 0.5068]],\n",
      "\n",
      "         [[ 0.0245]],\n",
      "\n",
      "         [[ 0.3188]],\n",
      "\n",
      "         [[-0.1226]],\n",
      "\n",
      "         [[-0.2125]],\n",
      "\n",
      "         [[-0.2289]],\n",
      "\n",
      "         [[-0.1553]],\n",
      "\n",
      "         [[ 0.5313]],\n",
      "\n",
      "         [[-0.0981]],\n",
      "\n",
      "         [[ 0.0490]],\n",
      "\n",
      "         [[-0.1635]],\n",
      "\n",
      "         [[ 0.6294]],\n",
      "\n",
      "         [[ 0.3924]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         [[ 0.4986]],\n",
      "\n",
      "         [[ 0.1063]],\n",
      "\n",
      "         [[ 0.5395]],\n",
      "\n",
      "         [[ 0.4005]],\n",
      "\n",
      "         [[ 0.4578]],\n",
      "\n",
      "         [[-0.8828]],\n",
      "\n",
      "         [[-0.2698]],\n",
      "\n",
      "         [[-0.1144]],\n",
      "\n",
      "         [[ 0.2616]],\n",
      "\n",
      "         [[ 0.3842]],\n",
      "\n",
      "         [[-0.0163]],\n",
      "\n",
      "         [[ 0.5395]],\n",
      "\n",
      "         [[ 0.3515]],\n",
      "\n",
      "         [[ 0.1553]],\n",
      "\n",
      "         [[ 0.3760]],\n",
      "\n",
      "         [[ 0.3433]],\n",
      "\n",
      "         [[ 0.3678]],\n",
      "\n",
      "         [[-0.1226]],\n",
      "\n",
      "         [[ 0.5477]],\n",
      "\n",
      "         [[ 0.6294]],\n",
      "\n",
      "         [[-0.4251]],\n",
      "\n",
      "         [[-0.3678]],\n",
      "\n",
      "         [[ 0.3760]],\n",
      "\n",
      "         [[-0.0327]],\n",
      "\n",
      "         [[-0.6376]],\n",
      "\n",
      "         [[ 0.2779]],\n",
      "\n",
      "         [[ 0.1144]],\n",
      "\n",
      "         [[ 0.1308]]]], size=(2, 90, 1, 1), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.008174257352948189,\n",
      "       zero_point=0)), ('tfeb.33.bias', Parameter containing:\n",
      "tensor([0.2968, 0.4328])), ('tfeb.33.scale', tensor(0.0294)), ('tfeb.33.zero_point', tensor(0)), ('tfeb.38.scale', tensor(0.0126)), ('tfeb.38.zero_point', tensor(125)), ('tfeb.38._packed_params.dtype', torch.qint8), ('tfeb.38._packed_params._packed_params', (tensor([[ 1.3822, -1.2916],\n",
      "        [-1.3369,  1.4388]], size=(2, 2), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.011329498142004013,\n",
      "       zero_point=0), tensor([-0.0100, -0.1046], grad_fn=<CopyBackwards>))), ('quant.scale', tensor([0.0134])), ('quant.zero_point', tensor([127]))])\n",
      "Quantization done\n",
      "Testing quantized model.\n",
      "torch.Size([176, 2])\n",
      "Testing: Acc(top1) 90.91%\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29a3ca04-7384-4a90-853e-050b593a0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt2 = getOpts();#opts.parse();\n",
    "# state2 = torch.load(to_convert_model_path, map_location=\"cuda:0\");\n",
    "# tmpnet = models.GetACDNetModel(input_len=opt2.inputLength, nclass=6, sr=20000, channel_config=state2['config']).to(\"cuda:0\");\n",
    "# tmpnet.load_state_dict(state2['weight']);\n",
    "# print(tmpnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59060995-a696-4bc6-8980-5a5d18cf4bd1",
   "metadata": {},
   "source": [
    "## Convert tflite to C array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab64fe84-ccca-4d35-80a3-edfd7012b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_TFLITE = os.path.join(MODELS_DIR, 'model.tflite')\n",
    "# FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'float_model.tflite')\n",
    "# MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'model.cc')\n",
    "tflite_model = \"../th/quantized_models/quant_retrained_model_95.4_20240130035114.pt\"\n",
    "target_c_file =\"../th/Model_C_Files/quant_from_retrain_95.4_20240130035114.cc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce47b1c0-ac1e-47d6-abf9-72dd595e94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get -qq install xxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "274d47a5-acc2-4eeb-bd3b-2c57ae556cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get -qq install xxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "190ecc22-9963-4c04-b389-7f339a8e6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!xxd -i {tflite_model} > {target_c_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6812387-b134-4bee-acc6-5d4402d34559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 1: \"../th/Model_C_Files/qua ...\": invalid command code .\n"
     ]
    }
   ],
   "source": [
    "# Update variable names\n",
    "REPLACE_TEXT = \"uec_iot_model_alarm\";#MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
    "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {target_c_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b37ec-3980-4aed-8b2e-e86e36718ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
