{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692379c8-c60a-4ddb-a425-09c0b030bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import glob;\n",
    "import math;\n",
    "import random;\n",
    "import torch;\n",
    "import torch.nn as nn;\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5939dcb3-4805-42d7-9900-637088d59e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1585005-0d41-4964-a79f-63ac0a2fc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7733cacf-1504-4ce6-983c-871ee1dc116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.opts as opts;\n",
    "import th.resources.models as models;\n",
    "import th.resources.calculator as calc;\n",
    "# import resources.train_generator as train_generator;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df514eb-ae13-469e-bb95-c92fbdd7cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# import common.tlopts as tlopts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d04c10f-d1a7-454a-b163-cac1d59dd7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.utils as U;\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from tinynn.converter import TFLiteConverter\n",
    "from tinynn.graph.quantization.quantizer import PostQuantizer\n",
    "from tinynn.graph.tracer import model_tracer\n",
    "from tinynn.util.train_util import DLContext, get_device\n",
    "from tinynn.graph.quantization.algorithm.cross_layer_equalization import cross_layer_equalize\n",
    "from tinynn.converter import TFLiteConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa9206ea-537a-4ad0-a008-a2d2a6c11b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#refsite:https://discuss.pytorch.org/t/how-to-generate-a-fully-quantized-model/175185\n",
    "from torch.ao.quantization.backend_config import BackendConfig, BackendPatternConfig, DTypeConfig, ObservationType\n",
    "from torch.quantization import quantize_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "from torch.ao.quantization.fx.custom_config import PrepareCustomConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ded3e2c-21bc-4bac-9c9f-fa5e6697b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9de080e-60bb-4cba-b9ac-8727f3b422d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076adfb-46fb-44ba-98e0-c38a9db82f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a1baf79-1e4b-4fe0-8f76-7b7163ec3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDataTimeStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H:%M:%S').replace('-',\"\").replace(' ',\"\").replace(':',\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fab4a323-4291-421b-867a-d52ee41063ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = dict([(52,1),(99,2)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[label1]- 1\n",
    "            idx2 = self.mapdict[label2] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb20ff6-42ad-4671-969f-d6ad8abe5bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a30933e-1045-41a0-b258-2ef29e94bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None):\n",
    "    # dataset = np.load(os.path.join(opt.data, opt.dataset, 'wav{}.npz'.format(opt.sr // 1000)), allow_pickle=True);\n",
    "    # dataset = np.load(\"../datasets/fold1_test16000.npz\", allow_pickle=True);\n",
    "    dataset = np.load(\"../datasets/forOneClassModel_alarm/train/trainSet_20240119002902.npz\", allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # print(len(dataset['x']))\n",
    "    # for i in range(1, opt.nFolds + 1):\n",
    "\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['fold{}'.format(1)].item()['sounds']\n",
    "    train_labels = dataset['fold{}'.format(1)].item()['labels']\n",
    "    # print(train_sounds)\n",
    "\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc776e4-0edf-4424-a3af-a946d33db7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='./datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 32;\n",
    "    opt.weightDecay = 5e-3;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.nEpochs = 800;#2000;\n",
    "    opt.LR = 0.1;\n",
    "    opt.schedule = [0.3, 0.6, 0.9];\n",
    "    opt.warmup = 10;\n",
    "\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 2#50;\n",
    "    opt.nFolds = 1;#5;\n",
    "    opt.split = 1#[i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    opt.ch_config = [8,64,32,64,64,128,128,256,256,512,512,2];\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb634178-30c9-4757-9db0-3a65a1c8479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_int8(x, axis):\n",
    "  '''Quantization into int8_t precision, operating on x along axis'''\n",
    "\n",
    "  scaling_factor_shape = tuple(np.append([len(x)],np.ones(x.ndim - 1, dtype = int)))\n",
    "  epsilon = 0.000000001\n",
    "  x_scaling_factor = (2 * np.max(np.abs(x), axis) / 255) + epsilon\n",
    "  x_scaling_factor = x_scaling_factor.reshape(scaling_factor_shape)\n",
    "  x_zero_offset = -0.5\n",
    "  result = (x / x_scaling_factor) + x_zero_offset\n",
    "\n",
    "  return np.rint(result).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d56ed47-bc98-4899-9e46-d4726cfa565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, opt=None, split=0):\n",
    "        self.opt = opt;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.trainX = None;\n",
    "        self.trainY = None;\n",
    "        # self.opt.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "        self.opt.device = torch.device(\"cpu\")\n",
    "        self.trainGen = getTrainGen(self.opt)#train_generator.setup(self.opt, self.opt.split);\n",
    "        self.qunt_nClass = 2;\n",
    "\n",
    "    def load_train_data(self):\n",
    "        print('Preparing calibration dataset..');\n",
    "        x,y = self.trainGen.__getitem__(0);\n",
    "        self.trainX = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "        \"\"\"\n",
    "        trainX size:torch.Size([1, 1, 30225]), but must be [1,1,1,30225]\n",
    "        Due to the reason: raise ValueError(\"Input shape must be `(N, C, H, W)`!\")\n",
    "        \"\"\"\n",
    "        # print(f\"trainX[0] shape:{self.trainX[0].shape}\")\n",
    "        self.trainY = torch.tensor(y).to(self.opt.device);\n",
    "        print('Calibration dataset is ready');\n",
    "        # self.opt.batchSize = 32;\n",
    "\n",
    "    def load_test_data(self):\n",
    "        if(self.testX is None):\n",
    "            data = np.load('../datasets/forOneClassModel_alarm/test_val/final_val_test_npz/final_valSet_20240119004614.npz', allow_pickle=True);\n",
    "            dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "            self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "            self.testY = torch.tensor(data['y']).to(self.opt.device);\n",
    "\n",
    "    def __validate(self, net, testX, testY):\n",
    "        net.eval();\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = len(self.testX);\n",
    "            x = self.testX[:];\n",
    "            \n",
    "            # batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "            # for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "            #     x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "            #     #print(x.shape);\n",
    "            #     # exit();\n",
    "            #     scores = net(x);\n",
    "            #     y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "            scores = net(x);\n",
    "            y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "            acc = self.__compute_accuracy(y_pred, self.testY);\n",
    "        return acc;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def __compute_accuracy(self, y_pred, y_target):\n",
    "        print(y_pred.shape);\n",
    "        with torch.no_grad():\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1);\n",
    "\n",
    "            y_pred = y_pred.argmax(dim=1);\n",
    "            y_target = y_target.argmax(dim=1);\n",
    "\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "        return acc;\n",
    "\n",
    "    def __load_model(self, quant=True):\n",
    "        state = torch.load(self.opt.model_path, map_location=self.opt.device);\n",
    "        print(state['config']);\n",
    "        net = None;\n",
    "        # if quant:\n",
    "        net = models.GetACDNetQuantModel(input_len=self.opt.inputLength, nclass=self.qunt_nClass, sr=self.opt.sr, channel_config=state['config']).to(self.opt.device);\n",
    "        # else:\n",
    "            # net = models.GetACDNetModel(input_len=self.opt.inputLength, nclass=self.qunt_nClass, sr=self.opt.sr, channel_config=state['config']).to(self.opt.device);\n",
    "            # net = GetTLACDNet(opt=self.opt);\n",
    "        calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        net.load_state_dict(state['weight']);\n",
    "        return net;\n",
    "\n",
    "    def __calibrate(self, net):\n",
    "        self.load_train_data();\n",
    "        net.eval();\n",
    "        with torch.no_grad():\n",
    "            for i in range(1,2):\n",
    "                x_pred = None;\n",
    "                for idx in range(math.ceil(len(self.trainX)/self.opt.batchSize)):\n",
    "                    x = self.trainX[idx*self.opt.batchSize : (idx+1)*self.opt.batchSize];\n",
    "                    #print(x.shape);\n",
    "                    # exit();\n",
    "                    scores = net(x);\n",
    "                    x_pred = scores.data if x_pred is None else torch.cat((x_pred, scores.data));\n",
    "\n",
    "                x_pred = x_pred.argmax(dim=1);\n",
    "                x_target = self.trainY.argmax(dim=1);\n",
    "\n",
    "                acc = (((x_pred==x_target)*1).float().mean()*100).item();\n",
    "                print('calibrate accuracy is: {:.2f}'.format(acc));\n",
    "        return acc;\n",
    "\n",
    "    def QuantizeModel(self):\n",
    "        net = self.__load_model(True);\n",
    "        # net = self.__load_model(False);\n",
    "        config = net.ch_config;\n",
    "        net.eval();\n",
    "        \n",
    "        #Fuse modules to\n",
    "        torch.quantization.fuse_modules(net.sfeb, ['0','1','2'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.sfeb, ['3','4','5'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['0','1','2'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['4','5','6'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['7','8','9'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['11','12','13'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['14','15','16'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['18','19','20'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['21','22','23'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['25','26','27'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['28','29','30'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['33','34','35'], inplace=True);\n",
    "        \n",
    "        net.qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')\n",
    "        torch.backends.quantized.engine = 'qnnpack';\n",
    "        print(net.qconfig);\n",
    "        torch.quantization.prepare(net, inplace=True);\n",
    "        \n",
    "        # Calibrate with the training data\n",
    "        self.__calibrate(net);\n",
    "\n",
    "        # Convert to quantized model\n",
    "        torch.quantization.convert(net, inplace=True);\n",
    "        print('Post Training Quantization: Convert done');\n",
    "\n",
    "        print(\"Size of model after quantization\");\n",
    "        torch.save(net.state_dict(), \"temp.p\")\n",
    "        print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "        os.remove('temp.p')\n",
    "        \n",
    "        self.load_test_data();\n",
    "        val_acc = self.__validate(net, self.testX, self.testY);\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "\n",
    "        # torch.jit.save(torch.jit.script(net), '{}/th/quantized_models/{}.pt'.format(os.getcwd(), self.opt.model_name.format()));\n",
    "        torch.jit.save(torch.jit.script(net), '../th/quantized_models/{}.pt'.format(self.opt.model_name.format()));\n",
    " \n",
    "        # **************convert to tflite**********\n",
    "        with torch.no_grad():\n",
    "            # dummy_input = torch.randn(1, 1, 30225, 1); wrong: RuntimeError: quantized::conv2d (qnnpack): each dimension of output tensor should be greater than 0.\n",
    "            dummy_input = torch.FloatTensor(quantize_int8(torch.randn(1, 1, 1, 30225).numpy(),3)); #correct,workable\n",
    "            # dummy_input = torch.randn(30225,1,1,1); wrong: RuntimeError: quantized::conv2d (qnnpack): each dimension of output tensor should be greater than 0.\n",
    "            # dummy_input = torch.randn(1,30225,1,1); wrong:RuntimeError: Input channel size of weight and bias must match.\n",
    "            #the followng setting for TFLiteConverter, especially quantize_input_output_type='int8',fuse_quant_dequant=True,\n",
    "            #we need to remove softmax layer from ACDQuantModel to satisfy the output is int8 type\n",
    "            converter = TFLiteConverter(net,\n",
    "                                        dummy_input,\n",
    "                                        quantize_input_output_type='int8',#設定此欄，輸入會強制為int8\n",
    "                                        fuse_quant_dequant=True,\n",
    "                                        quantize_target_type='int8',\n",
    "                                        hybrid_conv=False,\n",
    "                                        float16_quantization=True,\n",
    "                                        optimize=5,\n",
    "                                        tflite_path=\"../th/quantized_models/{}.tflite\".format(self.opt.model_name))\n",
    "            converter.convert()\n",
    "\n",
    "        print(net.state_dict())\n",
    "            # qmodel = copy.deepcopy(mynn)\n",
    "            # torch.quantization.convert(qmodel, inplace=False)\n",
    "            #\n",
    "            # torch.backends.quantized.engine = 'qnnpack'\n",
    "            # converter = TFLiteConverter(qmodel.module,\n",
    "            #                             torch.randn(1, 64, nn_h, nn_w,\n",
    "            #                             tflite_path=\"qmodel.tflite\")\n",
    "            # converter.convert()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def TestModel(self, quant=False):\n",
    "        if quant:\n",
    "            net = torch.jit.load('../th/quantized_models/{}.pt'.format(self.opt.model_name))\n",
    "        else:\n",
    "            net = self.__load_model();\n",
    "            # calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        self.load_test_data();\n",
    "        net.eval();\n",
    "        val_acc = self.__validate(net, self.testX, self.testY);\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "\n",
    "    def GetModelSize(self):\n",
    "        orig_net_path = self.opt.model_path;\n",
    "        print('Full precision model size (KB):', os.path.getsize(orig_net_path)/(1024));\n",
    "        save_onnx_name = \"{}.onnx\".format(self.opt.model_name);\n",
    "        quant_net_path = \"../th/onnx_models/\"+save_onnx_name;\n",
    "        print('Quantized model size (KB):', os.path.getsize(quant_net_path)/(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58eff9fb-5fd1-423b-9a60-61c9e9824dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_convert_model_path = \"./th/pruned_models/second_stage_pruned_models/magnitude_pruning/acdnet_tl_hybrid_pruning_magnitude_model_202312281149_80.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f986060d-9a68-4021-b9c4-bfae8d252d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_tensor type:<class 'torch.Tensor'>\n",
      "[[[[ 53  41  25 ... -43  53 -37]]]]\n",
      "(1, 1, 1, 30225)\n"
     ]
    }
   ],
   "source": [
    "#before calling quantize_int8, torch tensor need to be converted to ndarray\n",
    "test_ary = torch.randn(1, 1, 1, 30225).numpy();\n",
    "\n",
    "\n",
    "q_test_ary = quantize_int8(test_ary,3)\n",
    "test_tensor = torch.CharTensor(q_test_ary)\n",
    "print(f\"test_tensor type:{type(test_tensor)}\")\n",
    "print(q_test_ary)\n",
    "print(q_test_ary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce59dbc6-1088-427e-a3bb-5e88d07465d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    opt = getOpts();#opts.parse();\n",
    "    opt.device = 'cpu';\n",
    "    opt.model_path = \"./retrained_models_after_pruned/retrained_acdnet_after_tylor_pruning_model_from_acc_97.7_0.9compress_20240205181543_acc_82.95455169677734_997th_epoch.pt\"\n",
    "    # valid_path = False;\n",
    "    # while not valid_path:\n",
    "    #     model_path = input(\"Enter the model PATH for 8-bit post training quantization\\n:\");\n",
    "    #     file_paths = glob.glob(os.path.join(os.getcwd(), model_path));\n",
    "    #     if len(file_paths)>0 and os.path.isfile(file_paths[0]):\n",
    "    #         state = torch.load(file_paths[0], map_location='cpu');\n",
    "    #         opt.model_path = file_paths[0];\n",
    "    #         print('Model has been found at: {}'.format(opt.model_path));\n",
    "    #         valid_path = True;\n",
    "\n",
    "    opt.model_name = \"quant_retrained_model_0.9compress_acc_82.9_{}\".format(genDataTimeStr());\n",
    "    # valid_model_name = False;\n",
    "    # while not valid_model_name:\n",
    "    #     model_name = input('Enter a name that will be used to save the quantized model model: ');\n",
    "    #     if model_name != '':\n",
    "    #         opt.model_name = model_name;\n",
    "    #         valid_model_name = True;\n",
    "    opt.split = 1;\n",
    "    opt.hasQuated = False;\n",
    "    trainer = Trainer(opt);\n",
    "\n",
    "    print('Testing performance of the provided model.....');\n",
    "    trainer.TestModel();\n",
    "\n",
    "    print('Quantization process is started.....');\n",
    "    trainer.QuantizeModel();\n",
    "    print('Quantization done');\n",
    "\n",
    "    print('Testing quantized model.');\n",
    "    trainer.TestModel(True);\n",
    "    print('Finished');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a95756a3-5cc4-4276-9272-a22ec24efaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of samples:325\n",
      "Testing performance of the provided model.....\n",
      "[5, 32, 7, 6, 6, 11, 11, 26, 14, 30, 54, 2]\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (5, 1, 15109)         45      679,905\n",
      "  BatchNorm2d-2     (5, 1, 15109)     (5, 1, 15109)         10            0\n",
      "         ReLu-3     (5, 1, 15109)     (5, 1, 15109)          0       75,545\n",
      "       Conv2d-4     (5, 1, 15109)     (32, 1, 7553)        800    6,042,400\n",
      "  BatchNorm2d-5     (32, 1, 7553)     (32, 1, 7553)         64            0\n",
      "         ReLu-6     (32, 1, 7553)     (32, 1, 7553)          0      241,696\n",
      "    MaxPool2d-7     (32, 1, 7553)      (32, 1, 151)          0      241,600\n",
      "      Permute-8      (32, 1, 151)      (1, 32, 151)          0            0\n",
      "       Conv2d-9      (1, 32, 151)      (7, 32, 151)         63      304,416\n",
      " BatchNorm2d-10      (7, 32, 151)      (7, 32, 151)         14            0\n",
      "        ReLu-11      (7, 32, 151)      (7, 32, 151)          0       33,824\n",
      "   MaxPool2d-12      (7, 32, 151)       (7, 16, 75)          0       33,600\n",
      "      Conv2d-13       (7, 16, 75)       (6, 16, 75)        378      453,600\n",
      " BatchNorm2d-14       (6, 16, 75)       (6, 16, 75)         12            0\n",
      "        ReLu-15       (6, 16, 75)       (6, 16, 75)          0        7,200\n",
      "      Conv2d-16       (6, 16, 75)       (6, 16, 75)        324      388,800\n",
      " BatchNorm2d-17       (6, 16, 75)       (6, 16, 75)         12            0\n",
      "        ReLu-18       (6, 16, 75)       (6, 16, 75)          0        7,200\n",
      "   MaxPool2d-19       (6, 16, 75)        (6, 8, 37)          0        7,104\n",
      "      Conv2d-20        (6, 8, 37)       (11, 8, 37)        594      175,824\n",
      " BatchNorm2d-21       (11, 8, 37)       (11, 8, 37)         22            0\n",
      "        ReLu-22       (11, 8, 37)       (11, 8, 37)          0        3,256\n",
      "      Conv2d-23       (11, 8, 37)       (11, 8, 37)      1,089      322,344\n",
      " BatchNorm2d-24       (11, 8, 37)       (11, 8, 37)         22            0\n",
      "        ReLu-25       (11, 8, 37)       (11, 8, 37)          0        3,256\n",
      "   MaxPool2d-26       (11, 8, 37)       (11, 4, 18)          0        3,168\n",
      "      Conv2d-27       (11, 4, 18)       (26, 4, 18)      2,574      185,328\n",
      " BatchNorm2d-28       (26, 4, 18)       (26, 4, 18)         52            0\n",
      "        ReLu-29       (26, 4, 18)       (26, 4, 18)          0        1,872\n",
      "      Conv2d-30       (26, 4, 18)       (14, 4, 18)      3,276      235,872\n",
      " BatchNorm2d-31       (14, 4, 18)       (14, 4, 18)         28            0\n",
      "        ReLu-32       (14, 4, 18)       (14, 4, 18)          0        1,008\n",
      "   MaxPool2d-33       (14, 4, 18)        (14, 2, 9)          0        1,008\n",
      "      Conv2d-34        (14, 2, 9)        (30, 2, 9)      3,780       68,040\n",
      " BatchNorm2d-35        (30, 2, 9)        (30, 2, 9)         60            0\n",
      "        ReLu-36        (30, 2, 9)        (30, 2, 9)          0          540\n",
      "      Conv2d-37        (30, 2, 9)        (54, 2, 9)     14,580      262,440\n",
      " BatchNorm2d-38        (54, 2, 9)        (54, 2, 9)        108            0\n",
      "        ReLu-39        (54, 2, 9)        (54, 2, 9)          0          972\n",
      "   MaxPool2d-40        (54, 2, 9)        (54, 1, 4)          0          864\n",
      "      Conv2d-41        (54, 1, 4)         (2, 1, 4)        108          432\n",
      " BatchNorm2d-42         (2, 1, 4)         (2, 1, 4)          4            0\n",
      "        ReLu-43         (2, 1, 4)         (2, 1, 4)          0            8\n",
      "   AvgPool2d-44         (2, 1, 4)         (2, 1, 1)          0            8\n",
      "     Flatten-45         (2, 1, 1)            (1, 2)          0            0\n",
      "      Linear-46            (1, 2)            (1, 2)          6            6\n",
      "     Softmax-47            (1, 2)            (1, 2)          0            2\n",
      "==============================================================================\n",
      "Total Params: 28,025\n",
      "Total FLOPs : 9,783,138\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 0.11\n",
      "Total size (MB) : 0.22\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([176, 2])\n",
      "Testing: Acc(top1) 82.95%\n",
      "Quantization process is started.....\n",
      "[5, 32, 7, 6, 6, 11, 11, 26, 14, 30, 54, 2]\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (5, 1, 15109)         45      679,905\n",
      "  BatchNorm2d-2     (5, 1, 15109)     (5, 1, 15109)         10            0\n",
      "         ReLu-3     (5, 1, 15109)     (5, 1, 15109)          0       75,545\n",
      "       Conv2d-4     (5, 1, 15109)     (32, 1, 7553)        800    6,042,400\n",
      "  BatchNorm2d-5     (32, 1, 7553)     (32, 1, 7553)         64            0\n",
      "         ReLu-6     (32, 1, 7553)     (32, 1, 7553)          0      241,696\n",
      "    MaxPool2d-7     (32, 1, 7553)      (32, 1, 151)          0      241,600\n",
      "      Permute-8      (32, 1, 151)      (1, 32, 151)          0            0\n",
      "       Conv2d-9      (1, 32, 151)      (7, 32, 151)         63      304,416\n",
      " BatchNorm2d-10      (7, 32, 151)      (7, 32, 151)         14            0\n",
      "        ReLu-11      (7, 32, 151)      (7, 32, 151)          0       33,824\n",
      "   MaxPool2d-12      (7, 32, 151)       (7, 16, 75)          0       33,600\n",
      "      Conv2d-13       (7, 16, 75)       (6, 16, 75)        378      453,600\n",
      " BatchNorm2d-14       (6, 16, 75)       (6, 16, 75)         12            0\n",
      "        ReLu-15       (6, 16, 75)       (6, 16, 75)          0        7,200\n",
      "      Conv2d-16       (6, 16, 75)       (6, 16, 75)        324      388,800\n",
      " BatchNorm2d-17       (6, 16, 75)       (6, 16, 75)         12            0\n",
      "        ReLu-18       (6, 16, 75)       (6, 16, 75)          0        7,200\n",
      "   MaxPool2d-19       (6, 16, 75)        (6, 8, 37)          0        7,104\n",
      "      Conv2d-20        (6, 8, 37)       (11, 8, 37)        594      175,824\n",
      " BatchNorm2d-21       (11, 8, 37)       (11, 8, 37)         22            0\n",
      "        ReLu-22       (11, 8, 37)       (11, 8, 37)          0        3,256\n",
      "      Conv2d-23       (11, 8, 37)       (11, 8, 37)      1,089      322,344\n",
      " BatchNorm2d-24       (11, 8, 37)       (11, 8, 37)         22            0\n",
      "        ReLu-25       (11, 8, 37)       (11, 8, 37)          0        3,256\n",
      "   MaxPool2d-26       (11, 8, 37)       (11, 4, 18)          0        3,168\n",
      "      Conv2d-27       (11, 4, 18)       (26, 4, 18)      2,574      185,328\n",
      " BatchNorm2d-28       (26, 4, 18)       (26, 4, 18)         52            0\n",
      "        ReLu-29       (26, 4, 18)       (26, 4, 18)          0        1,872\n",
      "      Conv2d-30       (26, 4, 18)       (14, 4, 18)      3,276      235,872\n",
      " BatchNorm2d-31       (14, 4, 18)       (14, 4, 18)         28            0\n",
      "        ReLu-32       (14, 4, 18)       (14, 4, 18)          0        1,008\n",
      "   MaxPool2d-33       (14, 4, 18)        (14, 2, 9)          0        1,008\n",
      "      Conv2d-34        (14, 2, 9)        (30, 2, 9)      3,780       68,040\n",
      " BatchNorm2d-35        (30, 2, 9)        (30, 2, 9)         60            0\n",
      "        ReLu-36        (30, 2, 9)        (30, 2, 9)          0          540\n",
      "      Conv2d-37        (30, 2, 9)        (54, 2, 9)     14,580      262,440\n",
      " BatchNorm2d-38        (54, 2, 9)        (54, 2, 9)        108            0\n",
      "        ReLu-39        (54, 2, 9)        (54, 2, 9)          0          972\n",
      "   MaxPool2d-40        (54, 2, 9)        (54, 1, 4)          0          864\n",
      "      Conv2d-41        (54, 1, 4)         (2, 1, 4)        108          432\n",
      " BatchNorm2d-42         (2, 1, 4)         (2, 1, 4)          4            0\n",
      "        ReLu-43         (2, 1, 4)         (2, 1, 4)          0            8\n",
      "   AvgPool2d-44         (2, 1, 4)         (2, 1, 1)          0            8\n",
      "     Flatten-45         (2, 1, 1)            (1, 2)          0            0\n",
      "      Linear-46            (1, 2)            (1, 2)          6            6\n",
      "     Softmax-47            (1, 2)            (1, 2)          0            2\n",
      "==============================================================================\n",
      "Total Params: 28,025\n",
      "Total FLOPs : 9,783,138\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 0.11\n",
      "Total size (MB) : 0.22\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=False){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "Preparing calibration dataset..\n",
      "Calibration dataset is ready\n",
      "calibrate accuracy is: 65.62\n",
      "Post Training Quantization: Convert done\n",
      "Size of model after quantization\n",
      "Size (MB): 0.04519\n",
      "torch.Size([176, 2])\n",
      "Testing: Acc(top1) 81.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (tinynn.converter.base) Generated model saved to ../th/quantized_models/quant_retrained_model_95.4_20240206092318.tflite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('sfeb.0.weight', tensor([[[[ 0.5228,  0.8887,  0.3659,  3.3195, -0.9671, -1.3592, -1.6205,\n",
      "           -0.4443, -0.1568]]],\n",
      "\n",
      "\n",
      "        [[[-0.1830,  0.2352,  0.7319, -1.0716,  0.4705, -3.2672, -1.3853,\n",
      "            1.6205,  2.6138]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8364,  0.1307,  0.0261, -1.2285, -0.1830, -1.0716,  0.8364,\n",
      "            0.1307, -2.0649]]],\n",
      "\n",
      "\n",
      "        [[[-0.0523,  0.2352, -0.7057,  0.2614,  0.0523,  1.0194,  2.7183,\n",
      "           -1.9342, -1.6728]]],\n",
      "\n",
      "\n",
      "        [[[-0.5489, -0.7841, -0.1307,  0.8625,  0.2875,  0.2352, -1.0978,\n",
      "            0.3659,  2.2740]]]], size=(5, 1, 1, 9), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.026137681677937508,\n",
      "       zero_point=0)), ('sfeb.0.bias', Parameter containing:\n",
      "tensor([0.0388, 0.0782, 0.0408, 0.1859, 0.0744])), ('sfeb.0.scale', tensor(0.0283)), ('sfeb.0.zero_point', tensor(0)), ('sfeb.3.weight', tensor([[[[-0.2201, -0.0734, -0.1141,  0.0000,  0.0082]],\n",
      "\n",
      "         [[-0.0489, -0.0734, -0.0326, -0.0897,  0.0163]],\n",
      "\n",
      "         [[ 0.0734,  0.1712,  0.2527,  0.4727,  0.3505]],\n",
      "\n",
      "         [[ 0.0245, -0.0082,  0.0000,  0.0978, -0.0571]],\n",
      "\n",
      "         [[-0.0978,  0.0163,  0.0082,  0.0571,  0.1060]]],\n",
      "\n",
      "\n",
      "        [[[-0.2445, -0.2527, -0.1386, -0.1467, -0.0652]],\n",
      "\n",
      "         [[-0.0978, -0.1141, -0.0245,  0.0571, -0.1060]],\n",
      "\n",
      "         [[ 0.4727,  0.5542,  0.4564,  0.5705,  0.4564]],\n",
      "\n",
      "         [[-0.0082,  0.1549,  0.0000,  0.0000, -0.1386]],\n",
      "\n",
      "         [[-0.4401, -0.3586, -0.4157, -0.3016, -0.4157]]],\n",
      "\n",
      "\n",
      "        [[[-0.1630, -0.1630, -0.2445, -0.1386, -0.1386]],\n",
      "\n",
      "         [[ 0.0163, -0.0408, -0.0082,  0.0000,  0.0652]],\n",
      "\n",
      "         [[ 0.2690,  0.3505,  0.2934,  0.3831,  0.3505]],\n",
      "\n",
      "         [[-0.0408,  0.0245,  0.0815,  0.0489, -0.0489]],\n",
      "\n",
      "         [[-0.3097, -0.3505, -0.2038, -0.1386, -0.2445]]],\n",
      "\n",
      "\n",
      "        [[[-0.0489,  0.0082,  0.0652, -0.1060, -0.2364]],\n",
      "\n",
      "         [[-0.3831, -0.1793, -0.1549, -0.1956, -0.1956]],\n",
      "\n",
      "         [[-1.0433, -0.7662, -0.5053, -0.3831, -0.3179]],\n",
      "\n",
      "         [[ 0.0734, -0.0082,  0.0082, -0.0734, -0.0571]],\n",
      "\n",
      "         [[ 0.4320,  0.5461,  0.4646,  0.5950,  0.6358]]],\n",
      "\n",
      "\n",
      "        [[[-0.0082, -0.0326, -0.0978, -0.0734, -0.0571]],\n",
      "\n",
      "         [[-0.0489,  0.0000, -0.0408, -0.1304, -0.0571]],\n",
      "\n",
      "         [[ 0.0245,  0.0897,  0.0652,  0.0897,  0.0897]],\n",
      "\n",
      "         [[-0.0408,  0.0000,  0.0000,  0.0000, -0.0408]],\n",
      "\n",
      "         [[ 0.0000, -0.0082, -0.0408, -0.0163,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2201,  0.1549, -0.0815,  0.0163,  0.1549]],\n",
      "\n",
      "         [[-0.2282, -0.2282, -0.0734, -0.1304, -0.3260]],\n",
      "\n",
      "         [[-0.1875, -0.1304, -0.1549,  0.0408,  0.1793]],\n",
      "\n",
      "         [[ 0.0326,  0.0652,  0.0652,  0.1793,  0.2445]],\n",
      "\n",
      "         [[-0.0897, -0.2445, -0.3749, -0.2527, -0.1549]]],\n",
      "\n",
      "\n",
      "        [[[-0.1223, -0.2934, -0.4564, -0.2445, -0.2038]],\n",
      "\n",
      "         [[ 0.0815, -0.1223,  0.0163,  0.1304,  0.0326]],\n",
      "\n",
      "         [[-0.1793,  0.0163, -0.1549, -0.0489,  0.1630]],\n",
      "\n",
      "         [[ 0.0000, -0.2038, -0.1793, -0.1712, -0.2527]],\n",
      "\n",
      "         [[-0.0408,  0.1875,  0.0734,  0.3586,  0.4727]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1630,  0.3260,  0.1956,  0.0489,  0.0000]],\n",
      "\n",
      "         [[-0.2771, -0.3179, -0.2771, -0.2608, -0.2608]],\n",
      "\n",
      "         [[-0.2771,  0.0245,  0.2934,  0.4157,  0.4157]],\n",
      "\n",
      "         [[ 0.1304,  0.2201,  0.1467,  0.1141, -0.1875]],\n",
      "\n",
      "         [[ 0.0245, -0.1304, -0.2038, -0.1956, -0.2364]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0163, -0.0652, -0.0408, -0.0815, -0.0897]],\n",
      "\n",
      "         [[-0.0734, -0.0408, -0.0897,  0.0163, -0.0082]],\n",
      "\n",
      "         [[ 0.0000, -0.0082, -0.0815, -0.1141, -0.1141]],\n",
      "\n",
      "         [[ 0.0000,  0.0163, -0.0163, -0.0082, -0.0734]],\n",
      "\n",
      "         [[-0.0815, -0.0815, -0.0245,  0.0489,  0.1467]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0734,  0.0163, -0.0489, -0.0897, -0.0163]],\n",
      "\n",
      "         [[ 0.0408,  0.0163,  0.0082,  0.0489,  0.0571]],\n",
      "\n",
      "         [[-0.0571, -0.0245,  0.0489,  0.0408, -0.0082]],\n",
      "\n",
      "         [[-0.0082,  0.0163,  0.0652,  0.0000,  0.0326]],\n",
      "\n",
      "         [[ 0.0000,  0.0326, -0.0245, -0.0082,  0.0489]]],\n",
      "\n",
      "\n",
      "        [[[-0.2527, -0.0652, -0.0897,  0.1875,  0.1223]],\n",
      "\n",
      "         [[-0.1467, -0.1467, -0.2201, -0.2119, -0.2282]],\n",
      "\n",
      "         [[-0.0408,  0.0652, -0.0815, -0.0897, -0.0326]],\n",
      "\n",
      "         [[-0.0082, -0.1793, -0.2690,  0.0571,  0.1304]],\n",
      "\n",
      "         [[-0.0082,  0.0408,  0.0897,  0.0000, -0.0082]]],\n",
      "\n",
      "\n",
      "        [[[-0.0978, -0.1141,  0.1386, -0.3260,  0.0978]],\n",
      "\n",
      "         [[ 0.0326, -0.1630,  0.0734,  0.0163, -0.2771]],\n",
      "\n",
      "         [[-0.0652,  0.0082,  0.3668, -0.3016,  0.1875]],\n",
      "\n",
      "         [[-0.1712,  0.3586, -0.0408, -0.7743,  0.4890]],\n",
      "\n",
      "         [[-0.1060,  0.0815, -0.3831,  0.2853, -0.1467]]],\n",
      "\n",
      "\n",
      "        [[[-0.0082,  0.0489, -0.2690,  0.1141, -0.1549]],\n",
      "\n",
      "         [[-0.2608,  0.0652, -0.0408, -0.3016,  0.0163]],\n",
      "\n",
      "         [[-0.1304,  0.1630, -0.3668,  0.2527, -0.0652]],\n",
      "\n",
      "         [[ 0.1549, -0.2282, -0.3423,  0.4401, -0.3342]],\n",
      "\n",
      "         [[-0.0082, -0.2690,  0.3668, -0.3016,  0.1956]]],\n",
      "\n",
      "\n",
      "        [[[-0.1467,  0.1141, -0.1467,  0.1712, -0.1793]],\n",
      "\n",
      "         [[-0.2853,  0.1712, -0.2608,  0.1386, -0.0897]],\n",
      "\n",
      "         [[ 0.0326, -0.0245, -0.0163,  0.0408,  0.0000]],\n",
      "\n",
      "         [[ 0.2119, -0.3097,  0.0815, -0.1467,  0.0489]],\n",
      "\n",
      "         [[-0.1304,  0.1141, -0.0734,  0.0326,  0.0489]]],\n",
      "\n",
      "\n",
      "        [[[-0.0326, -0.0734,  0.0000, -0.0734,  0.0082]],\n",
      "\n",
      "         [[ 0.0408, -0.1060, -0.0082,  0.1060, -0.0897]],\n",
      "\n",
      "         [[ 0.0408,  0.0571, -0.0489,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0734,  0.0571, -0.0815, -0.0489,  0.0571]],\n",
      "\n",
      "         [[-0.0245, -0.0815,  0.0082,  0.0163,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0326,  0.1141, -0.0897, -0.0408,  0.1060]],\n",
      "\n",
      "         [[-0.2038,  0.0163, -0.0734, -0.1223,  0.0163]],\n",
      "\n",
      "         [[-0.0489, -0.0978, -0.0245,  0.0000,  0.0082]],\n",
      "\n",
      "         [[ 0.0489, -0.2282, -0.2038,  0.2364, -0.0082]],\n",
      "\n",
      "         [[-0.0082,  0.0408, -0.0163, -0.0571,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0326,  0.0082,  0.0408, -0.0652,  0.0326]],\n",
      "\n",
      "         [[-0.1386,  0.0000,  0.0652, -0.1630,  0.0082]],\n",
      "\n",
      "         [[ 0.0082,  0.0000, -0.0489, -0.0326, -0.0082]],\n",
      "\n",
      "         [[-0.0897, -0.0326, -0.2364,  0.0245,  0.2608]],\n",
      "\n",
      "         [[-0.0815, -0.0082,  0.0652, -0.0408, -0.0815]]],\n",
      "\n",
      "\n",
      "        [[[-0.1712,  0.0082,  0.0571, -0.1304, -0.0571]],\n",
      "\n",
      "         [[-0.3423, -0.0652,  0.1956, -0.0082, -0.3097]],\n",
      "\n",
      "         [[-0.0408,  0.0245,  0.0000,  0.0571,  0.0326]],\n",
      "\n",
      "         [[ 0.0571,  0.0082, -0.2934,  0.1712,  0.0815]],\n",
      "\n",
      "         [[-0.0245,  0.0489,  0.0897, -0.0652, -0.0652]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0734, -0.0163, -0.1223,  0.0326,  0.2445]],\n",
      "\n",
      "         [[-0.0897,  0.0326, -0.3342,  0.0245,  0.1060]],\n",
      "\n",
      "         [[ 0.0408, -0.0978,  0.0082,  0.0082,  0.1060]],\n",
      "\n",
      "         [[-0.0571, -0.3668,  0.0897,  0.2445,  0.0326]],\n",
      "\n",
      "         [[-0.1060, -0.0082, -0.0815, -0.0897,  0.0082]]],\n",
      "\n",
      "\n",
      "        [[[-0.1956, -0.2771,  0.0815,  0.1875, -0.0897]],\n",
      "\n",
      "         [[-0.0815, -0.3179, -0.2771,  0.0489,  0.3423]],\n",
      "\n",
      "         [[ 0.0245, -0.0163,  0.0897,  0.0000, -0.0652]],\n",
      "\n",
      "         [[-0.0326,  0.1793,  0.1060, -0.1630, -0.3097]],\n",
      "\n",
      "         [[ 0.1712,  0.0815, -0.1549, -0.0163,  0.1223]]],\n",
      "\n",
      "\n",
      "        [[[-0.0408, -0.0408, -0.0897, -0.0571,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.1386, -0.1141, -0.1304, -0.1304]],\n",
      "\n",
      "         [[-0.0815, -0.0734, -0.0082, -0.0082,  0.0408]],\n",
      "\n",
      "         [[-0.0326, -0.0489,  0.0082,  0.0408,  0.0652]],\n",
      "\n",
      "         [[-0.0326, -0.0326, -0.0163, -0.0408,  0.0082]]],\n",
      "\n",
      "\n",
      "        [[[-0.0163,  0.0245, -0.1223, -0.0815, -0.0652]],\n",
      "\n",
      "         [[-0.1630, -0.0571, -0.0408,  0.0000, -0.0082]],\n",
      "\n",
      "         [[-0.0245, -0.0326, -0.0245,  0.0000,  0.0245]],\n",
      "\n",
      "         [[ 0.0734, -0.0571, -0.0245, -0.1223, -0.1304]],\n",
      "\n",
      "         [[-0.0163, -0.0571,  0.0082,  0.0652,  0.0571]]],\n",
      "\n",
      "\n",
      "        [[[-0.0652,  0.1956,  0.1141, -0.1793, -0.3097]],\n",
      "\n",
      "         [[-0.2201, -0.2038, -0.1875,  0.1875,  0.0326]],\n",
      "\n",
      "         [[-0.1060,  0.1141,  0.0734,  0.0082, -0.0326]],\n",
      "\n",
      "         [[ 0.0978,  0.1467, -0.0571, -0.2038, -0.1712]],\n",
      "\n",
      "         [[-0.2119, -0.1712, -0.0978,  0.0652,  0.2527]]],\n",
      "\n",
      "\n",
      "        [[[-0.2690, -0.0245,  0.1386,  0.1304, -0.2608]],\n",
      "\n",
      "         [[-0.2364, -0.3097, -0.2282, -0.1386,  0.1956]],\n",
      "\n",
      "         [[-0.1223, -0.0734,  0.1060,  0.1467, -0.0245]],\n",
      "\n",
      "         [[ 0.0815,  0.0408,  0.0489, -0.0734, -0.2690]],\n",
      "\n",
      "         [[ 0.1060, -0.0897,  0.0082,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.1223, -0.1386, -0.0652, -0.0815, -0.1956]],\n",
      "\n",
      "         [[-0.0326,  0.2119, -0.0978, -0.1141, -0.0815]],\n",
      "\n",
      "         [[-0.0326, -0.0489, -0.0734,  0.0326, -0.0245]],\n",
      "\n",
      "         [[-0.0326, -0.1060, -0.0652, -0.1304, -0.1956]],\n",
      "\n",
      "         [[ 0.0082,  0.0326, -0.0326,  0.0082,  0.0978]]],\n",
      "\n",
      "\n",
      "        [[[-0.1386, -0.1549, -0.2038,  0.0245,  0.3831]],\n",
      "\n",
      "         [[ 0.0897,  0.3016, -0.1630, -0.3097, -0.0408]],\n",
      "\n",
      "         [[ 0.0082, -0.1060, -0.0245,  0.0163, -0.0163]],\n",
      "\n",
      "         [[-0.1712, -0.1956, -0.0082,  0.2364,  0.3831]],\n",
      "\n",
      "         [[ 0.1141,  0.4890, -0.0571, -0.1223, -0.2608]]],\n",
      "\n",
      "\n",
      "        [[[-0.4401, -0.3586,  0.1956,  0.2364,  0.4320]],\n",
      "\n",
      "         [[-0.1875, -0.0734, -0.3260, -0.2201, -0.0978]],\n",
      "\n",
      "         [[-0.1630, -0.3097,  0.0000,  0.1712,  0.3994]],\n",
      "\n",
      "         [[-0.0734, -0.1712,  0.0408,  0.2038,  0.2608]],\n",
      "\n",
      "         [[ 0.2445,  0.3097,  0.0326, -0.0978, -0.2608]]],\n",
      "\n",
      "\n",
      "        [[[-0.0734, -0.0652, -0.0815, -0.0897, -0.0815]],\n",
      "\n",
      "         [[-0.0408, -0.0978, -0.0163,  0.0245, -0.0082]],\n",
      "\n",
      "         [[-0.0326, -0.0326, -0.0408, -0.0408, -0.0408]],\n",
      "\n",
      "         [[-0.0326, -0.0082, -0.0163, -0.0326, -0.0897]],\n",
      "\n",
      "         [[ 0.0000, -0.0326, -0.0326,  0.0000,  0.0571]]],\n",
      "\n",
      "\n",
      "        [[[-0.2119, -0.3505, -0.3586, -0.1304, -0.2608]],\n",
      "\n",
      "         [[ 0.1630,  0.1141,  0.1304,  0.1386, -0.1467]],\n",
      "\n",
      "         [[ 0.0408,  0.1223, -0.1141, -0.1875, -0.1304]],\n",
      "\n",
      "         [[-0.1223, -0.1793, -0.1141, -0.2038, -0.2038]],\n",
      "\n",
      "         [[ 0.1549,  0.1304,  0.0408,  0.2934,  0.4564]]],\n",
      "\n",
      "\n",
      "        [[[-0.1630, -0.2364, -0.1630, -0.1467, -0.1141]],\n",
      "\n",
      "         [[-0.1223, -0.0408, -0.0082,  0.0408, -0.1060]],\n",
      "\n",
      "         [[-0.0408, -0.0082, -0.0897,  0.0000, -0.1304]],\n",
      "\n",
      "         [[-0.0571, -0.0897,  0.0734,  0.0000, -0.1304]],\n",
      "\n",
      "         [[ 0.1875,  0.1712,  0.0571,  0.1386,  0.2201]]],\n",
      "\n",
      "\n",
      "        [[[-0.1304, -0.1793, -0.2445, -0.1060, -0.1060]],\n",
      "\n",
      "         [[-0.0082, -0.1060,  0.1223,  0.0734,  0.1060]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.1141, -0.1304, -0.1712]],\n",
      "\n",
      "         [[-0.1223,  0.0000, -0.0652, -0.0082, -0.2201]],\n",
      "\n",
      "         [[-0.0734, -0.1304, -0.0734, -0.1304,  0.1304]]],\n",
      "\n",
      "\n",
      "        [[[-0.0082, -0.0082, -0.0245, -0.0082,  0.0000]],\n",
      "\n",
      "         [[ 0.3016,  0.1712,  0.0000, -0.0163,  0.0000]],\n",
      "\n",
      "         [[-0.0652, -0.1304, -0.0815, -0.0082, -0.1956]],\n",
      "\n",
      "         [[ 0.1223, -0.0163, -0.0571,  0.0082, -0.1793]],\n",
      "\n",
      "         [[-0.0082, -0.0815,  0.0000, -0.1223,  0.0082]]]], size=(32, 5, 1, 5),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.008150692097842693, zero_point=0)), ('sfeb.3.bias', Parameter containing:\n",
      "tensor([-0.1491,  0.2208,  0.1236,  0.2514,  0.1153,  0.1293,  0.2678,  0.0172,\n",
      "         0.1226, -0.0020,  0.2560,  0.1871,  0.2772,  0.1731,  0.1584,  0.1423,\n",
      "         0.1337,  0.1346,  0.1024,  0.1510,  0.1389,  0.1205,  0.1440,  0.1831,\n",
      "         0.1638,  0.0139,  0.0920,  0.1433,  0.2125,  0.1074,  0.2689,  0.0810])), ('sfeb.3.scale', tensor(0.0328)), ('sfeb.3.zero_point', tensor(0)), ('tfeb.0.weight', tensor([[[[-0.0060, -0.0060, -0.0060],\n",
      "          [-0.0119, -0.0119, -0.0119],\n",
      "          [ 0.0000,  0.0715,  0.0060]]],\n",
      "\n",
      "\n",
      "        [[[-0.0060, -0.0060, -0.0060],\n",
      "          [-0.0060, -0.0060, -0.0060],\n",
      "          [ 0.0358,  0.0656,  0.0417]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2325,  0.1431,  0.1014],\n",
      "          [-0.0775, -0.2206, -0.1073],\n",
      "          [-0.2564, -0.1669,  0.0119]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0358,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0298,  0.0477,  0.0060]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0298,  0.0954,  0.0715],\n",
      "          [-0.0537,  0.2444,  0.0238],\n",
      "          [-0.0537,  0.1192,  0.0238]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1789, -0.7631,  0.3816],\n",
      "          [ 0.1610, -0.2146,  0.0000],\n",
      "          [-0.1610, -0.0775, -0.0715]]],\n",
      "\n",
      "\n",
      "        [[[-0.0835, -0.0537,  0.1192],\n",
      "          [ 0.0119,  0.0119,  0.0119],\n",
      "          [ 0.0060,  0.0000,  0.0000]]]], size=(7, 1, 3, 3), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.005962006282061338,\n",
      "       zero_point=0)), ('tfeb.0.bias', Parameter containing:\n",
      "tensor([-0.0676, -0.1134,  0.1348, -0.1274, -0.0208,  0.1428, -0.0377])), ('tfeb.0.scale', tensor(0.0144)), ('tfeb.0.zero_point', tensor(0)), ('tfeb.4.weight', tensor([[[[ 0.0055,  0.0111,  0.0221],\n",
      "          [ 0.0111,  0.0166,  0.0221],\n",
      "          [-0.0111, -0.0055,  0.0000]],\n",
      "\n",
      "         [[ 0.0221,  0.0277,  0.0387],\n",
      "          [ 0.0277,  0.0332,  0.0387],\n",
      "          [-0.0166, -0.0111, -0.0055]],\n",
      "\n",
      "         [[-0.0609, -0.0332, -0.0055],\n",
      "          [ 0.0277,  0.0387,  0.0553],\n",
      "          [ 0.0387,  0.0498,  0.0609]],\n",
      "\n",
      "         [[ 0.0166,  0.0221,  0.0332],\n",
      "          [ 0.0221,  0.0277,  0.0277],\n",
      "          [-0.0055,  0.0000,  0.0055]],\n",
      "\n",
      "         [[-0.0996,  0.0609,  0.1162],\n",
      "          [-0.0443,  0.0885,  0.1273],\n",
      "          [ 0.0387,  0.0387,  0.0719]],\n",
      "\n",
      "         [[-0.0221,  0.1550,  0.1162],\n",
      "          [-0.0166,  0.1051,  0.0277],\n",
      "          [ 0.0111,  0.0277,  0.0553]],\n",
      "\n",
      "         [[-0.0055,  0.0055,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0055],\n",
      "          [ 0.0000,  0.0055,  0.0055]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0111,  0.0000],\n",
      "          [ 0.0000, -0.0111,  0.0000],\n",
      "          [ 0.0000, -0.0055,  0.0000]],\n",
      "\n",
      "         [[-0.0111, -0.0166, -0.0055],\n",
      "          [-0.0111, -0.0111, -0.0055],\n",
      "          [ 0.0000,  0.0000,  0.0055]],\n",
      "\n",
      "         [[-0.0166, -0.0166, -0.0111],\n",
      "          [-0.1162, -0.0387,  0.0055],\n",
      "          [-0.0111, -0.0111,  0.0885]],\n",
      "\n",
      "         [[-0.0055, -0.0111, -0.0055],\n",
      "          [-0.0055, -0.0111, -0.0055],\n",
      "          [ 0.0000, -0.0055,  0.0000]],\n",
      "\n",
      "         [[-0.2324, -0.0664, -0.0277],\n",
      "          [-0.1494, -0.0830,  0.1051],\n",
      "          [-0.1384, -0.0885,  0.0498]],\n",
      "\n",
      "         [[-0.1273,  0.0055, -0.1605],\n",
      "          [-0.0941, -0.0664, -0.1162],\n",
      "          [-0.0775,  0.0332, -0.0609]],\n",
      "\n",
      "         [[ 0.0000, -0.0055,  0.0000],\n",
      "          [-0.0055, -0.0055,  0.0000],\n",
      "          [-0.0055, -0.0055, -0.0055]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0055, -0.0055],\n",
      "          [-0.0055, -0.0055, -0.0055],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0055, -0.0055, -0.0055],\n",
      "          [-0.0055, -0.0111, -0.0055],\n",
      "          [ 0.0000, -0.0055,  0.0000]],\n",
      "\n",
      "         [[-0.0111, -0.0111, -0.0055],\n",
      "          [-0.0055,  0.0000, -0.0111],\n",
      "          [ 0.0885,  0.1107,  0.0885]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0055, -0.0055, -0.0055],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0166, -0.0277, -0.1107],\n",
      "          [-0.0553, -0.0719, -0.0553],\n",
      "          [-0.0387, -0.0443, -0.0277]],\n",
      "\n",
      "         [[ 0.0000, -0.0055,  0.0000],\n",
      "          [-0.0055, -0.0166, -0.0221],\n",
      "          [-0.0498, -0.0498, -0.0387]],\n",
      "\n",
      "         [[ 0.0000, -0.0055,  0.0000],\n",
      "          [ 0.0000, -0.0055, -0.0055],\n",
      "          [-0.0111, -0.0111, -0.0055]]],\n",
      "\n",
      "\n",
      "        [[[-0.0055,  0.0000, -0.0055],\n",
      "          [ 0.0055,  0.0055,  0.0111],\n",
      "          [ 0.0055,  0.0055,  0.0055]],\n",
      "\n",
      "         [[ 0.0000,  0.0055, -0.0055],\n",
      "          [ 0.0221,  0.0221,  0.0277],\n",
      "          [ 0.0055,  0.0111,  0.0111]],\n",
      "\n",
      "         [[ 0.7028,  0.0664, -0.5645],\n",
      "          [-0.5036, -0.1716,  0.5091],\n",
      "          [ 0.2878,  0.1882, -0.0221]],\n",
      "\n",
      "         [[-0.0055,  0.0000, -0.0609],\n",
      "          [ 0.0111,  0.0111,  0.0111],\n",
      "          [ 0.0000,  0.0055,  0.0055]],\n",
      "\n",
      "         [[-0.1716, -0.0277,  0.1660],\n",
      "          [-0.3763, -0.0111,  0.3818],\n",
      "          [ 0.2048,  0.0775,  0.0885]],\n",
      "\n",
      "         [[ 0.1882,  0.1605, -0.1660],\n",
      "          [ 0.0166, -0.0055,  0.0055],\n",
      "          [-0.0553, -0.0719, -0.0277]],\n",
      "\n",
      "         [[-0.0055, -0.0055,  0.0055],\n",
      "          [ 0.0000,  0.0000, -0.0055],\n",
      "          [-0.0111, -0.0111,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0166, -0.0221, -0.0111],\n",
      "          [-0.0111, -0.0166, -0.0055],\n",
      "          [ 0.0055,  0.0000,  0.0055]],\n",
      "\n",
      "         [[-0.0166, -0.0221, -0.0166],\n",
      "          [-0.0166, -0.0221, -0.0111],\n",
      "          [ 0.0055,  0.0000,  0.0055]],\n",
      "\n",
      "         [[-0.0111, -0.0111, -0.0055],\n",
      "          [ 0.0443,  0.0498,  0.0609],\n",
      "          [ 0.1051,  0.1162,  0.1217]],\n",
      "\n",
      "         [[-0.0166, -0.0221, -0.0166],\n",
      "          [-0.0166, -0.0221, -0.0166],\n",
      "          [ 0.0000, -0.0055,  0.0000]],\n",
      "\n",
      "         [[-0.0277, -0.0498, -0.0111],\n",
      "          [-0.0719,  0.0055, -0.0664],\n",
      "          [-0.1107, -0.0498, -0.0996]],\n",
      "\n",
      "         [[-0.0221, -0.0166, -0.0166],\n",
      "          [-0.0609, -0.0609, -0.0553],\n",
      "          [-0.0221, -0.0221, -0.0111]],\n",
      "\n",
      "         [[-0.0166, -0.0166, -0.0111],\n",
      "          [-0.0277, -0.0277, -0.0221],\n",
      "          [-0.0221, -0.0221, -0.0111]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0111,  0.0055,  0.0055],\n",
      "          [-0.0055, -0.0055, -0.0055],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0111,  0.0055,  0.0111],\n",
      "          [-0.0221, -0.0221, -0.0111],\n",
      "          [-0.0055, -0.0055, -0.0055]],\n",
      "\n",
      "         [[ 0.0609,  0.0609, -0.0166],\n",
      "          [ 0.0553,  0.0609,  0.1217],\n",
      "          [-0.0885, -0.0775,  0.0830]],\n",
      "\n",
      "         [[ 0.0055,  0.0000,  0.0000],\n",
      "          [-0.0055, -0.0111, -0.0055],\n",
      "          [-0.0055, -0.0055, -0.0055]],\n",
      "\n",
      "         [[ 0.0387,  0.0387,  0.3265],\n",
      "          [-0.1882, -0.1273,  0.2380],\n",
      "          [-0.1273, -0.0664,  0.2048]],\n",
      "\n",
      "         [[-0.0221, -0.0443, -0.2048],\n",
      "          [-0.0111, -0.0387, -0.1273],\n",
      "          [ 0.0166, -0.0664, -0.1328]],\n",
      "\n",
      "         [[-0.0055, -0.0111, -0.0111],\n",
      "          [-0.0055, -0.0111, -0.0111],\n",
      "          [ 0.0000, -0.0055,  0.0000]]]], size=(6, 7, 3, 3), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0055340491235256195,\n",
      "       zero_point=0)), ('tfeb.4.bias', Parameter containing:\n",
      "tensor([-0.2252,  0.2322, -0.1583, -0.0732,  0.0179, -0.0283])), ('tfeb.4.scale', tensor(0.0082)), ('tfeb.4.zero_point', tensor(0)), ('tfeb.7.weight', tensor([[[[ 0.0291,  0.0218,  0.1018],\n",
      "          [ 0.0000,  0.0145,  0.1091],\n",
      "          [ 0.0000, -0.0073,  0.0872]],\n",
      "\n",
      "         [[-0.0509,  0.0218,  0.0218],\n",
      "          [ 0.1818,  0.2181, -0.0436],\n",
      "          [-0.0582, -0.0436, -0.0727]],\n",
      "\n",
      "         [[-0.0291, -0.0509, -0.0654],\n",
      "          [-0.0436, -0.0654, -0.0727],\n",
      "          [-0.0509, -0.0436, -0.0291]],\n",
      "\n",
      "         [[-0.1745,  0.0291,  0.2763],\n",
      "          [-0.1454,  0.0000,  0.4435],\n",
      "          [-0.1527, -0.0800, -0.0073]],\n",
      "\n",
      "         [[-0.0436, -0.0436, -0.0218],\n",
      "          [-0.1091, -0.1163, -0.0800],\n",
      "          [-0.0872, -0.0727, -0.0509]],\n",
      "\n",
      "         [[ 0.0509,  0.2617,  0.0509],\n",
      "          [-0.0291,  0.3490,  0.1963],\n",
      "          [-0.0509,  0.0436,  0.0654]]],\n",
      "\n",
      "\n",
      "        [[[-0.0145, -0.0364,  0.0000],\n",
      "          [-0.0364, -0.0436, -0.0073],\n",
      "          [ 0.0073, -0.0218,  0.0000]],\n",
      "\n",
      "         [[-0.0800, -0.2036,  0.0727],\n",
      "          [ 0.2835, -0.2108, -0.0291],\n",
      "          [ 0.0509, -0.0436,  0.1963]],\n",
      "\n",
      "         [[-0.0291, -0.0291, -0.0073],\n",
      "          [ 0.0145,  0.0000,  0.0000],\n",
      "          [-0.0291, -0.0218, -0.0145]],\n",
      "\n",
      "         [[-0.1672, -0.1091, -0.3781],\n",
      "          [-0.0872, -0.5744, -0.1381],\n",
      "          [-0.3563,  0.0800, -0.1527]],\n",
      "\n",
      "         [[ 0.0436,  0.0218,  0.0291],\n",
      "          [-0.0145, -0.0291, -0.0145],\n",
      "          [-0.0218, -0.0218, -0.0218]],\n",
      "\n",
      "         [[ 0.3490,  0.1890,  0.1745],\n",
      "          [ 0.0000,  0.0582,  0.0145],\n",
      "          [-0.0582, -0.1018, -0.1381]]],\n",
      "\n",
      "\n",
      "        [[[-0.0218, -0.0218, -0.0073],\n",
      "          [-0.0291, -0.0291, -0.0073],\n",
      "          [-0.0436, -0.0654, -0.0364]],\n",
      "\n",
      "         [[ 0.0145, -0.0727,  0.0000],\n",
      "          [ 0.0872, -0.2108, -0.0073],\n",
      "          [ 0.1309,  0.0291,  0.0364]],\n",
      "\n",
      "         [[-0.1236, -0.0218, -0.0218],\n",
      "          [-0.0145, -0.0218, -0.0291],\n",
      "          [-0.0218, -0.0291, -0.0291]],\n",
      "\n",
      "         [[-0.0073, -0.1454, -0.0727],\n",
      "          [-0.1600, -0.1309, -0.2254],\n",
      "          [-0.0291, -0.1091, -0.2545]],\n",
      "\n",
      "         [[-0.0582, -0.0654, -0.1018],\n",
      "          [-0.0509, -0.0582, -0.0800],\n",
      "          [-0.0218, -0.0364, -0.0509]],\n",
      "\n",
      "         [[-0.2327, -0.1236, -0.0582],\n",
      "          [ 0.0800, -0.0945, -0.0436],\n",
      "          [-0.1091,  0.0073, -0.0800]]],\n",
      "\n",
      "\n",
      "        [[[-0.0436, -0.0291,  0.0436],\n",
      "          [-0.1309, -0.1163, -0.0582],\n",
      "          [-0.1091, -0.1018, -0.0509]],\n",
      "\n",
      "         [[-0.0727, -0.0291, -0.0145],\n",
      "          [-0.0582,  0.0000, -0.0218],\n",
      "          [-0.0364,  0.0073, -0.0073]],\n",
      "\n",
      "         [[ 0.0218, -0.0073, -0.0218],\n",
      "          [ 0.0582,  0.0218,  0.0073],\n",
      "          [ 0.1745,  0.1672,  0.1672]],\n",
      "\n",
      "         [[-0.1018, -0.0436, -0.0872],\n",
      "          [ 0.1091,  0.1600, -0.0145],\n",
      "          [ 0.2399,  0.2617,  0.1236]],\n",
      "\n",
      "         [[ 0.0364, -0.0073,  0.0218],\n",
      "          [ 0.0727,  0.0145,  0.0218],\n",
      "          [ 0.2545,  0.2181,  0.2181]],\n",
      "\n",
      "         [[ 0.0291,  0.0727,  0.0945],\n",
      "          [-0.1890, -0.3199, -0.0800],\n",
      "          [-0.1381, -0.0727, -0.0436]]],\n",
      "\n",
      "\n",
      "        [[[-0.0218, -0.2254, -0.1890],\n",
      "          [-0.0291, -0.1818, -0.2181],\n",
      "          [-0.0218, -0.3490, -0.1527]],\n",
      "\n",
      "         [[-0.1600,  0.2399, -0.0218],\n",
      "          [-0.7125,  0.5816, -0.1236],\n",
      "          [-0.3344,  0.0291, -0.2108]],\n",
      "\n",
      "         [[ 0.1236,  0.1236,  0.1091],\n",
      "          [-0.2254, -0.0582, -0.0800],\n",
      "          [-0.0291, -0.0364, -0.0436]],\n",
      "\n",
      "         [[ 0.1091, -0.2472,  0.7998],\n",
      "          [-0.6834,  0.1818, -0.3999],\n",
      "          [ 0.4290, -0.9306,  0.3853]],\n",
      "\n",
      "         [[ 0.1381,  0.1600,  0.1818],\n",
      "          [ 0.1236,  0.1236,  0.1381],\n",
      "          [-0.0945, -0.1018, -0.0800]],\n",
      "\n",
      "         [[-0.2472,  0.2617, -0.2690],\n",
      "          [ 0.0800,  0.3635,  0.1672],\n",
      "          [ 0.1018,  0.1672, -0.1381]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0073,  0.0000,  0.0000],\n",
      "          [-0.0291, -0.0291, -0.0073],\n",
      "          [-0.0291, -0.0291,  0.0000]],\n",
      "\n",
      "         [[-0.1600, -0.1236, -0.1672],\n",
      "          [ 0.0436,  0.0073,  0.0145],\n",
      "          [ 0.0000, -0.0291, -0.0509]],\n",
      "\n",
      "         [[ 0.0218,  0.0145, -0.1381],\n",
      "          [ 0.0000, -0.0073, -0.0218],\n",
      "          [-0.0145, -0.0145, -0.0218]],\n",
      "\n",
      "         [[-0.0872, -0.3563, -0.1600],\n",
      "          [-0.1600, -0.1091, -0.1236],\n",
      "          [-0.1672, -0.1091, -0.1236]],\n",
      "\n",
      "         [[-0.0364, -0.0364, -0.0073],\n",
      "          [-0.0291, -0.0364, -0.0218],\n",
      "          [-0.0291, -0.0509, -0.0727]],\n",
      "\n",
      "         [[-0.0291, -0.1963, -0.0436],\n",
      "          [-0.0654, -0.1963, -0.1963],\n",
      "          [ 0.1890,  0.0872,  0.3126]]]], size=(6, 6, 3, 3), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.007270483300089836,\n",
      "       zero_point=0)), ('tfeb.7.bias', Parameter containing:\n",
      "tensor([-0.2824,  0.0131,  0.1082, -0.0754,  0.0030,  0.1833])), ('tfeb.7.scale', tensor(0.0084)), ('tfeb.7.zero_point', tensor(0)), ('tfeb.11.weight', tensor([[[[-0.0522, -0.1131, -0.0957],\n",
      "          [-0.0348, -0.1088, -0.0740],\n",
      "          [-0.0653, -0.1131, -0.0783]],\n",
      "\n",
      "         [[ 0.1740, -0.0261,  0.3698],\n",
      "          [ 0.1218, -0.0609,  0.2567],\n",
      "          [-0.1001, -0.1175,  0.0566]],\n",
      "\n",
      "         [[-0.0914, -0.0566, -0.0653],\n",
      "          [-0.0870, -0.0522, -0.0740],\n",
      "          [-0.0653, -0.0087, -0.0653]],\n",
      "\n",
      "         [[-0.1262,  0.0348,  0.5351],\n",
      "          [-0.1088,  0.0827,  0.5003],\n",
      "          [-0.0609,  0.0522,  0.2654]],\n",
      "\n",
      "         [[-0.0566, -0.0305, -0.0740],\n",
      "          [-0.1697,  0.0174,  0.0000],\n",
      "          [-0.0131,  0.2567,  0.3437]],\n",
      "\n",
      "         [[ 0.0044, -0.0044,  0.0174],\n",
      "          [ 0.1392, -0.0261, -0.0131],\n",
      "          [ 0.1175, -0.0305, -0.0522]]],\n",
      "\n",
      "\n",
      "        [[[-0.0392, -0.1436, -0.0696],\n",
      "          [-0.0305, -0.1218, -0.1392],\n",
      "          [-0.1131, -0.2045, -0.2175]],\n",
      "\n",
      "         [[ 0.0740, -0.1740, -0.1784],\n",
      "          [ 0.0392, -0.4264, -0.4786],\n",
      "          [-0.1871, -0.1653, -0.3785]],\n",
      "\n",
      "         [[-0.0957, -0.4133, -0.0479],\n",
      "          [ 0.1697, -0.1653, -0.0783],\n",
      "          [ 0.1349,  0.1827, -0.1044]],\n",
      "\n",
      "         [[-0.3306, -0.0522, -0.0870],\n",
      "          [ 0.0870,  0.0957,  0.1044],\n",
      "          [ 0.1958,  0.1914,  0.2262]],\n",
      "\n",
      "         [[ 0.2306,  0.5525,  0.0653],\n",
      "          [-0.0827, -0.1262, -0.1479],\n",
      "          [ 0.1175,  0.2654,  0.1479]],\n",
      "\n",
      "         [[-0.1653, -0.4220, -0.1610],\n",
      "          [-0.1523, -0.1523, -0.1566],\n",
      "          [-0.0522, -0.0609, -0.3785]]],\n",
      "\n",
      "\n",
      "        [[[-0.0696, -0.0696, -0.0479],\n",
      "          [-0.0261, -0.0261,  0.0000],\n",
      "          [-0.0696, -0.0609, -0.0609]],\n",
      "\n",
      "         [[-0.1392, -0.0087,  0.0174],\n",
      "          [-0.0348,  0.1610,  0.2349],\n",
      "          [ 0.0609,  0.1610,  0.1479]],\n",
      "\n",
      "         [[-0.0044, -0.0174, -0.0174],\n",
      "          [ 0.0000, -0.0044,  0.0044],\n",
      "          [ 0.0174,  0.0174,  0.0087]],\n",
      "\n",
      "         [[-0.0566, -0.0522, -0.0522],\n",
      "          [-0.0827, -0.0653, -0.0609],\n",
      "          [-0.0566, -0.0522, -0.0609]],\n",
      "\n",
      "         [[-0.2784, -0.1740, -0.1566],\n",
      "          [-0.2132, -0.0087, -0.1218],\n",
      "          [-0.1001, -0.0261, -0.1218]],\n",
      "\n",
      "         [[ 0.0827,  0.0914,  0.0827],\n",
      "          [ 0.0479,  0.0522,  0.0740],\n",
      "          [ 0.0044,  0.0218,  0.0131]]],\n",
      "\n",
      "\n",
      "        [[[-0.0827, -0.0305, -0.0305],\n",
      "          [-0.0827,  0.0044, -0.0044],\n",
      "          [-0.0174,  0.0087, -0.0087]],\n",
      "\n",
      "         [[-0.2828, -0.0653, -0.5047],\n",
      "          [-0.0479,  0.3219, -0.5003],\n",
      "          [-0.0131,  0.4873,  0.2436]],\n",
      "\n",
      "         [[ 0.0087, -0.0348, -0.0174],\n",
      "          [ 0.0044, -0.0131, -0.2349],\n",
      "          [ 0.0000, -0.0174, -0.0174]],\n",
      "\n",
      "         [[-0.0653, -0.0609, -0.0653],\n",
      "          [-0.0609, -0.0609, -0.0653],\n",
      "          [-0.0696, -0.0609, -0.0609]],\n",
      "\n",
      "         [[-0.2175, -0.3524, -0.4786],\n",
      "          [ 0.0305, -0.3916, -0.0914],\n",
      "          [-0.1088,  0.2480,  0.0174]],\n",
      "\n",
      "         [[-0.0435, -0.3132,  0.0000],\n",
      "          [ 0.0392, -0.2306,  0.0522],\n",
      "          [ 0.2610,  0.2567,  0.2262]]],\n",
      "\n",
      "\n",
      "        [[[-0.0566, -0.0261,  0.0000],\n",
      "          [-0.0348, -0.1175, -0.1001],\n",
      "          [-0.0305,  0.0000, -0.0087]],\n",
      "\n",
      "         [[ 0.0522,  0.2001,  0.0479],\n",
      "          [ 0.1349,  0.1523,  0.0087],\n",
      "          [ 0.0392,  0.0392,  0.0522]],\n",
      "\n",
      "         [[ 0.0174,  0.0392,  0.0131],\n",
      "          [-0.0479, -0.0218, -0.0261],\n",
      "          [-0.0218, -0.1175, -0.0218]],\n",
      "\n",
      "         [[-0.0392, -0.0261, -0.0131],\n",
      "          [-0.1044, -0.1044, -0.0044],\n",
      "          [ 0.0305,  0.0305,  0.0305]],\n",
      "\n",
      "         [[-0.0522, -0.0609, -0.0218],\n",
      "          [-0.0696, -0.1044, -0.0696],\n",
      "          [-0.1653, -0.0087, -0.1001]],\n",
      "\n",
      "         [[ 0.0566,  0.0522,  0.0566],\n",
      "          [ 0.0435,  0.0696,  0.0957],\n",
      "          [-0.0087, -0.0087, -0.0044]]],\n",
      "\n",
      "\n",
      "        [[[-0.0783, -0.0479, -0.0305],\n",
      "          [-0.0348, -0.0261, -0.1218],\n",
      "          [-0.1958, -0.1914, -0.0044]],\n",
      "\n",
      "         [[-0.0522, -0.0566, -0.3350],\n",
      "          [-0.0348,  0.2523, -0.1001],\n",
      "          [-0.0479,  0.1218, -0.0435]],\n",
      "\n",
      "         [[-0.0131,  0.2001,  0.1001],\n",
      "          [-0.0566,  0.0000, -0.0435],\n",
      "          [-0.2523, -0.2175, -0.0174]],\n",
      "\n",
      "         [[ 0.0044,  0.0044, -0.1088],\n",
      "          [-0.0087,  0.0000,  0.0218],\n",
      "          [-0.0174, -0.0218, -0.0131]],\n",
      "\n",
      "         [[-0.2175, -0.0914, -0.0740],\n",
      "          [-0.0261, -0.0957, -0.1610],\n",
      "          [ 0.0435, -0.0870, -0.1566]],\n",
      "\n",
      "         [[-0.0218, -0.0392, -0.0653],\n",
      "          [-0.0479, -0.0261, -0.0261],\n",
      "          [-0.2045, -0.0174, -0.0305]]],\n",
      "\n",
      "\n",
      "        [[[-0.0609, -0.0696, -0.0522],\n",
      "          [-0.0653, -0.1044, -0.0479],\n",
      "          [-0.0566, -0.1088, -0.0653]],\n",
      "\n",
      "         [[ 0.2001,  0.1436,  0.2828],\n",
      "          [ 0.0653,  0.0305, -0.0087],\n",
      "          [ 0.0305,  0.0000, -0.0174]],\n",
      "\n",
      "         [[-0.0131, -0.0348, -0.0479],\n",
      "          [ 0.0087,  0.0044, -0.0174],\n",
      "          [ 0.0131, -0.0087, -0.0348]],\n",
      "\n",
      "         [[-0.0218, -0.0348, -0.0261],\n",
      "          [ 0.0348,  0.0305,  0.0348],\n",
      "          [ 0.0914,  0.0566,  0.0522]],\n",
      "\n",
      "         [[ 0.0827,  0.2654,  0.2523],\n",
      "          [-0.1088, -0.0653, -0.0435],\n",
      "          [ 0.0957,  0.0174,  0.0305]],\n",
      "\n",
      "         [[-0.0653, -0.0435, -0.0131],\n",
      "          [-0.0174, -0.0174, -0.0087],\n",
      "          [ 0.0348,  0.0261,  0.0305]]],\n",
      "\n",
      "\n",
      "        [[[-0.0131, -0.0392, -0.0087],\n",
      "          [-0.0522, -0.0566, -0.0479],\n",
      "          [-0.0261, -0.0174, -0.0131]],\n",
      "\n",
      "         [[ 0.1479,  0.1218,  0.1305],\n",
      "          [ 0.2828,  0.1827,  0.0218],\n",
      "          [ 0.0218,  0.0000,  0.0174]],\n",
      "\n",
      "         [[-0.0044, -0.0131,  0.0131],\n",
      "          [-0.0435, -0.0392,  0.1436],\n",
      "          [-0.0522, -0.0566, -0.0479]],\n",
      "\n",
      "         [[ 0.0218,  0.0479, -0.0870],\n",
      "          [ 0.0261,  0.0392,  0.0522],\n",
      "          [ 0.0218, -0.1349,  0.0435]],\n",
      "\n",
      "         [[ 0.0609, -0.0174,  0.2436],\n",
      "          [ 0.2045,  0.1827,  0.0305],\n",
      "          [ 0.2262,  0.0870,  0.0827]],\n",
      "\n",
      "         [[-0.0044,  0.0044, -0.0044],\n",
      "          [-0.0131, -0.0131,  0.1610],\n",
      "          [-0.0131, -0.0131, -0.0131]]],\n",
      "\n",
      "\n",
      "        [[[-0.1044, -0.0218,  0.1610],\n",
      "          [-0.1044, -0.0827,  0.1305],\n",
      "          [-0.0261,  0.0174,  0.1523]],\n",
      "\n",
      "         [[-0.2001, -0.2262,  0.0522],\n",
      "          [ 0.0305, -0.1349,  0.0044],\n",
      "          [ 0.0131,  0.0218, -0.0044]],\n",
      "\n",
      "         [[-0.0392, -0.0348, -0.0479],\n",
      "          [-0.0174, -0.0261, -0.2654],\n",
      "          [-0.0218, -0.0218, -0.0305]],\n",
      "\n",
      "         [[ 0.0261, -0.0044,  0.0174],\n",
      "          [ 0.0000, -0.0392, -0.0131],\n",
      "          [-0.0914, -0.1044, -0.0740]],\n",
      "\n",
      "         [[ 0.0435,  0.1392,  0.1349],\n",
      "          [ 0.0087,  0.0305,  0.3350],\n",
      "          [-0.0522, -0.1349,  0.3437]],\n",
      "\n",
      "         [[-0.0305,  0.2045, -0.0174],\n",
      "          [ 0.0305,  0.0174,  0.0392],\n",
      "          [ 0.0348, -0.1653,  0.0305]]],\n",
      "\n",
      "\n",
      "        [[[-0.0044, -0.0174, -0.1871],\n",
      "          [-0.1566, -0.1784, -0.2523],\n",
      "          [ 0.0000, -0.0174, -0.0261]],\n",
      "\n",
      "         [[-0.0479, -0.0392, -0.0348],\n",
      "          [-0.0522, -0.0435, -0.0261],\n",
      "          [-0.0261, -0.0218,  0.0044]],\n",
      "\n",
      "         [[-0.0348, -0.2871,  0.1001],\n",
      "          [-0.0174, -0.1871, -0.0218],\n",
      "          [ 0.0131, -0.0087,  0.0000]],\n",
      "\n",
      "         [[-0.0305, -0.0218, -0.0261],\n",
      "          [-0.1566, -0.0131,  0.1436],\n",
      "          [-0.0305, -0.0435, -0.0435]],\n",
      "\n",
      "         [[-0.0783, -0.0870, -0.0522],\n",
      "          [-0.2175, -0.0566,  0.1262],\n",
      "          [-0.0218, -0.0044, -0.1436]],\n",
      "\n",
      "         [[-0.0174, -0.2436, -0.0218],\n",
      "          [ 0.0131, -0.0044, -0.0174],\n",
      "          [ 0.2132,  0.2088, -0.1566]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0870,  0.1653, -0.3437],\n",
      "          [ 0.1566,  0.1262, -0.3437],\n",
      "          [ 0.2828,  0.2306,  0.1001]],\n",
      "\n",
      "         [[ 0.0914, -0.1436,  0.0914],\n",
      "          [-0.1305,  0.0870, -0.3219],\n",
      "          [ 0.1088, -0.1262, -0.1392]],\n",
      "\n",
      "         [[ 0.0305,  0.4090,  0.4438],\n",
      "          [ 0.0914,  0.3263,  0.2697],\n",
      "          [-0.0087,  0.1958,  0.2871]],\n",
      "\n",
      "         [[ 0.0044,  0.0435, -0.2175],\n",
      "          [-0.2784, -0.3829, -0.4612],\n",
      "          [-0.0305, -0.3132, -0.0522]],\n",
      "\n",
      "         [[ 0.2349,  0.4699,  0.2175],\n",
      "          [ 0.2088,  0.0522, -0.0914],\n",
      "          [ 0.3872,  0.0957, -0.1479]],\n",
      "\n",
      "         [[ 0.1044,  0.3959,  0.0783],\n",
      "          [ 0.1001,  0.1827,  0.1697],\n",
      "          [-0.0218,  0.0174,  0.0174]]]], size=(11, 6, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.004350647330284119, zero_point=0)), ('tfeb.11.bias', Parameter containing:\n",
      "tensor([ 0.0558,  0.0457, -0.0737, -0.0161,  0.0166,  0.0816, -0.0278, -0.1410,\n",
      "        -0.0801,  0.0943, -0.3023])), ('tfeb.11.scale', tensor(0.0055)), ('tfeb.11.zero_point', tensor(0)), ('tfeb.14.weight', tensor([[[[-0.0046, -0.2456, -0.2919],\n",
      "          [-0.0417, -0.0880, -0.3753],\n",
      "          [ 0.0232, -0.0602, -0.0834]],\n",
      "\n",
      "         [[ 0.0232, -0.2039,  0.0556],\n",
      "          [ 0.2270, -0.4355, -0.1344],\n",
      "          [ 0.0278, -0.0139, -0.0741]],\n",
      "\n",
      "         [[ 0.0324,  0.0324,  0.0185],\n",
      "          [ 0.0093, -0.0417, -0.0417],\n",
      "          [ 0.0463, -0.2502,  0.0139]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3290,  0.0324,  0.0000],\n",
      "          [-0.0324, -0.0324, -0.0417],\n",
      "          [-0.0463, -0.0649, -0.0741]],\n",
      "\n",
      "         [[ 0.0046, -0.2085,  0.3151],\n",
      "          [ 0.0139, -0.2734,  0.2502],\n",
      "          [ 0.0093, -0.3382, -0.0185]],\n",
      "\n",
      "         [[-0.0556, -0.0788, -0.1853],\n",
      "          [-0.3197, -0.0973, -0.0324],\n",
      "          [-0.2641, -0.0556, -0.0880]]],\n",
      "\n",
      "\n",
      "        [[[-0.0788,  0.0278, -0.0093],\n",
      "          [-0.0834,  0.0139, -0.0371],\n",
      "          [-0.0324,  0.0649,  0.0093]],\n",
      "\n",
      "         [[-0.2039,  0.0232,  0.0000],\n",
      "          [ 0.0324,  0.0046, -0.0139],\n",
      "          [-0.2456,  0.0185, -0.0046]],\n",
      "\n",
      "         [[-0.0324, -0.0649, -0.2687],\n",
      "          [-0.1066, -0.0185, -0.2826],\n",
      "          [ 0.0510,  0.0927,  0.1390]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1575, -0.0417, -0.0371],\n",
      "          [-0.1622, -0.1158, -0.0556],\n",
      "          [-0.1436, -0.3058, -0.0463]],\n",
      "\n",
      "         [[ 0.0000, -0.0093, -0.0139],\n",
      "          [ 0.0463,  0.0324,  0.0371],\n",
      "          [ 0.0093,  0.0000, -0.0417]],\n",
      "\n",
      "         [[-0.0602,  0.1761, -0.0788],\n",
      "          [-0.0788,  0.0834, -0.1622],\n",
      "          [-0.0602, -0.0834, -0.1158]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1529, -0.1390,  0.1483],\n",
      "          [ 0.2085, -0.0324,  0.0371],\n",
      "          [ 0.2595,  0.2641,  0.2965]],\n",
      "\n",
      "         [[ 0.1483,  0.0602,  0.4726],\n",
      "          [-0.0371, -0.0232, -0.0232],\n",
      "          [ 0.5328,  0.0602,  0.1529]],\n",
      "\n",
      "         [[-0.0417, -0.0510, -0.0139],\n",
      "          [-0.0324, -0.0556, -0.0417],\n",
      "          [-0.0556, -0.0278, -0.0463]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0093,  0.0371,  0.0278],\n",
      "          [ 0.0834, -0.1668,  0.0093],\n",
      "          [ 0.0973,  0.1575,  0.0834]],\n",
      "\n",
      "         [[-0.4494, -0.0139,  0.0000],\n",
      "          [-0.1946, -0.2502, -0.0046],\n",
      "          [ 0.0232, -0.2131,  0.0046]],\n",
      "\n",
      "         [[ 0.0000,  0.1483, -0.0695],\n",
      "          [-0.4448, -0.1807, -0.1761],\n",
      "          [-0.3058, -0.3290,  0.0463]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1668,  0.0278,  0.3707],\n",
      "          [ 0.0000, -0.0232,  0.2224],\n",
      "          [ 0.0788,  0.0139, -0.0093]],\n",
      "\n",
      "         [[ 0.1853,  0.0093,  0.0371],\n",
      "          [ 0.0232,  0.3012,  0.0602],\n",
      "          [ 0.0417,  0.0556, -0.0232]],\n",
      "\n",
      "         [[ 0.0324, -0.1575, -0.0834],\n",
      "          [ 0.0695, -0.1761, -0.0371],\n",
      "          [-0.1668, -0.2502, -0.0046]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0139,  0.0185,  0.0741],\n",
      "          [ 0.0093,  0.0278,  0.0927],\n",
      "          [ 0.0139,  0.0278,  0.0556]],\n",
      "\n",
      "         [[ 0.2363,  0.4541,  0.0139],\n",
      "          [ 0.0000,  0.2409, -0.0185],\n",
      "          [ 0.0046,  0.0000, -0.0093]],\n",
      "\n",
      "         [[ 0.0139,  0.0417,  0.0510],\n",
      "          [-0.0278,  0.0232,  0.0232],\n",
      "          [-0.0278, -0.0093, -0.0046]]],\n",
      "\n",
      "\n",
      "        [[[-0.0371, -0.3012, -0.3892],\n",
      "          [-0.0093, -0.0371, -0.4216],\n",
      "          [ 0.2317, -0.0649, -0.4216]],\n",
      "\n",
      "         [[-0.4355, -0.2687, -0.0417],\n",
      "          [ 0.0880, -0.1158, -0.1112],\n",
      "          [ 0.4541, -0.0602, -0.0695]],\n",
      "\n",
      "         [[-0.0232,  0.0000,  0.0093],\n",
      "          [-0.0093,  0.0371,  0.0371],\n",
      "          [-0.0463,  0.0417,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0417, -0.0927, -0.0602],\n",
      "          [ 0.2780, -0.0741, -0.0602],\n",
      "          [ 0.4355, -0.1066, -0.0741]],\n",
      "\n",
      "         [[-0.1807, -0.3568, -0.0046],\n",
      "          [ 0.0093, -0.3012, -0.0185],\n",
      "          [ 0.0000, -0.0093, -0.0139]],\n",
      "\n",
      "         [[-0.0185,  0.0232, -0.0093],\n",
      "          [-0.0695,  0.0046,  0.0463],\n",
      "          [-0.1205,  0.3985,  0.2131]]],\n",
      "\n",
      "\n",
      "        [[[-0.0324, -0.0556, -0.0417],\n",
      "          [-0.0185, -0.0093, -0.0139],\n",
      "          [ 0.0093, -0.0093, -0.0324]],\n",
      "\n",
      "         [[-0.2224, -0.0139, -0.0185],\n",
      "          [-0.0741, -0.0510, -0.0649],\n",
      "          [ 0.0139,  0.0093,  0.0139]],\n",
      "\n",
      "         [[-0.0417,  0.0278, -0.0556],\n",
      "          [ 0.0139,  0.0185, -0.0510],\n",
      "          [-0.0093, -0.0139, -0.0649]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0324,  0.0324,  0.0927],\n",
      "          [ 0.0649,  0.0510,  0.1112],\n",
      "          [-0.0185, -0.0139,  0.0093]],\n",
      "\n",
      "         [[-0.2317,  0.0185,  0.0185],\n",
      "          [-0.4865, -0.3429, -0.3846],\n",
      "          [-0.0093,  0.0000,  0.0046]],\n",
      "\n",
      "         [[-0.0927, -0.0927, -0.0695],\n",
      "          [-0.0417, -0.0834, -0.1066],\n",
      "          [-0.0695, -0.1066, -0.0927]]]], size=(11, 11, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.004633272998034954, zero_point=0)), ('tfeb.14.bias', Parameter containing:\n",
      "tensor([ 0.0663, -0.0296, -0.2983,  0.0645, -0.0229, -0.1878, -0.2326, -0.0213,\n",
      "        -0.2253, -0.0285,  0.1444])), ('tfeb.14.scale', tensor(0.0053)), ('tfeb.14.zero_point', tensor(0)), ('tfeb.18.weight', tensor([[[[-0.1812,  0.0259, -0.2071],\n",
      "          [-0.0037,  0.0074,  0.0000],\n",
      "          [-0.0370, -0.0407, -0.0185]],\n",
      "\n",
      "         [[ 0.0111, -0.0222,  0.0037],\n",
      "          [ 0.0703,  0.0333,  0.0481],\n",
      "          [ 0.0851,  0.0518,  0.0481]],\n",
      "\n",
      "         [[-0.0185,  0.0148, -0.0185],\n",
      "          [-0.0111, -0.2034, -0.0444],\n",
      "          [-0.2145, -0.1997, -0.0037]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2219,  0.0037, -0.0259],\n",
      "          [-0.2737,  0.0037, -0.0222],\n",
      "          [-0.0037,  0.0296, -0.0185]],\n",
      "\n",
      "         [[ 0.2700, -0.0407, -0.0481],\n",
      "          [ 0.0259,  0.0037, -0.0259],\n",
      "          [ 0.0148, -0.0111, -0.0074]],\n",
      "\n",
      "         [[ 0.0148, -0.3476, -0.2478],\n",
      "          [-0.2663, -0.4179, -0.3402],\n",
      "          [-0.0111, -0.0111, -0.0185]]],\n",
      "\n",
      "\n",
      "        [[[-0.1109, -0.1294, -0.0111],\n",
      "          [ 0.0222,  0.0037,  0.1479],\n",
      "          [ 0.0222,  0.0037,  0.1664]],\n",
      "\n",
      "         [[-0.0037, -0.0074, -0.0037],\n",
      "          [-0.0074, -0.0148, -0.0185],\n",
      "          [ 0.0000, -0.1146, -0.1072]],\n",
      "\n",
      "         [[-0.0037,  0.1368,  0.0962],\n",
      "          [ 0.0296,  0.0148,  0.2700],\n",
      "          [-0.0111, -0.0037,  0.1035]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0111,  0.0000, -0.0185],\n",
      "          [ 0.0000,  0.0185, -0.0111],\n",
      "          [ 0.0037,  0.0111, -0.0111]],\n",
      "\n",
      "         [[ 0.0851,  0.0185, -0.0037],\n",
      "          [ 0.1738,  0.0185, -0.0111],\n",
      "          [ 0.0518,  0.0111,  0.0111]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0037],\n",
      "          [-0.0074, -0.0074,  0.0000],\n",
      "          [ 0.0000,  0.0037,  0.0074]]],\n",
      "\n",
      "\n",
      "        [[[-0.0370, -0.0074,  0.0185],\n",
      "          [-0.0259, -0.0111, -0.0037],\n",
      "          [ 0.0148,  0.0333,  0.0259]],\n",
      "\n",
      "         [[-0.0518, -0.0777, -0.0851],\n",
      "          [-0.0629, -0.0814, -0.0592],\n",
      "          [-0.0222, -0.0481, -0.0444]],\n",
      "\n",
      "         [[-0.0555, -0.0074,  0.0148],\n",
      "          [-0.0851, -0.0481, -0.0370],\n",
      "          [-0.0740, -0.0222, -0.0111]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0111,  0.2663, -0.0555],\n",
      "          [ 0.0222,  0.1664, -0.0259],\n",
      "          [-0.0185, -0.0111, -0.0111]],\n",
      "\n",
      "         [[-0.0518,  0.0111, -0.0370],\n",
      "          [-0.0592, -0.0185, -0.0740],\n",
      "          [-0.0481, -0.0148, -0.0481]],\n",
      "\n",
      "         [[-0.0037, -0.0111, -0.0259],\n",
      "          [-0.0296,  0.4290,  0.4438],\n",
      "          [ 0.0296,  0.0444,  0.0407]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1479,  0.0222,  0.0185],\n",
      "          [ 0.0000,  0.0333,  0.0296],\n",
      "          [-0.0037,  0.0444,  0.0444]],\n",
      "\n",
      "         [[ 0.0222,  0.0074,  0.0296],\n",
      "          [ 0.0555,  0.0592,  0.0851],\n",
      "          [ 0.0370,  0.0259,  0.0296]],\n",
      "\n",
      "         [[-0.0222,  0.4401,  0.0037],\n",
      "          [-0.0370,  0.4697,  0.0703],\n",
      "          [-0.0444,  0.0000,  0.0444]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0333, -0.0370, -0.0037],\n",
      "          [ 0.0000, -0.0222,  0.0148],\n",
      "          [ 0.0296,  0.0222,  0.0259]],\n",
      "\n",
      "         [[-0.0148,  0.1960, -0.0481],\n",
      "          [-0.0037, -0.0407, -0.0259],\n",
      "          [-0.0111, -0.0296, -0.0037]],\n",
      "\n",
      "         [[ 0.0111,  0.0148,  0.0185],\n",
      "          [ 0.0000,  0.0185,  0.0222],\n",
      "          [-0.1294,  0.0296,  0.0518]]],\n",
      "\n",
      "\n",
      "        [[[-0.0037, -0.0333, -0.0037],\n",
      "          [ 0.0111,  0.0000,  0.0000],\n",
      "          [ 0.0111,  0.0037,  0.0111]],\n",
      "\n",
      "         [[ 0.0074, -0.0037,  0.0037],\n",
      "          [-0.1109, -0.1220, -0.1035],\n",
      "          [-0.0925, -0.0888, -0.1072]],\n",
      "\n",
      "         [[ 0.0000,  0.1331,  0.2367],\n",
      "          [-0.0074,  0.1812,  0.2404],\n",
      "          [-0.0111,  0.0851, -0.0185]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1183,  0.0148,  0.0851],\n",
      "          [ 0.0703,  0.0222,  0.1368],\n",
      "          [-0.0074,  0.0888, -0.0111]],\n",
      "\n",
      "         [[-0.0111,  0.0000,  0.1627],\n",
      "          [-0.0037,  0.0296,  0.2515],\n",
      "          [-0.0111,  0.0777, -0.0777]],\n",
      "\n",
      "         [[ 0.0259,  0.0111,  0.0074],\n",
      "          [ 0.0814, -0.0037,  0.0000],\n",
      "          [ 0.0037,  0.0074, -0.0111]]],\n",
      "\n",
      "\n",
      "        [[[-0.1368,  0.0000,  0.0000],\n",
      "          [ 0.0074,  0.0111,  0.0037],\n",
      "          [ 0.0148,  0.0000, -0.0111]],\n",
      "\n",
      "         [[-0.0111, -0.0037,  0.0000],\n",
      "          [ 0.0333,  0.0370,  0.0111],\n",
      "          [ 0.0703,  0.0666,  0.0333]],\n",
      "\n",
      "         [[-0.1627, -0.0185, -0.1442],\n",
      "          [-0.0074,  0.0037,  0.0111],\n",
      "          [ 0.0444,  0.0555,  0.0703]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0037, -0.0111, -0.0259],\n",
      "          [-0.1960, -0.1997, -0.0296],\n",
      "          [-0.1405,  0.0000,  0.1294]],\n",
      "\n",
      "         [[-0.0037, -0.0148,  0.0074],\n",
      "          [-0.0740, -0.0666, -0.0444],\n",
      "          [-0.0407, -0.0629, -0.0222]],\n",
      "\n",
      "         [[-0.0074,  0.0000,  0.0000],\n",
      "          [ 0.0370,  0.0481,  0.0555],\n",
      "          [ 0.0259,  0.0222,  0.0259]]]], size=(26, 11, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.0036981531884521246, zero_point=0)), ('tfeb.18.bias', Parameter containing:\n",
      "tensor([ 0.0051, -0.1598, -0.0619, -0.1436,  0.0606, -0.1754, -0.0213, -0.0309,\n",
      "        -0.0317, -0.1019, -0.1286, -0.1616,  0.0274, -0.0320,  0.0614, -0.1486,\n",
      "         0.1157, -0.0655, -0.0519,  0.0522, -0.1483, -0.0199, -0.1942, -0.1534,\n",
      "        -0.2652, -0.0400])), ('tfeb.18.scale', tensor(0.0036)), ('tfeb.18.zero_point', tensor(0)), ('tfeb.21.weight', tensor([[[[ 0.0099,  0.0099,  0.0000],\n",
      "          [ 0.0000,  0.0049,  0.0000],\n",
      "          [ 0.0049, -0.0049,  0.0049]],\n",
      "\n",
      "         [[ 0.0247,  0.0197,  0.0247],\n",
      "          [ 0.0000, -0.0247, -0.0197],\n",
      "          [-0.0099, -0.0049, -0.0346]],\n",
      "\n",
      "         [[ 0.0000,  0.0099,  0.0148],\n",
      "          [-0.0099, -0.0049, -0.0049],\n",
      "          [ 0.0247,  0.1827,  0.0197]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0148,  0.1580,  0.1333],\n",
      "          [-0.0247, -0.0296, -0.0395],\n",
      "          [-0.0099,  0.0000,  0.0247]],\n",
      "\n",
      "         [[-0.2074, -0.0197, -0.1629],\n",
      "          [-0.0395, -0.0592, -0.1580],\n",
      "          [-0.0247, -0.0247,  0.0000]],\n",
      "\n",
      "         [[-0.0099, -0.0099, -0.0099],\n",
      "          [ 0.0000,  0.0247,  0.0296],\n",
      "          [ 0.0395,  0.0494,  0.0543]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2222, -0.0346, -0.2172],\n",
      "          [ 0.2172,  0.2222, -0.3160],\n",
      "          [ 0.0197,  0.3357, -0.0099]],\n",
      "\n",
      "         [[ 0.0000, -0.0247, -0.0346],\n",
      "          [-0.0247, -0.0543, -0.2419],\n",
      "          [-0.0494, -0.0494, -0.0296]],\n",
      "\n",
      "         [[-0.0099, -0.0197, -0.1975],\n",
      "          [-0.0494, -0.0444, -0.0296],\n",
      "          [-0.0099, -0.0197, -0.0049]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0296, -0.0346, -0.0197],\n",
      "          [-0.0494, -0.0691, -0.0346],\n",
      "          [-0.0543, -0.0296, -0.0148]],\n",
      "\n",
      "         [[-0.0197, -0.2321, -0.0197],\n",
      "          [-0.0839, -0.0543, -0.0148],\n",
      "          [-0.0247, -0.0444,  0.0000]],\n",
      "\n",
      "         [[-0.0247,  0.0000, -0.0049],\n",
      "          [-0.0296,  0.0049,  0.0099],\n",
      "          [ 0.0000,  0.0049, -0.0049]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0346,  0.0000,  0.0346],\n",
      "          [ 0.0395,  0.0395,  0.1037],\n",
      "          [ 0.0197,  0.0247,  0.0741]],\n",
      "\n",
      "         [[-0.0049, -0.0395, -0.0444],\n",
      "          [ 0.0296, -0.2913, -0.2814],\n",
      "          [ 0.0099,  0.0592,  0.0197]],\n",
      "\n",
      "         [[ 0.0197, -0.0197,  0.2074],\n",
      "          [ 0.0543,  0.0000, -0.0346],\n",
      "          [ 0.0543,  0.0099,  0.1580]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0197,  0.0197,  0.0889],\n",
      "          [-0.0197,  0.2765,  0.0099],\n",
      "          [-0.0049,  0.0395,  0.0049]],\n",
      "\n",
      "         [[-0.0839, -0.2716,  0.0741],\n",
      "          [-0.0790, -0.4740,  0.0247],\n",
      "          [-0.0444,  0.0444,  0.0790]],\n",
      "\n",
      "         [[ 0.1333,  0.1086,  0.0987],\n",
      "          [ 0.0592,  0.0296,  0.0148],\n",
      "          [ 0.0148,  0.0000, -0.0148]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0049, -0.0148, -0.1136],\n",
      "          [-0.0099, -0.0148, -0.0197],\n",
      "          [-0.0099,  0.0889, -0.0099]],\n",
      "\n",
      "         [[ 0.0642, -0.0148, -0.0099],\n",
      "          [-0.0148, -0.1086, -0.0197],\n",
      "          [-0.0148, -0.1679, -0.1185]],\n",
      "\n",
      "         [[-0.0049, -0.0938, -0.0049],\n",
      "          [-0.0049, -0.0049,  0.0000],\n",
      "          [ 0.1284, -0.0148,  0.0049]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0099, -0.1136, -0.1629],\n",
      "          [ 0.0099,  0.0889, -0.0148],\n",
      "          [ 0.0049,  0.0099, -0.0049]],\n",
      "\n",
      "         [[ 0.1679,  0.1037, -0.1827],\n",
      "          [-0.0197, -0.1481, -0.1876],\n",
      "          [-0.0296, -0.1580, -0.0148]],\n",
      "\n",
      "         [[ 0.0889, -0.0049, -0.0099],\n",
      "          [ 0.0000,  0.0099, -0.0049],\n",
      "          [ 0.0049,  0.0099,  0.0049]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0099, -0.0395],\n",
      "          [ 0.0099,  0.0000, -0.0197],\n",
      "          [ 0.0049, -0.0148, -0.0148]],\n",
      "\n",
      "         [[-0.0148, -0.0197, -0.0197],\n",
      "          [-0.0494, -0.0395, -0.0444],\n",
      "          [-0.0543, -0.0346, -0.0247]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0099],\n",
      "          [-0.0099, -0.0099,  0.0099],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0247,  0.0099,  0.0099],\n",
      "          [ 0.0197,  0.1679,  0.0197],\n",
      "          [ 0.0444,  0.1531,  0.0296]],\n",
      "\n",
      "         [[-0.0197, -0.0148,  0.0049],\n",
      "          [ 0.0099,  0.1679,  0.0197],\n",
      "          [ 0.0197,  0.0395,  0.0395]],\n",
      "\n",
      "         [[-0.0197, -0.0099,  0.0049],\n",
      "          [ 0.0197,  0.0296,  0.0247],\n",
      "          [ 0.0197,  0.0049, -0.0099]]],\n",
      "\n",
      "\n",
      "        [[[-0.0395, -0.0543, -0.0247],\n",
      "          [-0.0444, -0.0148,  0.0642],\n",
      "          [-0.0346, -0.0197,  0.0494]],\n",
      "\n",
      "         [[-0.0444, -0.0444, -0.0543],\n",
      "          [-0.0444, -0.0444, -0.0346],\n",
      "          [-0.0049, -0.0197,  0.2716]],\n",
      "\n",
      "         [[-0.0395, -0.0494, -0.0247],\n",
      "          [ 0.0346,  0.0197,  0.0296],\n",
      "          [-0.2518, -0.0148, -0.2469]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0691, -0.0741, -0.0839],\n",
      "          [-0.0296, -0.0691, -0.0691],\n",
      "          [ 0.0000, -0.0741, -0.1185]],\n",
      "\n",
      "         [[ 0.1481, -0.0543, -0.0296],\n",
      "          [-0.0889, -0.0691, -0.0741],\n",
      "          [-0.0543, -0.0592, -0.0296]],\n",
      "\n",
      "         [[ 0.0247,  0.0099,  0.0049],\n",
      "          [-0.0296, -0.0543, -0.0296],\n",
      "          [-0.0543, -0.0494, -0.0346]]]], size=(14, 26, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.004937299527227879, zero_point=0)), ('tfeb.21.bias', Parameter containing:\n",
      "tensor([-0.0903, -0.0773, -0.1218, -0.0867, -0.2401, -0.0844, -0.2230, -0.0394,\n",
      "        -0.1717, -0.0829, -0.1480, -0.1635, -0.1357, -0.1717])), ('tfeb.21.scale', tensor(0.0051)), ('tfeb.21.zero_point', tensor(0)), ('tfeb.25.weight', tensor([[[[ 0.0052,  0.0311,  0.2382],\n",
      "          [-0.0518, -0.0621, -0.0880],\n",
      "          [-0.0104,  0.0052, -0.0104]],\n",
      "\n",
      "         [[ 0.0518,  0.0466,  0.0414],\n",
      "          [ 0.0000, -0.0104, -0.0207],\n",
      "          [ 0.0155,  0.0155,  0.0000]],\n",
      "\n",
      "         [[-0.0259,  0.0362,  0.0311],\n",
      "          [ 0.0673,  0.1657,  0.0259],\n",
      "          [ 0.2382,  0.3418,  0.1087]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0052, -0.0104,  0.0518],\n",
      "          [-0.0207, -0.0518,  0.1761],\n",
      "          [-0.0259, -0.0259,  0.0052]],\n",
      "\n",
      "         [[-0.0518, -0.0155, -0.0311],\n",
      "          [ 0.1191,  0.1968, -0.0104],\n",
      "          [-0.0104, -0.0259, -0.0207]],\n",
      "\n",
      "         [[ 0.0311, -0.0052,  0.0052],\n",
      "          [-0.3935, -0.2900, -0.2486],\n",
      "          [-0.1036, -0.0621, -0.0155]]],\n",
      "\n",
      "\n",
      "        [[[-0.0052,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0052,  0.0052],\n",
      "          [-0.0052,  0.0000,  0.0052]],\n",
      "\n",
      "         [[ 0.0000,  0.0052, -0.1036],\n",
      "          [ 0.0000,  0.0052, -0.0155],\n",
      "          [ 0.0000,  0.0000, -0.0052]],\n",
      "\n",
      "         [[ 0.1191, -0.0052, -0.1243],\n",
      "          [-0.0155, -0.1761, -0.1191],\n",
      "          [-0.0104, -0.0052, -0.0052]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0052, -0.1087,  0.0052],\n",
      "          [ 0.0000,  0.0000, -0.1191]],\n",
      "\n",
      "         [[-0.0104,  0.0000,  0.0000],\n",
      "          [-0.0984, -0.0052, -0.0052],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0052,  0.0000, -0.1243],\n",
      "          [-0.0155, -0.0052, -0.1139],\n",
      "          [-0.0052, -0.0052, -0.0052]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0414, -0.0207, -0.0052],\n",
      "          [ 0.0052, -0.2019,  0.0104],\n",
      "          [ 0.0259,  0.0052,  0.0000]],\n",
      "\n",
      "         [[-0.0104, -0.0207, -0.0052],\n",
      "          [ 0.0207,  0.0000,  0.0000],\n",
      "          [ 0.0104, -0.0052,  0.0155]],\n",
      "\n",
      "         [[ 0.0829,  0.0414,  0.0052],\n",
      "          [ 0.0984,  0.0673,  0.0000],\n",
      "          [ 0.0104,  0.0104, -0.0104]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0104,  0.2227,  0.2175],\n",
      "          [-0.0311,  0.0000,  0.2848],\n",
      "          [ 0.0052,  0.0052,  0.0311]],\n",
      "\n",
      "         [[ 0.0259,  0.0207, -0.0104],\n",
      "          [-0.0104, -0.0311,  0.0207],\n",
      "          [ 0.0207,  0.0155,  0.0311]],\n",
      "\n",
      "         [[-0.2641,  0.0000, -0.0104],\n",
      "          [-0.0155, -0.0362, -0.0259],\n",
      "          [-0.0311, -0.0362, -0.0104]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0259, -0.0155,  0.0000],\n",
      "          [-0.0311,  0.0155,  0.0466],\n",
      "          [-0.0052, -0.0052,  0.0155]],\n",
      "\n",
      "         [[ 0.0000, -0.0207, -0.0155],\n",
      "          [-0.1916, -0.2227, -0.0052],\n",
      "          [ 0.0259,  0.0259,  0.0311]],\n",
      "\n",
      "         [[ 0.0362, -0.0104,  0.0518],\n",
      "          [-0.0725,  0.0414,  0.0673],\n",
      "          [-0.0155,  0.1657,  0.1502]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0052, -0.2434, -0.1968],\n",
      "          [-0.0104, -0.0259, -0.0311],\n",
      "          [ 0.0000, -0.0570, -0.0725]],\n",
      "\n",
      "         [[ 0.0104, -0.0207,  0.0104],\n",
      "          [-0.0673,  0.0984,  0.0829],\n",
      "          [-0.0777, -0.0829, -0.0621]],\n",
      "\n",
      "         [[ 0.0000,  0.0052,  0.0207],\n",
      "          [-0.2744, -0.1968, -0.1812],\n",
      "          [ 0.0207,  0.0311,  0.0207]]],\n",
      "\n",
      "\n",
      "        [[[-0.0052,  0.0000,  0.0000],\n",
      "          [-0.0052, -0.0104, -0.0104],\n",
      "          [-0.0052, -0.0052,  0.0000]],\n",
      "\n",
      "         [[ 0.0155, -0.0104, -0.0259],\n",
      "          [ 0.0104,  0.0104,  0.0104],\n",
      "          [ 0.0259,  0.0207,  0.0104]],\n",
      "\n",
      "         [[-0.0207, -0.0311, -0.0259],\n",
      "          [-0.0570, -0.0362, -0.0362],\n",
      "          [ 0.0052,  0.0207,  0.0104]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0052, -0.0155, -0.0155],\n",
      "          [-0.0052, -0.1761,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.1502]],\n",
      "\n",
      "         [[-0.0259, -0.0466, -0.0311],\n",
      "          [-0.0259, -0.0155, -0.0052],\n",
      "          [ 0.0155,  0.0104,  0.0155]],\n",
      "\n",
      "         [[ 0.0155,  0.0104,  0.0052],\n",
      "          [ 0.0104,  0.0104,  0.0104],\n",
      "          [-0.0207, -0.0155, -0.0207]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0207,  0.0052, -0.0052],\n",
      "          [ 0.0104,  0.0052,  0.0155],\n",
      "          [ 0.0259,  0.0259,  0.0259]],\n",
      "\n",
      "         [[ 0.0207,  0.0207,  0.0104],\n",
      "          [ 0.0362,  0.0311,  0.0104],\n",
      "          [ 0.0000,  0.0984, -0.0052]],\n",
      "\n",
      "         [[ 0.0259,  0.0311,  0.0052],\n",
      "          [-0.0104, -0.0104, -0.0259],\n",
      "          [ 0.0000,  0.0052,  0.0155]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0207,  0.0155,  0.0104],\n",
      "          [ 0.0207, -0.0052, -0.0052],\n",
      "          [ 0.0052, -0.0104,  0.0000]],\n",
      "\n",
      "         [[ 0.0362,  0.0259, -0.0104],\n",
      "          [-0.0052, -0.0207, -0.0207],\n",
      "          [-0.0104, -0.0259, -0.0104]],\n",
      "\n",
      "         [[-0.1139, -0.1346, -0.1553],\n",
      "          [-0.1139, -0.1036, -0.0880],\n",
      "          [-0.0777, -0.1191, -0.0570]]]], size=(30, 14, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.005178161431103945, zero_point=0)), ('tfeb.25.bias', Parameter containing:\n",
      "tensor([-0.1323, -0.0728, -0.1815, -0.0200, -0.2661, -0.2432,  0.0240, -0.0144,\n",
      "        -0.1583, -0.1017, -0.1239, -0.0138, -0.1624, -0.0539, -0.2352, -0.1513,\n",
      "        -0.0974, -0.2201, -0.1345, -0.1980, -0.0605, -0.1748, -0.1092, -0.2075,\n",
      "        -0.0581, -0.0281, -0.0527, -0.0444, -0.1258, -0.0650])), ('tfeb.25.scale', tensor(0.0047)), ('tfeb.25.zero_point', tensor(0)), ('tfeb.28.weight', tensor([[[[ 0.0269, -0.0806, -0.1236],\n",
      "          [ 0.0161,  0.0054, -0.0215],\n",
      "          [ 0.0322,  0.0054, -0.0269]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0054],\n",
      "          [ 0.0054,  0.0054, -0.0107],\n",
      "          [ 0.0000,  0.0054, -0.0054]],\n",
      "\n",
      "         [[ 0.0107,  0.2472,  0.0161],\n",
      "          [ 0.0752, -0.0107, -0.0376],\n",
      "          [ 0.0054,  0.0054, -0.0161]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0645, -0.0376, -0.0484],\n",
      "          [-0.0054,  0.0000, -0.0107],\n",
      "          [ 0.0054, -0.0054,  0.0161]],\n",
      "\n",
      "         [[ 0.0269,  0.0054,  0.0054],\n",
      "          [-0.0161,  0.0054,  0.0000],\n",
      "          [ 0.0000,  0.0054,  0.0161]],\n",
      "\n",
      "         [[-0.0054,  0.0000, -0.0107],\n",
      "          [ 0.1075,  0.0699,  0.0484],\n",
      "          [ 0.0000,  0.0107,  0.0215]]],\n",
      "\n",
      "\n",
      "        [[[-0.0054,  0.0000, -0.0054],\n",
      "          [-0.0107,  0.0054, -0.0215],\n",
      "          [-0.0054, -0.0215, -0.0161]],\n",
      "\n",
      "         [[ 0.0000, -0.0054,  0.0054],\n",
      "          [-0.0054, -0.0107,  0.0000],\n",
      "          [-0.0161, -0.0161, -0.0161]],\n",
      "\n",
      "         [[-0.0054,  0.0000, -0.0054],\n",
      "          [ 0.0054,  0.0107,  0.0215],\n",
      "          [-0.0107,  0.0000, -0.1720]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0269, -0.0107, -0.0054],\n",
      "          [-0.2311, -0.0537, -0.0752],\n",
      "          [-0.2150, -0.2042, -0.0269]],\n",
      "\n",
      "         [[ 0.0000,  0.0054, -0.0054],\n",
      "          [-0.0107,  0.0000,  0.0054],\n",
      "          [-0.0107,  0.0000, -0.0107]],\n",
      "\n",
      "         [[-0.0161, -0.0161, -0.0107],\n",
      "          [-0.0322, -0.0322, -0.0322],\n",
      "          [ 0.0054,  0.0054,  0.0161]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0107,  0.0054, -0.0054],\n",
      "          [ 0.0054,  0.0107,  0.0054],\n",
      "          [ 0.0000, -0.0107, -0.0215]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0430],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0806]],\n",
      "\n",
      "         [[ 0.0054,  0.0000,  0.0000],\n",
      "          [ 0.0161,  0.0054,  0.0054],\n",
      "          [ 0.0054,  0.0054,  0.0215]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -0.0054, -0.0054],\n",
      "          [ 0.0000, -0.0054, -0.0054],\n",
      "          [ 0.0000,  0.0000,  0.0054]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0054, -0.0054, -0.0107],\n",
      "          [-0.0054, -0.0054,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0107, -0.0430,  0.0054],\n",
      "          [ 0.0161,  0.0107,  0.0215]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0269, -0.0215, -0.0215],\n",
      "          [-0.0376, -0.2150, -0.0269],\n",
      "          [ 0.0054,  0.0322,  0.0484]],\n",
      "\n",
      "         [[ 0.0054,  0.0000, -0.0054],\n",
      "          [ 0.0054,  0.0000,  0.2956],\n",
      "          [ 0.0054,  0.0000, -0.0054]],\n",
      "\n",
      "         [[ 0.0000, -0.0054,  0.0054],\n",
      "          [ 0.0161,  0.0107,  0.0215],\n",
      "          [ 0.0000,  0.0000,  0.0107]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0215, -0.0161, -0.0269],\n",
      "          [ 0.0322, -0.1881, -0.2849],\n",
      "          [ 0.0107,  0.0054,  0.0054]],\n",
      "\n",
      "         [[ 0.0000, -0.0054,  0.0000],\n",
      "          [-0.0054,  0.0000, -0.0107],\n",
      "          [ 0.0161,  0.0215,  0.0161]],\n",
      "\n",
      "         [[ 0.0107,  0.0000,  0.0000],\n",
      "          [ 0.0161,  0.0000, -0.0322],\n",
      "          [-0.0107, -0.0161, -0.0215]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0215,  0.0107],\n",
      "          [ 0.0430,  0.0322,  0.0322],\n",
      "          [ 0.0430,  0.0161,  0.0054]],\n",
      "\n",
      "         [[ 0.0054,  0.0107,  0.2204],\n",
      "          [-0.0054,  0.0107, -0.0215],\n",
      "          [ 0.0054,  0.0107,  0.0000]],\n",
      "\n",
      "         [[-0.0161,  0.0000,  0.0215],\n",
      "          [-0.0269, -0.0107, -0.0484],\n",
      "          [ 0.0161,  0.0215,  0.0107]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3977, -0.3440,  0.1612],\n",
      "          [ 0.0322,  0.0322,  0.0269],\n",
      "          [ 0.0054,  0.0107,  0.0107]],\n",
      "\n",
      "         [[-0.0215, -0.0269,  0.0000],\n",
      "          [ 0.0107,  0.0107,  0.0000],\n",
      "          [ 0.0054,  0.0000, -0.0107]],\n",
      "\n",
      "         [[ 0.0054,  0.0215,  0.0054],\n",
      "          [-0.0215, -0.0054, -0.0054],\n",
      "          [ 0.0161,  0.0215,  0.0161]]],\n",
      "\n",
      "\n",
      "        [[[-0.0107, -0.0215, -0.0322],\n",
      "          [ 0.0269,  0.0054, -0.0376],\n",
      "          [ 0.0054, -0.0161, -0.0322]],\n",
      "\n",
      "         [[-0.0054, -0.1989, -0.0054],\n",
      "          [-0.0107,  0.0000, -0.0215],\n",
      "          [ 0.0107,  0.0054,  0.1774]],\n",
      "\n",
      "         [[-0.0430, -0.0161,  0.0000],\n",
      "          [-0.0054, -0.0107,  0.0107],\n",
      "          [-0.0215, -0.0161, -0.0161]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0376, -0.0322, -0.0322],\n",
      "          [-0.0161, -0.0645, -0.0430],\n",
      "          [ 0.0000, -0.0645, -0.0376]],\n",
      "\n",
      "         [[-0.0215, -0.0161,  0.0107],\n",
      "          [-0.0161,  0.0107, -0.0054],\n",
      "          [ 0.0484,  0.0645,  0.0161]],\n",
      "\n",
      "         [[-0.0269, -0.0322, -0.0269],\n",
      "          [-0.0860, -0.1290, -0.1075],\n",
      "          [ 0.0000, -0.0161, -0.0322]]]], size=(54, 30, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.005374914966523647, zero_point=0)), ('tfeb.28.bias', Parameter containing:\n",
      "tensor([-0.1097, -0.0420, -0.1319, -0.1229, -0.1395, -0.0460, -0.1189, -0.0797,\n",
      "        -0.0811, -0.1149, -0.0475, -0.0479, -0.0576, -0.0577, -0.1459, -0.1622,\n",
      "        -0.0133, -0.0224, -0.0952, -0.0722, -0.0535, -0.0499, -0.1521, -0.0660,\n",
      "        -0.1545, -0.1229, -0.0364, -0.0590, -0.0141, -0.0777, -0.0753, -0.0973,\n",
      "        -0.0723, -0.0244, -0.0294, -0.0905, -0.0354, -0.0418, -0.0505, -0.2003,\n",
      "        -0.0871, -0.1525, -0.1355, -0.1331, -0.0411, -0.1233, -0.1054, -0.1257,\n",
      "        -0.0771, -0.0236,  0.0141, -0.0305, -0.1008, -0.0351])), ('tfeb.28.scale', tensor(0.0033)), ('tfeb.28.zero_point', tensor(0)), ('tfeb.33.weight', tensor([[[[-0.9426]],\n",
      "\n",
      "         [[ 1.9146]],\n",
      "\n",
      "         [[-3.7703]],\n",
      "\n",
      "         [[-0.2946]],\n",
      "\n",
      "         [[-0.7953]],\n",
      "\n",
      "         [[ 1.1782]],\n",
      "\n",
      "         [[ 0.9720]],\n",
      "\n",
      "         [[-1.5022]],\n",
      "\n",
      "         [[-1.4728]],\n",
      "\n",
      "         [[-0.7658]],\n",
      "\n",
      "         [[-1.1193]],\n",
      "\n",
      "         [[ 3.1223]],\n",
      "\n",
      "         [[ 0.1473]],\n",
      "\n",
      "         [[ 0.6775]],\n",
      "\n",
      "         [[-1.4728]],\n",
      "\n",
      "         [[ 0.0884]],\n",
      "\n",
      "         [[ 2.1208]],\n",
      "\n",
      "         [[ 1.8851]],\n",
      "\n",
      "         [[ 0.0295]],\n",
      "\n",
      "         [[-1.7379]],\n",
      "\n",
      "         [[-2.2975]],\n",
      "\n",
      "         [[ 0.7658]],\n",
      "\n",
      "         [[-1.7379]],\n",
      "\n",
      "         [[ 2.4742]],\n",
      "\n",
      "         [[-0.0589]],\n",
      "\n",
      "         [[ 1.0015]],\n",
      "\n",
      "         [[ 0.6480]],\n",
      "\n",
      "         [[-0.1473]],\n",
      "\n",
      "         [[ 1.7968]],\n",
      "\n",
      "         [[-0.2062]],\n",
      "\n",
      "         [[ 1.3549]],\n",
      "\n",
      "         [[-0.6480]],\n",
      "\n",
      "         [[ 0.7953]],\n",
      "\n",
      "         [[ 1.0309]],\n",
      "\n",
      "         [[ 2.2091]],\n",
      "\n",
      "         [[-1.1488]],\n",
      "\n",
      "         [[ 0.3240]],\n",
      "\n",
      "         [[ 0.8837]],\n",
      "\n",
      "         [[-0.0589]],\n",
      "\n",
      "         [[-1.7673]],\n",
      "\n",
      "         [[ 0.0295]],\n",
      "\n",
      "         [[-2.1208]],\n",
      "\n",
      "         [[-1.7968]],\n",
      "\n",
      "         [[-0.7069]],\n",
      "\n",
      "         [[ 2.5626]],\n",
      "\n",
      "         [[ 0.2356]],\n",
      "\n",
      "         [[ 0.5007]],\n",
      "\n",
      "         [[-0.2946]],\n",
      "\n",
      "         [[ 0.1767]],\n",
      "\n",
      "         [[ 1.6200]],\n",
      "\n",
      "         [[ 1.8262]],\n",
      "\n",
      "         [[-1.0015]],\n",
      "\n",
      "         [[-0.1767]],\n",
      "\n",
      "         [[ 2.5921]]],\n",
      "\n",
      "\n",
      "        [[[-1.4728]],\n",
      "\n",
      "         [[-0.7953]],\n",
      "\n",
      "         [[ 1.5317]],\n",
      "\n",
      "         [[-1.0898]],\n",
      "\n",
      "         [[-1.5611]],\n",
      "\n",
      "         [[-1.9735]],\n",
      "\n",
      "         [[-0.8837]],\n",
      "\n",
      "         [[ 0.2651]],\n",
      "\n",
      "         [[ 1.6495]],\n",
      "\n",
      "         [[-2.0913]],\n",
      "\n",
      "         [[ 0.2946]],\n",
      "\n",
      "         [[-1.8557]],\n",
      "\n",
      "         [[-0.5891]],\n",
      "\n",
      "         [[ 1.4433]],\n",
      "\n",
      "         [[ 0.2356]],\n",
      "\n",
      "         [[ 1.0604]],\n",
      "\n",
      "         [[-1.4728]],\n",
      "\n",
      "         [[-1.3549]],\n",
      "\n",
      "         [[ 0.5891]],\n",
      "\n",
      "         [[ 1.1193]],\n",
      "\n",
      "         [[ 1.8851]],\n",
      "\n",
      "         [[-1.4139]],\n",
      "\n",
      "         [[ 0.8542]],\n",
      "\n",
      "         [[-1.3844]],\n",
      "\n",
      "         [[-1.4433]],\n",
      "\n",
      "         [[ 0.1473]],\n",
      "\n",
      "         [[-1.0898]],\n",
      "\n",
      "         [[ 0.7953]],\n",
      "\n",
      "         [[-1.4728]],\n",
      "\n",
      "         [[ 0.4418]],\n",
      "\n",
      "         [[-0.7658]],\n",
      "\n",
      "         [[-1.5906]],\n",
      "\n",
      "         [[ 0.7364]],\n",
      "\n",
      "         [[-0.4713]],\n",
      "\n",
      "         [[-1.7379]],\n",
      "\n",
      "         [[ 0.3535]],\n",
      "\n",
      "         [[-2.2386]],\n",
      "\n",
      "         [[-1.1193]],\n",
      "\n",
      "         [[-0.2062]],\n",
      "\n",
      "         [[ 0.2946]],\n",
      "\n",
      "         [[ 0.5302]],\n",
      "\n",
      "         [[-0.0589]],\n",
      "\n",
      "         [[ 0.0884]],\n",
      "\n",
      "         [[ 1.1782]],\n",
      "\n",
      "         [[-1.4433]],\n",
      "\n",
      "         [[ 0.4713]],\n",
      "\n",
      "         [[-0.5891]],\n",
      "\n",
      "         [[-1.7673]],\n",
      "\n",
      "         [[ 1.0309]],\n",
      "\n",
      "         [[-1.4728]],\n",
      "\n",
      "         [[-1.7968]],\n",
      "\n",
      "         [[ 0.7069]],\n",
      "\n",
      "         [[-0.9720]],\n",
      "\n",
      "         [[-1.3255]]]], size=(2, 54, 1, 1), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.02945530414581299,\n",
      "       zero_point=0)), ('tfeb.33.bias', Parameter containing:\n",
      "tensor([1.4660, 1.6133])), ('tfeb.33.scale', tensor(0.0065)), ('tfeb.33.zero_point', tensor(0)), ('tfeb.38.scale', tensor(0.0021)), ('tfeb.38.zero_point', tensor(78)), ('tfeb.38._packed_params.dtype', torch.qint8), ('tfeb.38._packed_params._packed_params', (tensor([[ 1.4038, -1.3341],\n",
      "        [-1.3457,  1.4734]], size=(2, 2), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.011601276695728302,\n",
      "       zero_point=0), tensor([ 0.0133, -0.1265], grad_fn=<CopyBackwards>))), ('quant.scale', tensor([0.0134])), ('quant.zero_point', tensor([127]))])\n",
      "Quantization done\n",
      "Testing quantized model.\n",
      "torch.Size([176, 2])\n",
      "Testing: Acc(top1) 81.82%\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29a3ca04-7384-4a90-853e-050b593a0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt2 = getOpts();#opts.parse();\n",
    "# state2 = torch.load(to_convert_model_path, map_location=\"cuda:0\");\n",
    "# tmpnet = models.GetACDNetModel(input_len=opt2.inputLength, nclass=6, sr=20000, channel_config=state2['config']).to(\"cuda:0\");\n",
    "# tmpnet.load_state_dict(state2['weight']);\n",
    "# print(tmpnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59060995-a696-4bc6-8980-5a5d18cf4bd1",
   "metadata": {},
   "source": [
    "## Convert tflite to C array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab64fe84-ccca-4d35-80a3-edfd7012b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_TFLITE = os.path.join(MODELS_DIR, 'model.tflite')\n",
    "# FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'float_model.tflite')\n",
    "# MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'model.cc')\n",
    "tflite_model = \"../th/quantized_models/quant_retrained_model_95.4_20240130035114.pt\"\n",
    "target_c_file =\"../th/Model_C_Files/quant_from_retrain_95.4_20240130035114.cc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce47b1c0-ac1e-47d6-abf9-72dd595e94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get -qq install xxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "274d47a5-acc2-4eeb-bd3b-2c57ae556cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get -qq install xxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "190ecc22-9963-4c04-b389-7f339a8e6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!xxd -i {tflite_model} > {target_c_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6812387-b134-4bee-acc6-5d4402d34559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 1: \"../th/Model_C_Files/qua ...\": invalid command code .\n"
     ]
    }
   ],
   "source": [
    "# Update variable names\n",
    "REPLACE_TEXT = \"uec_iot_model_alarm\";#MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
    "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {target_c_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b37ec-3980-4aed-8b2e-e86e36718ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
