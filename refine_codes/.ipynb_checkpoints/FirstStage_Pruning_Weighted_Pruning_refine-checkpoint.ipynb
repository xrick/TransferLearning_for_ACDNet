{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15eb05a5-0216-4c21-9226-27bca5bb4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import glob;\n",
    "import math;\n",
    "import numpy as np;\n",
    "import random;\n",
    "import time;\n",
    "import torch\n",
    "import torch.optim as optim;\n",
    "import torch.nn as nn;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a949309-13bb-4795-aab2-229c6dbc05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886f8b70-1961-4c4f-823f-55393ff9c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.utils as U;\n",
    "import common.opts as opt;\n",
    "import th.resources.models as models;\n",
    "import th.resources.train_generator as train_generator;\n",
    "import th.resources.pruning_tools.weight_pruning as weight_pruner;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd58c73b-11bf-48dc-895b-0959625f38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import common.tlopts as tlopts\n",
    "import th.resources.calculator as calc;\n",
    "from datetime import datetime;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb31f54-0d24-4942-ad4c-f498900e61d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944e5af4-56fa-45e2-8ef8-a1b3beb25156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDataTimeStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H:%M:%S').replace('-',\"\").replace(' ',\"\").replace(':',\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f626e5-5ed6-4aef-908f-2a39342bb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for pytorch\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = dict([(52,1),(99,2)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[label1]- 1\n",
    "            idx2 = self.mapdict[label2] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        \n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e473f603-d73b-4aff-bda7-2aaf0bdab6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACDNetV2(nn.Module):\n",
    "    def __init__(self, input_length, n_class, sr, ch_conf=None):\n",
    "        super(ACDNetV2, self).__init__();\n",
    "        self.input_length = input_length;\n",
    "        self.ch_config = ch_conf;\n",
    "\n",
    "        stride1 = 2;\n",
    "        stride2 = 2;\n",
    "        channels = 8;\n",
    "        k_size = (3, 3);\n",
    "        n_frames = (sr/1000)*10; #No of frames per 10ms\n",
    "\n",
    "        sfeb_pool_size = int(n_frames/(stride1*stride2));\n",
    "        # tfeb_pool_size = (2,2);\n",
    "        if self.ch_config is None:\n",
    "            self.ch_config = [channels, channels*8, channels*4, channels*8, channels*8, channels*16, channels*16, channels*32, channels*32, channels*64, channels*64, n_class];\n",
    "        # avg_pool_kernel_size = (1,4) if self.ch_config[1] < 64 else (2,4);\n",
    "        fcn_no_of_inputs = self.ch_config[-1];\n",
    "        conv1, bn1 = self.make_layers(1, self.ch_config[0], (1, 9), (1, stride1));\n",
    "        conv2, bn2 = self.make_layers(self.ch_config[0], self.ch_config[1], (1, 5), (1, stride2));\n",
    "        conv3, bn3 = self.make_layers(1, self.ch_config[2], k_size, padding=1);\n",
    "        conv4, bn4 = self.make_layers(self.ch_config[2], self.ch_config[3], k_size, padding=1);\n",
    "        conv5, bn5 = self.make_layers(self.ch_config[3], self.ch_config[4], k_size, padding=1);\n",
    "        conv6, bn6 = self.make_layers(self.ch_config[4], self.ch_config[5], k_size, padding=1);\n",
    "        conv7, bn7 = self.make_layers(self.ch_config[5], self.ch_config[6], k_size, padding=1);\n",
    "        conv8, bn8 = self.make_layers(self.ch_config[6], self.ch_config[7], k_size, padding=1);\n",
    "        conv9, bn9 = self.make_layers(self.ch_config[7], self.ch_config[8], k_size, padding=1);\n",
    "        conv10, bn10 = self.make_layers(self.ch_config[8], self.ch_config[9], k_size, padding=1);\n",
    "        conv11, bn11 = self.make_layers(self.ch_config[9], self.ch_config[10], k_size, padding=1);\n",
    "        conv12, bn12 = self.make_layers(self.ch_config[10], self.ch_config[11], (1, 1));\n",
    "        fcn = nn.Linear(fcn_no_of_inputs, n_class);\n",
    "        nn.init.kaiming_normal_(fcn.weight, nonlinearity='sigmoid') # kaiming with sigoid is equivalent to lecun_normal in keras\n",
    "\n",
    "        self.sfeb = nn.Sequential(\n",
    "            #Start: Filter bank\n",
    "            conv1, bn1, nn.ReLU(),\\\n",
    "            conv2, bn2, nn.ReLU(),\\\n",
    "            nn.MaxPool2d(kernel_size=(1, sfeb_pool_size))\n",
    "        );\n",
    "\n",
    "        tfeb_modules = [];\n",
    "        self.tfeb_width = int(((self.input_length / sr)*1000)/10); # 10ms frames of audio length in seconds\n",
    "        tfeb_pool_sizes = self.get_tfeb_pool_sizes(self.ch_config[1], self.tfeb_width);\n",
    "        p_index = 0;\n",
    "        for i in [3,4,6,8,10]:\n",
    "            tfeb_modules.extend([eval('conv{}'.format(i)), eval('bn{}'.format(i)), nn.ReLU()]);\n",
    "\n",
    "            if i != 3:\n",
    "                tfeb_modules.extend([eval('conv{}'.format(i+1)), eval('bn{}'.format(i+1)), nn.ReLU()]);\n",
    "\n",
    "            h, w = tfeb_pool_sizes[p_index];\n",
    "            if h>1 or w>1:\n",
    "                tfeb_modules.append(nn.MaxPool2d(kernel_size = (h,w)));\n",
    "            p_index += 1;\n",
    "\n",
    "        tfeb_modules.append(nn.Dropout(0.2));\n",
    "        tfeb_modules.extend([conv12, bn12, nn.ReLU()]);\n",
    "        h, w = tfeb_pool_sizes[-1];\n",
    "        if h>1 or w>1:\n",
    "            tfeb_modules.append(nn.AvgPool2d(kernel_size = (h,w)));\n",
    "        tfeb_modules.extend([nn.Flatten(), fcn]);\n",
    "\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules);\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Softmax(dim=1)\n",
    "        );\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sfeb(x);\n",
    "        #swapaxes\n",
    "        x = x.permute((0, 2, 1, 3));\n",
    "        x = self.tfeb(x);\n",
    "        y = self.output[0](x);\n",
    "        return y;\n",
    "\n",
    "    def make_layers(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias);\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu'); # kaiming with relu is equivalent to he_normal in keras\n",
    "        bn = nn.BatchNorm2d(out_channels);\n",
    "        return conv, bn;\n",
    "\n",
    "    def get_tfeb_pool_sizes(self, con2_ch, width):\n",
    "        h = self.get_tfeb_pool_size_component(con2_ch);\n",
    "        w = self.get_tfeb_pool_size_component(width);\n",
    "        # print(w);\n",
    "        pool_size = [];\n",
    "        for  (h1, w1) in zip(h, w):\n",
    "            pool_size.append((h1, w1));\n",
    "        return pool_size;\n",
    "\n",
    "    def get_tfeb_pool_size_component(self, length):\n",
    "        # print(length);\n",
    "        c = [];\n",
    "        index = 1;\n",
    "        while index <= 6:\n",
    "            if length >= 2:\n",
    "                if index == 6:\n",
    "                    c.append(length);\n",
    "                else:\n",
    "                    c.append(2);\n",
    "                    length = length // 2;\n",
    "            else:\n",
    "               c.append(1);\n",
    "\n",
    "            index += 1;\n",
    "\n",
    "        return c;\n",
    "\n",
    "def GetACDNetModel(input_len=30225, nclass=50, sr=20000, channel_config=None):\n",
    "    net = ACDNetV2(input_len, nclass, sr, ch_conf=channel_config);\n",
    "    return net;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d298ee4-f3f7-4001-8224-54b8817a2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;\n",
    "###########################################\n",
    "\n",
    "class Customed_ACDNetV2(nn.Module):\n",
    "    def __init__(self, input_length, n_class, sr, ch_conf=None):\n",
    "        super(Customed_ACDNetV2, self).__init__();\n",
    "        self.input_length = input_length;\n",
    "        self.ch_config = ch_conf;\n",
    "\n",
    "        stride1 = 2;\n",
    "        stride2 = 2;\n",
    "        channels = 8;\n",
    "        k_size = (3, 3);\n",
    "        n_frames = (sr/1000)*10; #No of frames per 10ms\n",
    "\n",
    "        sfeb_pool_size = int(n_frames/(stride1*stride2));\n",
    "        # tfeb_pool_size = (2,2);\n",
    "        if self.ch_config is None:\n",
    "            self.ch_config = [channels, channels*8, channels*4, channels*8, channels*8, channels*16, channels*16, channels*32, channels*32, channels*64, channels*64, n_class];\n",
    "        # avg_pool_kernel_size = (1,4) if self.ch_config[1] < 64 else (2,4);\n",
    "        fcn_no_of_inputs =  2 #self.ch_config[-1];\n",
    "        ch_confing_10 = 512 #8 * 64\n",
    "        ch_n_class = 2\n",
    "        conv1, bn1 = self.make_layers(1, self.ch_config[0], (1, 9), (1, stride1));\n",
    "        conv2, bn2 = self.make_layers(self.ch_config[0], self.ch_config[1], (1, 5), (1, stride2));\n",
    "        conv3, bn3 = self.make_layers(1, self.ch_config[2], k_size, padding=1);\n",
    "        conv4, bn4 = self.make_layers(self.ch_config[2], self.ch_config[3], k_size, padding=1);\n",
    "        conv5, bn5 = self.make_layers(self.ch_config[3], self.ch_config[4], k_size, padding=1);\n",
    "        conv6, bn6 = self.make_layers(self.ch_config[4], self.ch_config[5], k_size, padding=1);\n",
    "        conv7, bn7 = self.make_layers(self.ch_config[5], self.ch_config[6], k_size, padding=1);\n",
    "        conv8, bn8 = self.make_layers(self.ch_config[6], self.ch_config[7], k_size, padding=1);\n",
    "        conv9, bn9 = self.make_layers(self.ch_config[7], self.ch_config[8], k_size, padding=1);\n",
    "        conv10, bn10 = self.make_layers(self.ch_config[8], self.ch_config[9], k_size, padding=1);\n",
    "        conv11, bn11 = self.make_layers(self.ch_config[9], self.ch_config[10], k_size, padding=1);\n",
    "        conv12, bn12 = self.make_layers(ch_confing_10, ch_n_class, (1, 1));\n",
    "        fcn = nn.Linear(fcn_no_of_inputs, ch_n_class);\n",
    "        nn.init.kaiming_normal_(fcn.weight, nonlinearity='sigmoid') # kaiming with sigoid is equivalent to lecun_normal in keras\n",
    "\n",
    "        self.sfeb = nn.Sequential(\n",
    "            #Start: Filter bank\n",
    "            conv1, bn1, nn.ReLU(),\\\n",
    "            conv2, bn2, nn.ReLU(),\\\n",
    "            nn.MaxPool2d(kernel_size=(1, sfeb_pool_size))\n",
    "        );\n",
    "\n",
    "        tfeb_modules = [];\n",
    "        self.tfeb_width = int(((self.input_length / sr)*1000)/10); # 10ms frames of audio length in seconds\n",
    "        tfeb_pool_sizes = self.get_tfeb_pool_sizes(self.ch_config[1], self.tfeb_width);\n",
    "        p_index = 0;\n",
    "        for i in [3,4,6,8,10]:\n",
    "            tfeb_modules.extend([eval('conv{}'.format(i)), eval('bn{}'.format(i)), nn.ReLU()]);\n",
    "\n",
    "            if i != 3:\n",
    "                tfeb_modules.extend([eval('conv{}'.format(i+1)), eval('bn{}'.format(i+1)), nn.ReLU()]);\n",
    "\n",
    "            h, w = tfeb_pool_sizes[p_index];\n",
    "            if h>1 or w>1:\n",
    "                tfeb_modules.append(nn.MaxPool2d(kernel_size = (h,w)));\n",
    "            p_index += 1;\n",
    "\n",
    "        tfeb_modules.append(nn.Dropout(0.2));\n",
    "        tfeb_modules.extend([conv12, bn12, nn.ReLU()]);\n",
    "        h, w = tfeb_pool_sizes[-1];\n",
    "        if h>1 or w>1:\n",
    "            tfeb_modules.append(nn.AvgPool2d(kernel_size = (2,4)));\n",
    "        tfeb_modules.extend([nn.Flatten(), fcn]);\n",
    "\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules);\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Softmax(dim=1)\n",
    "        );\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"sfeb:\\n{list(self.sfeb.children())}\");\n",
    "        # print(f\"input x shape:{x.size()}\");\n",
    "        \"\"\"\n",
    "        input dim should be input x shape:torch.Size([32, 1, 1, 30225])\n",
    "        if you got input x shape:[32, 30225, 1, 1], that is wrong.\n",
    "        \"\"\"\n",
    "        x = self.sfeb(x);\n",
    "        #swapaxes\n",
    "        x = x.permute((0, 2, 1, 3));\n",
    "        x = self.tfeb(x);\n",
    "        y = self.output[0](x);\n",
    "        return y;\n",
    "\n",
    "    def make_layers(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias);\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu'); # kaiming with relu is equivalent to he_normal in keras\n",
    "        bn = nn.BatchNorm2d(out_channels);\n",
    "        return conv, bn;\n",
    "\n",
    "    def get_tfeb_pool_sizes(self, con2_ch, width):\n",
    "        h = self.get_tfeb_pool_size_component(con2_ch);\n",
    "        w = self.get_tfeb_pool_size_component(width);\n",
    "        # print(w);\n",
    "        pool_size = [];\n",
    "        for  (h1, w1) in zip(h, w):\n",
    "            pool_size.append((h1, w1));\n",
    "        return pool_size;\n",
    "\n",
    "    def get_tfeb_pool_size_component(self, length):\n",
    "        # print(length);\n",
    "        c = [];\n",
    "        index = 1;\n",
    "        while index <= 6:\n",
    "            if length >= 2:\n",
    "                if index == 6:\n",
    "                    c.append(length);\n",
    "                else:\n",
    "                    c.append(2);\n",
    "                    length = length // 2;\n",
    "            else:\n",
    "               c.append(1);\n",
    "\n",
    "            index += 1;\n",
    "\n",
    "        return c;\n",
    "\n",
    "def GetCustomedACDNetModel(input_len=30225, nclass=2, sr=20000, channel_config=None):\n",
    "    net = Customed_ACDNetV2(input_len, nclass, sr, ch_conf=channel_config);\n",
    "    return net;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d0fab78-4fcd-4cd2-88c8-032daf68fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acdnet_model = GetCustomedACDNetModel()\n",
    "# pretrain_weight= torch.load('./th/resources/pretrained_models/acdnet_20khz_trained_model_fold4_91.00.pt', map_location=torch.device('cpu'))['weight']\n",
    "# model_state = acdnet_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c8d8d4-86db-4d94-9910-b970ac6434d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(acdnet_model)\n",
    "# print(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22423f68-d925-450f-a24b-567422ef736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='../datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 32;\n",
    "    opt.weightDecay = 5e-3;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.nEpochs = 1000;#2000;\n",
    "    opt.LR = 0.1;\n",
    "    opt.schedule = [0.3, 0.6, 0.9];\n",
    "    opt.warmup = 10;\n",
    "\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 2#50;\n",
    "    opt.nFolds = 1;#5;\n",
    "    opt.split = 1#[i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48a4c5d5-e2b4-4775-b7f6-4de0c5147e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None):\n",
    "    # dataset = np.load(os.path.join(opt.data, opt.dataset, 'wav{}.npz'.format(opt.sr // 1000)), allow_pickle=True);\n",
    "    # dataset = np.load(\"../datasets/fold1_test16000.npz\", allow_pickle=True);\n",
    "    dataset = np.load(\"../datasets/forOneClassModel_alarm/train/trainSet_20240119002902.npz\", allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # print(len(dataset['x']))\n",
    "    # for i in range(1, opt.nFolds + 1):\n",
    "\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['fold{}'.format(1)].item()['sounds']\n",
    "    train_labels = dataset['fold{}'.format(1)].item()['labels']\n",
    "    # print(train_sounds)\n",
    "\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec8011c4-e99c-4675-b692-204ee0cfd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruningTrainer:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt;\n",
    "        #Conditional compression settings\n",
    "        self.opt.LR = 0.01;\n",
    "        self.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "        self.opt.warmup = 0;\n",
    "        self.opt.prune_ratio = 0.75#0.85;\n",
    "        self.opt.prune_algo = 'l0norm';\n",
    "        self.opt.prune_interval = 1;\n",
    "        self.opt.nEpochs = 1000;\n",
    "\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.bestAcc = 0.0;\n",
    "        self.bestAccEpoch = 0;\n",
    "        self.trainGen = getTrainGen(opt)#train_generator.setup(self.opt, self.opt.split);\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "        self.start_time = time.time();\n",
    "\n",
    "    def PruneAndTrain(self):\n",
    "        self.load_test_data();\n",
    "        print(self.device);\n",
    "        loss_func = torch.nn.KLDivLoss(reduction='batchmean');\n",
    "\n",
    "        #Load saved model dict\n",
    "        net = GetCustomedACDNetModel()#GetACDNetModel()\n",
    "        net.load_state_dict(torch.load(\"./trained_models/current_best/acdnet_alarm_3rd_20240119093633_acc_97.7272720336914_190th_epoch.pt\", map_location=self.device)['weight']);\n",
    "        calc.summary(net, (1,1,self.opt.inputLength))\n",
    "        \n",
    "        # dir = os.getcwd();\n",
    "        # net = models.GetACDNetModel().to(self.device);\n",
    "        # file_paths = glob.glob(self.opt.model_path);\n",
    "        # if len(file_paths)>0 and os.path.isfile(file_paths[0]):\n",
    "        #     net.load_state_dict(torch.load(file_paths[0], map_location=self.device)['weight']);\n",
    "        #     print('Model Loaded from: {}'.format(file_paths[0]));\n",
    "        # else:\n",
    "        #     print('Model is not found at: {}'.format(net_path));\n",
    "        #     exit();\n",
    "\n",
    "        net.cuda();\n",
    "        net.eval();\n",
    "        val_acc, val_loss = self.__validate(net, loss_func);\n",
    "        print('Testing - Val: Loss {:.3f}  Acc(top1) {:.3f}%'.format(val_loss, val_acc));\n",
    "        net.train();\n",
    "\n",
    "        optimizer = optim.SGD(net.parameters(), lr=self.opt.LR, weight_decay=self.opt.weightDecay, momentum=self.opt.momentum, nesterov=True)\n",
    "\n",
    "        weight_name = [\"weight\"]# if not self.opt.factorize else [\"weightA\", \"weightB\", \"weightC\"]\n",
    "        layers_n = weight_pruner.layers_n(net, param_name=[\"weight\"])[1];\n",
    "        all_num = sum(layers_n.values());\n",
    "        print(\"\\t TOTAL PRUNABLE PARAMS: {}\".format(all_num));\n",
    "        print(\"\\t PRUNE RATIO :{}\".format(self.opt.prune_ratio));\n",
    "        sparse_factor = int(all_num * (1-self.opt.prune_ratio));\n",
    "        print(\"\\t SPARSE FACTOR: {}\".format(sparse_factor));\n",
    "        model_size = (sparse_factor * 4)/1024**2;\n",
    "        print(\"\\t MODEL SIZE: {:.2f} MB\".format(model_size));\n",
    "        prune_algo = getattr(weight_pruner, self.opt.prune_algo);\n",
    "        prune_func = lambda m: prune_algo(m, sparse_factor, param_name=weight_name);\n",
    "\n",
    "        for epoch_idx in range(self.opt.nEpochs):\n",
    "            epoch_start_time = time.time();\n",
    "            optimizer.param_groups[0]['lr'] = self.__get_lr(epoch_idx+1);\n",
    "            cur_lr = optimizer.param_groups[0]['lr'];\n",
    "            running_loss = 0.0;\n",
    "            running_acc = 0.0;\n",
    "            n_batches = math.ceil(len(self.trainGen.data)/self.opt.batchSize);\n",
    "            net.train();\n",
    "            for batch_idx in range(n_batches):\n",
    "                # with torch.no_grad():\n",
    "                x,y = self.trainGen.__getitem__(batch_idx)\n",
    "                x = torch.tensor(np.moveaxis(x, 3, 1)).to(self.device);\n",
    "                y = torch.tensor(y).to(self.device);\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad();\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = net(x);\n",
    "                running_acc += (((outputs.data.argmax(dim=1) == y.argmax(dim=1))*1).float().mean()).item();\n",
    "                loss = loss_func(outputs.log(), y);\n",
    "\n",
    "                loss.backward();\n",
    "                optimizer.step();\n",
    "\n",
    "                running_loss += loss.item();\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    prune_func(net);\n",
    "\n",
    "            prune_func(net)\n",
    "\n",
    "            tr_acc = (running_acc / n_batches)*100;\n",
    "            tr_loss = running_loss / n_batches;\n",
    "\n",
    "            #Epoch wise validation Validation\n",
    "            epoch_train_time = time.time() - epoch_start_time;\n",
    "            net.eval();\n",
    "            val_acc, val_loss = self.__validate(net, loss_func);\n",
    "            #Save best model\n",
    "            self.__save_model(val_acc, epoch_idx, net);\n",
    "\n",
    "            self.__on_epoch_end(epoch_start_time, epoch_train_time, epoch_idx, cur_lr, tr_loss, tr_acc, val_loss, val_acc);\n",
    "\n",
    "            running_loss = 0;\n",
    "            running_acc = 0;\n",
    "            net.train();\n",
    "\n",
    "        total_time_taken = time.time() - self.start_time;\n",
    "        print(\"Execution finished in: {}\".format(U.to_hms(total_time_taken)));\n",
    "\n",
    "    def load_test_data(self):\n",
    "        if(self.testX is None):\n",
    "            data = np.load(\"../datasets/forOneClassModel_alarm/test_val/final_val_test_npz/final_valSet_20240119004614.npz\", allow_pickle=True);\n",
    "            dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "            self.testX = torch.tensor(dataX).to(self.device);\n",
    "            self.testY = torch.tensor(data['y']).to(self.device);\n",
    "\n",
    "    def __get_lr(self, epoch):\n",
    "        divide_epoch = np.array([self.opt.nEpochs * i for i in self.opt.schedule]);\n",
    "        decay = sum(epoch > divide_epoch);\n",
    "        if epoch <= self.opt.warmup:\n",
    "            decay = 1;\n",
    "        return self.opt.LR * np.power(0.1, decay);\n",
    "\n",
    "    def __validate(self, net, lossFunc):\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops\n",
    "            for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "                x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "                scores = net(x);\n",
    "                y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "\n",
    "            acc, loss = self.__compute_accuracy(y_pred, self.testY, lossFunc);\n",
    "        return acc, loss;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def __compute_accuracy(self, y_pred, y_target, lossFunc):\n",
    "        with torch.no_grad():\n",
    "            #Reshape to shape theme like each sample comtains 10 samples, calculate mean and find theindices that has highest average value for each sample\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "            # valLossFunc = torch.nn.KLDivLoss();\n",
    "            loss = lossFunc(y_pred.float().log(), y_target.float()).item();\n",
    "            # loss = 0.0;\n",
    "        return acc, loss;\n",
    "\n",
    "    def __on_epoch_end(self, epoch_start_time, train_time, epochIdx, lr, tr_loss, tr_acc, val_loss, val_acc):\n",
    "        epoch_time = time.time() - epoch_start_time;\n",
    "        val_time = epoch_time - train_time;\n",
    "        total_time = time.time() - self.start_time;\n",
    "        line = '{} Epoch: {}/{} | Time: {} (Train {}  Val {}) | Train: LR {}  Loss {:.2f}  Acc {:.2f}% | Val: Loss {:.2f}  Acc(top1) {:.2f}% | HA {:.2f}@{}\\n'.format(\n",
    "            U.to_hms(total_time), epochIdx+1, self.opt.nEpochs, U.to_hms(epoch_time), U.to_hms(train_time), U.to_hms(val_time),\n",
    "            lr, tr_loss, tr_acc, val_loss, val_acc, self.bestAcc, self.bestAccEpoch);\n",
    "        # print(line)\n",
    "        sys.stdout.write(line);\n",
    "        sys.stdout.flush();\n",
    "\n",
    "    def __save_model(self, acc, epochIdx, net):\n",
    "        if acc > self.bestAcc:\n",
    "            # dir = os.getcwd();\n",
    "            old_model = self.opt.model_name;#fname.format(dir, self.opt.model_name.lower());\n",
    "            if os.path.isfile(old_model):\n",
    "                os.remove(old_model);\n",
    "            self.bestAcc = acc;\n",
    "            self.bestAccEpoch = epochIdx +1;\n",
    "            torch.save({'weight':net.state_dict(), 'config':net.ch_config}, self.opt.model_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb69075-8799-469d-b9c1-a545cee964d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9fd15f0-cbdf-4b2f-b3c9-96f694eb4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    opt = getOpts()\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    opt.trainer = None\n",
    "    opt.prune_ratio = 0.85\n",
    "    # import torch;\n",
    "    opt.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "    # tlopts.display_info(opt)\n",
    "    # opt.model_name = \"model_stage1_prunned_from_acc_{}_{}\";\n",
    "    # valid_path = False;\n",
    "    opt.model_name = \"../th/pruned_models/first_stage_pruning/acdne_3rd_97.7_purn_{}.pt\".format(genDataTimeStr())\n",
    "    print(\"Initializing PruneAndTrain Object.....\")\n",
    "    trainer = PruningTrainer(opt)#TLTrainer(opt)\n",
    "    print(\"Start to pruning.....\")\n",
    "    trainer.PruneAndTrain();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8780ef94-ad0c-46bc-abb9-a5573b8b3480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PruneAndTrain Object.....\n",
      "length of samples:325\n",
      "Start to pruning.....\n",
      "cuda:0\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (8, 1, 15109)         72    1,087,848\n",
      "  BatchNorm2d-2     (8, 1, 15109)     (8, 1, 15109)         16            0\n",
      "         ReLu-3     (8, 1, 15109)     (8, 1, 15109)          0      120,872\n",
      "       Conv2d-4     (8, 1, 15109)     (64, 1, 7553)      2,560   19,335,680\n",
      "  BatchNorm2d-5     (64, 1, 7553)     (64, 1, 7553)        128            0\n",
      "         ReLu-6     (64, 1, 7553)     (64, 1, 7553)          0      483,392\n",
      "    MaxPool2d-7     (64, 1, 7553)      (64, 1, 151)          0      483,200\n",
      "      Permute-8      (64, 1, 151)      (1, 64, 151)          0            0\n",
      "       Conv2d-9      (1, 64, 151)     (32, 64, 151)        288    2,783,232\n",
      " BatchNorm2d-10     (32, 64, 151)     (32, 64, 151)         64            0\n",
      "        ReLu-11     (32, 64, 151)     (32, 64, 151)          0      309,248\n",
      "   MaxPool2d-12     (32, 64, 151)      (32, 32, 75)          0      307,200\n",
      "      Conv2d-13      (32, 32, 75)      (64, 32, 75)     18,432   44,236,800\n",
      " BatchNorm2d-14      (64, 32, 75)      (64, 32, 75)        128            0\n",
      "        ReLu-15      (64, 32, 75)      (64, 32, 75)          0      153,600\n",
      "      Conv2d-16      (64, 32, 75)      (64, 32, 75)     36,864   88,473,600\n",
      " BatchNorm2d-17      (64, 32, 75)      (64, 32, 75)        128            0\n",
      "        ReLu-18      (64, 32, 75)      (64, 32, 75)          0      153,600\n",
      "   MaxPool2d-19      (64, 32, 75)      (64, 16, 37)          0      151,552\n",
      "      Conv2d-20      (64, 16, 37)     (128, 16, 37)     73,728   43,646,976\n",
      " BatchNorm2d-21     (128, 16, 37)     (128, 16, 37)        256            0\n",
      "        ReLu-22     (128, 16, 37)     (128, 16, 37)          0       75,776\n",
      "      Conv2d-23     (128, 16, 37)     (128, 16, 37)    147,456   87,293,952\n",
      " BatchNorm2d-24     (128, 16, 37)     (128, 16, 37)        256            0\n",
      "        ReLu-25     (128, 16, 37)     (128, 16, 37)          0       75,776\n",
      "   MaxPool2d-26     (128, 16, 37)      (128, 8, 18)          0       73,728\n",
      "      Conv2d-27      (128, 8, 18)      (256, 8, 18)    294,912   42,467,328\n",
      " BatchNorm2d-28      (256, 8, 18)      (256, 8, 18)        512            0\n",
      "        ReLu-29      (256, 8, 18)      (256, 8, 18)          0       36,864\n",
      "      Conv2d-30      (256, 8, 18)      (256, 8, 18)    589,824   84,934,656\n",
      " BatchNorm2d-31      (256, 8, 18)      (256, 8, 18)        512            0\n",
      "        ReLu-32      (256, 8, 18)      (256, 8, 18)          0       36,864\n",
      "   MaxPool2d-33      (256, 8, 18)       (256, 4, 9)          0       36,864\n",
      "      Conv2d-34       (256, 4, 9)       (512, 4, 9)  1,179,648   42,467,328\n",
      " BatchNorm2d-35       (512, 4, 9)       (512, 4, 9)      1,024            0\n",
      "        ReLu-36       (512, 4, 9)       (512, 4, 9)          0       18,432\n",
      "      Conv2d-37       (512, 4, 9)       (512, 4, 9)  2,359,296   84,934,656\n",
      " BatchNorm2d-38       (512, 4, 9)       (512, 4, 9)      1,024            0\n",
      "        ReLu-39       (512, 4, 9)       (512, 4, 9)          0       18,432\n",
      "   MaxPool2d-40       (512, 4, 9)       (512, 2, 4)          0       16,384\n",
      "      Conv2d-41       (512, 2, 4)         (2, 2, 4)      1,024        8,192\n",
      " BatchNorm2d-42         (2, 2, 4)         (2, 2, 4)          4            0\n",
      "        ReLu-43         (2, 2, 4)         (2, 2, 4)          0           16\n",
      "   AvgPool2d-44         (2, 2, 4)         (2, 1, 1)          0           16\n",
      "     Flatten-45         (2, 1, 1)            (1, 2)          0            0\n",
      "      Linear-46            (1, 2)            (1, 2)          6            6\n",
      "     Softmax-47            (1, 2)            (1, 2)          0            2\n",
      "==============================================================================\n",
      "Total Params: 4,708,162\n",
      "Total FLOPs : 544,222,072\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 17.96\n",
      "Total size (MB) : 18.08\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Testing - Val: Loss nan  Acc(top1) 97.727%\n",
      "\t TOTAL PRUNABLE PARAMS: 4704108\n",
      "\t PRUNE RATIO :0.75\n",
      "\t SPARSE FACTOR: 1176027\n",
      "\t MODEL SIZE: 4.49 MB\n",
      "0m02s Epoch: 1/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.11  Acc 74.43% | Val: Loss nan  Acc(top1) 95.45% | HA 95.45@1\n",
      "0m05s Epoch: 2/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.11  Acc 78.12% | Val: Loss nan  Acc(top1) 93.18% | HA 95.45@1\n",
      "0m07s Epoch: 3/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.09  Acc 78.12% | Val: Loss nan  Acc(top1) 95.45% | HA 95.45@1\n",
      "0m10s Epoch: 4/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.10  Acc 79.26% | Val: Loss nan  Acc(top1) 95.45% | HA 95.45@1\n",
      "0m12s Epoch: 5/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.10  Acc 78.98% | Val: Loss nan  Acc(top1) 95.45% | HA 95.45@1\n",
      "0m14s Epoch: 6/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.09  Acc 79.83% | Val: Loss nan  Acc(top1) 95.45% | HA 95.45@1\n",
      "0m17s Epoch: 7/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.09  Acc 80.40% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "0m19s Epoch: 8/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.09  Acc 79.83% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "0m22s Epoch: 9/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.09  Acc 81.53% | Val: Loss nan  Acc(top1) 94.32% | HA 96.59@7\n",
      "0m24s Epoch: 10/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.09  Acc 80.97% | Val: Loss nan  Acc(top1) 94.32% | HA 96.59@7\n",
      "0m26s Epoch: 11/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 81.53% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "0m29s Epoch: 12/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "0m31s Epoch: 13/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 81.53% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "0m34s Epoch: 14/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.09  Acc 81.25% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "0m36s Epoch: 15/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 84.09% | Val: Loss nan  Acc(top1) 94.32% | HA 96.59@7\n",
      "0m38s Epoch: 16/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 78.69% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "0m41s Epoch: 17/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.09  Acc 82.95% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "0m43s Epoch: 18/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 80.68% | Val: Loss nan  Acc(top1) 94.32% | HA 96.59@7\n",
      "0m46s Epoch: 19/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 82.39% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "0m48s Epoch: 20/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 85.23% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "0m50s Epoch: 21/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 83.81% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "0m53s Epoch: 22/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 82.10% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "0m55s Epoch: 23/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 82.10% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "0m58s Epoch: 24/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 81.53% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m00s Epoch: 25/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m02s Epoch: 26/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 83.24% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m05s Epoch: 27/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 78.12% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m07s Epoch: 28/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 78.69% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m10s Epoch: 29/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 82.10% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m12s Epoch: 30/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 81.82% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m14s Epoch: 31/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 80.68% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m17s Epoch: 32/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 82.39% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m19s Epoch: 33/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m22s Epoch: 34/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 83.81% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m24s Epoch: 35/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m26s Epoch: 36/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m29s Epoch: 37/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 84.09% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "1m31s Epoch: 38/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m34s Epoch: 39/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 82.39% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "1m36s Epoch: 40/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 82.10% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m38s Epoch: 41/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m41s Epoch: 42/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m43s Epoch: 43/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m46s Epoch: 44/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m48s Epoch: 45/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 81.25% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m50s Epoch: 46/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 83.81% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m53s Epoch: 47/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 80.40% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "1m55s Epoch: 48/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 82.95% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "1m58s Epoch: 49/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 82.95% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m00s Epoch: 50/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m02s Epoch: 51/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m05s Epoch: 52/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 82.95% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m07s Epoch: 53/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 81.25% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m10s Epoch: 54/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m12s Epoch: 55/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m14s Epoch: 56/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 81.82% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m17s Epoch: 57/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m19s Epoch: 58/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m22s Epoch: 59/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m24s Epoch: 60/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m26s Epoch: 61/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 85.23% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "2m29s Epoch: 62/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 82.39% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m31s Epoch: 63/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 82.10% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m34s Epoch: 64/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 82.95% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "2m36s Epoch: 65/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 80.97% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "2m38s Epoch: 66/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "2m41s Epoch: 67/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m43s Epoch: 68/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 82.39% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m46s Epoch: 69/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m48s Epoch: 70/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 83.24% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m50s Epoch: 71/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m53s Epoch: 72/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.08  Acc 81.82% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m55s Epoch: 73/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "2m58s Epoch: 74/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 84.66% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "3m00s Epoch: 75/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 81.53% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "3m02s Epoch: 76/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 82.67% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m05s Epoch: 77/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m07s Epoch: 78/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m10s Epoch: 79/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 82.39% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m12s Epoch: 80/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m14s Epoch: 81/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m17s Epoch: 82/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m19s Epoch: 83/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m22s Epoch: 84/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 82.67% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m24s Epoch: 85/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m26s Epoch: 86/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "3m29s Epoch: 87/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "3m31s Epoch: 88/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 81.53% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m33s Epoch: 89/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 80.68% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m36s Epoch: 90/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 82.95% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m38s Epoch: 91/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 83.24% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m41s Epoch: 92/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m43s Epoch: 93/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "3m45s Epoch: 94/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 82.67% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "3m48s Epoch: 95/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 85.80% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "3m50s Epoch: 96/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m53s Epoch: 97/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m55s Epoch: 98/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "3m57s Epoch: 99/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m00s Epoch: 100/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 80.11% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m02s Epoch: 101/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m05s Epoch: 102/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "4m07s Epoch: 103/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.07  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m09s Epoch: 104/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m12s Epoch: 105/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m14s Epoch: 106/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "4m17s Epoch: 107/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 84.09% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "4m19s Epoch: 108/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m21s Epoch: 109/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "4m24s Epoch: 110/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "4m26s Epoch: 111/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m29s Epoch: 112/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m31s Epoch: 113/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 91.19% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m33s Epoch: 114/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m36s Epoch: 115/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m38s Epoch: 116/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m40s Epoch: 117/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m43s Epoch: 118/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m45s Epoch: 119/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m48s Epoch: 120/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "4m50s Epoch: 121/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m52s Epoch: 122/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m55s Epoch: 123/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "4m57s Epoch: 124/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 84.38% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "5m00s Epoch: 125/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m02s Epoch: 126/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m04s Epoch: 127/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m07s Epoch: 128/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "5m09s Epoch: 129/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.04  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m12s Epoch: 130/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 90.62% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m14s Epoch: 131/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m16s Epoch: 132/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 89.20% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "5m19s Epoch: 133/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 82.95% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "5m21s Epoch: 134/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m24s Epoch: 135/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 88.07% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "5m26s Epoch: 136/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m28s Epoch: 137/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m31s Epoch: 138/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m33s Epoch: 139/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m36s Epoch: 140/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 82.67% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m38s Epoch: 141/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m40s Epoch: 142/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "5m43s Epoch: 143/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m45s Epoch: 144/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m48s Epoch: 145/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m50s Epoch: 146/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m52s Epoch: 147/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "5m55s Epoch: 148/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "5m57s Epoch: 149/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "6m00s Epoch: 150/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.01  Loss 0.06  Acc 83.52% | Val: Loss nan  Acc(top1) 95.45% | HA 96.59@7\n",
      "6m02s Epoch: 151/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 83.81% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "6m05s Epoch: 152/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 96.59@7\n",
      "6m07s Epoch: 153/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 97.73% | HA 97.73@153\n",
      "6m09s Epoch: 154/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m12s Epoch: 155/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m14s Epoch: 156/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m17s Epoch: 157/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m19s Epoch: 158/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m21s Epoch: 159/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m24s Epoch: 160/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m26s Epoch: 161/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m28s Epoch: 162/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m31s Epoch: 163/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m33s Epoch: 164/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m36s Epoch: 165/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m38s Epoch: 166/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m40s Epoch: 167/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m43s Epoch: 168/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m45s Epoch: 169/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m48s Epoch: 170/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m50s Epoch: 171/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m52s Epoch: 172/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m55s Epoch: 173/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "6m57s Epoch: 174/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m00s Epoch: 175/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m02s Epoch: 176/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m04s Epoch: 177/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m07s Epoch: 178/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 95.45% | HA 97.73@153\n",
      "7m09s Epoch: 179/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 90.62% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m12s Epoch: 180/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m14s Epoch: 181/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m16s Epoch: 182/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m19s Epoch: 183/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m21s Epoch: 184/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m23s Epoch: 185/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m26s Epoch: 186/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m28s Epoch: 187/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m31s Epoch: 188/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m33s Epoch: 189/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m35s Epoch: 190/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 83.24% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m38s Epoch: 191/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m40s Epoch: 192/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m43s Epoch: 193/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m45s Epoch: 194/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m47s Epoch: 195/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m50s Epoch: 196/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m52s Epoch: 197/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m55s Epoch: 198/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.04  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m57s Epoch: 199/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "7m59s Epoch: 200/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m02s Epoch: 201/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m04s Epoch: 202/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m07s Epoch: 203/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m09s Epoch: 204/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m11s Epoch: 205/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m14s Epoch: 206/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m16s Epoch: 207/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m19s Epoch: 208/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 83.81% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m21s Epoch: 209/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m23s Epoch: 210/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m26s Epoch: 211/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m28s Epoch: 212/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m30s Epoch: 213/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m33s Epoch: 214/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m35s Epoch: 215/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m38s Epoch: 216/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m40s Epoch: 217/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m42s Epoch: 218/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m45s Epoch: 219/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m47s Epoch: 220/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m50s Epoch: 221/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m52s Epoch: 222/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.04  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m54s Epoch: 223/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m57s Epoch: 224/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "8m59s Epoch: 225/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m01s Epoch: 226/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m04s Epoch: 227/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m06s Epoch: 228/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m09s Epoch: 229/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m11s Epoch: 230/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m13s Epoch: 231/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m16s Epoch: 232/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.04  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m18s Epoch: 233/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m21s Epoch: 234/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m23s Epoch: 235/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m25s Epoch: 236/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m28s Epoch: 237/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m30s Epoch: 238/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m33s Epoch: 239/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m35s Epoch: 240/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m37s Epoch: 241/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m40s Epoch: 242/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m42s Epoch: 243/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m44s Epoch: 244/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m47s Epoch: 245/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m49s Epoch: 246/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m52s Epoch: 247/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m54s Epoch: 248/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m56s Epoch: 249/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "9m59s Epoch: 250/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m01s Epoch: 251/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m04s Epoch: 252/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m06s Epoch: 253/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m08s Epoch: 254/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m11s Epoch: 255/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m13s Epoch: 256/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m16s Epoch: 257/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m18s Epoch: 258/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m20s Epoch: 259/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m23s Epoch: 260/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m25s Epoch: 261/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 90.91% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m28s Epoch: 262/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 86.36% | Val: Loss nan  Acc(top1) 97.73% | HA 97.73@153\n",
      "10m30s Epoch: 263/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m32s Epoch: 264/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m35s Epoch: 265/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m37s Epoch: 266/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m39s Epoch: 267/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m42s Epoch: 268/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m44s Epoch: 269/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m47s Epoch: 270/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m49s Epoch: 271/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m51s Epoch: 272/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m54s Epoch: 273/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m56s Epoch: 274/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "10m59s Epoch: 275/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m01s Epoch: 276/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m03s Epoch: 277/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 83.24% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m06s Epoch: 278/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m08s Epoch: 279/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m11s Epoch: 280/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m13s Epoch: 281/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m15s Epoch: 282/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m18s Epoch: 283/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m20s Epoch: 284/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m22s Epoch: 285/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m25s Epoch: 286/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m27s Epoch: 287/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m30s Epoch: 288/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.04  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m32s Epoch: 289/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m34s Epoch: 290/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.04  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m37s Epoch: 291/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m39s Epoch: 292/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m42s Epoch: 293/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m44s Epoch: 294/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m46s Epoch: 295/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.04  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m49s Epoch: 296/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m51s Epoch: 297/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m54s Epoch: 298/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m56s Epoch: 299/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "11m58s Epoch: 300/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.001  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m01s Epoch: 301/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m03s Epoch: 302/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m06s Epoch: 303/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m08s Epoch: 304/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m10s Epoch: 305/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m13s Epoch: 306/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m15s Epoch: 307/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m18s Epoch: 308/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 90.91% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m20s Epoch: 309/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m22s Epoch: 310/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m25s Epoch: 311/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m27s Epoch: 312/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m30s Epoch: 313/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.04  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m32s Epoch: 314/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m34s Epoch: 315/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m37s Epoch: 316/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m39s Epoch: 317/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m42s Epoch: 318/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m44s Epoch: 319/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m46s Epoch: 320/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.07  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m49s Epoch: 321/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m51s Epoch: 322/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m54s Epoch: 323/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m56s Epoch: 324/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "12m58s Epoch: 325/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m01s Epoch: 326/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m03s Epoch: 327/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m06s Epoch: 328/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m08s Epoch: 329/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.04  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m10s Epoch: 330/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.04  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m13s Epoch: 331/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m15s Epoch: 332/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 82.95% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m18s Epoch: 333/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m20s Epoch: 334/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m22s Epoch: 335/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m25s Epoch: 336/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m27s Epoch: 337/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m29s Epoch: 338/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m32s Epoch: 339/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m34s Epoch: 340/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m37s Epoch: 341/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m39s Epoch: 342/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 83.81% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m42s Epoch: 343/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m44s Epoch: 344/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m46s Epoch: 345/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 82.67% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m49s Epoch: 346/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m51s Epoch: 347/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m53s Epoch: 348/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 90.34% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m56s Epoch: 349/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "13m58s Epoch: 350/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.07  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m01s Epoch: 351/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m03s Epoch: 352/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.04  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m05s Epoch: 353/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m08s Epoch: 354/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m10s Epoch: 355/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m13s Epoch: 356/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m15s Epoch: 357/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m17s Epoch: 358/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m20s Epoch: 359/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m22s Epoch: 360/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m25s Epoch: 361/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m27s Epoch: 362/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m29s Epoch: 363/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m32s Epoch: 364/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m34s Epoch: 365/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m37s Epoch: 366/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m39s Epoch: 367/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 82.10% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m41s Epoch: 368/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m44s Epoch: 369/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m46s Epoch: 370/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 90.62% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m49s Epoch: 371/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m51s Epoch: 372/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m53s Epoch: 373/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m56s Epoch: 374/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 92.05% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "14m58s Epoch: 375/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m01s Epoch: 376/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m03s Epoch: 377/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m05s Epoch: 378/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m08s Epoch: 379/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m10s Epoch: 380/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m13s Epoch: 381/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m15s Epoch: 382/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m17s Epoch: 383/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m20s Epoch: 384/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m22s Epoch: 385/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.07  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m25s Epoch: 386/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m27s Epoch: 387/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m29s Epoch: 388/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m32s Epoch: 389/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m34s Epoch: 390/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m37s Epoch: 391/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m39s Epoch: 392/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m41s Epoch: 393/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m44s Epoch: 394/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m46s Epoch: 395/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m49s Epoch: 396/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.04  Acc 92.05% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m51s Epoch: 397/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m53s Epoch: 398/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m56s Epoch: 399/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 90.62% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "15m58s Epoch: 400/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m00s Epoch: 401/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m03s Epoch: 402/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m05s Epoch: 403/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m08s Epoch: 404/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m10s Epoch: 405/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m12s Epoch: 406/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m15s Epoch: 407/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m17s Epoch: 408/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m20s Epoch: 409/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m22s Epoch: 410/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 83.24% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m24s Epoch: 411/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m27s Epoch: 412/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m29s Epoch: 413/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.04  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m32s Epoch: 414/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m34s Epoch: 415/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m36s Epoch: 416/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m39s Epoch: 417/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m41s Epoch: 418/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m44s Epoch: 419/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m46s Epoch: 420/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m48s Epoch: 421/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m51s Epoch: 422/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m53s Epoch: 423/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 83.81% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m55s Epoch: 424/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "16m58s Epoch: 425/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m00s Epoch: 426/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m03s Epoch: 427/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m05s Epoch: 428/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m07s Epoch: 429/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m10s Epoch: 430/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m12s Epoch: 431/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m15s Epoch: 432/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.04  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m17s Epoch: 433/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m19s Epoch: 434/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 90.62% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m22s Epoch: 435/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m24s Epoch: 436/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m27s Epoch: 437/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m29s Epoch: 438/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m31s Epoch: 439/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m34s Epoch: 440/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m36s Epoch: 441/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m39s Epoch: 442/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m41s Epoch: 443/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m44s Epoch: 444/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m46s Epoch: 445/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m48s Epoch: 446/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m51s Epoch: 447/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m53s Epoch: 448/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m56s Epoch: 449/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 82.10% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "17m58s Epoch: 450/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m00s Epoch: 451/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 82.95% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m03s Epoch: 452/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m05s Epoch: 453/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m07s Epoch: 454/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m10s Epoch: 455/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m12s Epoch: 456/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m15s Epoch: 457/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m17s Epoch: 458/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m19s Epoch: 459/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m22s Epoch: 460/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m24s Epoch: 461/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m27s Epoch: 462/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 82.67% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m29s Epoch: 463/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m31s Epoch: 464/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m34s Epoch: 465/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m36s Epoch: 466/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m39s Epoch: 467/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m41s Epoch: 468/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 81.53% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m43s Epoch: 469/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m46s Epoch: 470/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 82.95% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m48s Epoch: 471/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m51s Epoch: 472/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m53s Epoch: 473/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m55s Epoch: 474/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "18m58s Epoch: 475/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m00s Epoch: 476/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m03s Epoch: 477/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m05s Epoch: 478/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m07s Epoch: 479/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m10s Epoch: 480/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m12s Epoch: 481/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m15s Epoch: 482/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m17s Epoch: 483/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m19s Epoch: 484/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m22s Epoch: 485/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m24s Epoch: 486/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m27s Epoch: 487/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m29s Epoch: 488/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m31s Epoch: 489/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m34s Epoch: 490/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.04  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m36s Epoch: 491/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m39s Epoch: 492/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m41s Epoch: 493/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m43s Epoch: 494/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m46s Epoch: 495/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m48s Epoch: 496/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m51s Epoch: 497/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m53s Epoch: 498/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m55s Epoch: 499/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "19m58s Epoch: 500/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m00s Epoch: 501/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m03s Epoch: 502/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.09% | Val: Loss nan  Acc(top1) 95.45% | HA 97.73@153\n",
      "20m05s Epoch: 503/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m07s Epoch: 504/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m10s Epoch: 505/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m12s Epoch: 506/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m15s Epoch: 507/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 83.24% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m17s Epoch: 508/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m19s Epoch: 509/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m22s Epoch: 510/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m24s Epoch: 511/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m27s Epoch: 512/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m29s Epoch: 513/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m31s Epoch: 514/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m34s Epoch: 515/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 90.91% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m36s Epoch: 516/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m39s Epoch: 517/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m41s Epoch: 518/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m43s Epoch: 519/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m46s Epoch: 520/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m48s Epoch: 521/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m51s Epoch: 522/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m53s Epoch: 523/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m55s Epoch: 524/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "20m58s Epoch: 525/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m00s Epoch: 526/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m02s Epoch: 527/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m05s Epoch: 528/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m07s Epoch: 529/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m10s Epoch: 530/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m12s Epoch: 531/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.04  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m14s Epoch: 532/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m17s Epoch: 533/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m19s Epoch: 534/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m22s Epoch: 535/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m24s Epoch: 536/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m26s Epoch: 537/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m29s Epoch: 538/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m31s Epoch: 539/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m34s Epoch: 540/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m36s Epoch: 541/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m38s Epoch: 542/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m41s Epoch: 543/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m43s Epoch: 544/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m46s Epoch: 545/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m48s Epoch: 546/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m50s Epoch: 547/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m53s Epoch: 548/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m55s Epoch: 549/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "21m58s Epoch: 550/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m00s Epoch: 551/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m02s Epoch: 552/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m05s Epoch: 553/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m07s Epoch: 554/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m10s Epoch: 555/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m12s Epoch: 556/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m14s Epoch: 557/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m17s Epoch: 558/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m19s Epoch: 559/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m22s Epoch: 560/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.04  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m24s Epoch: 561/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m26s Epoch: 562/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m29s Epoch: 563/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m31s Epoch: 564/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m34s Epoch: 565/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m36s Epoch: 566/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m38s Epoch: 567/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m41s Epoch: 568/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m43s Epoch: 569/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m46s Epoch: 570/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m48s Epoch: 571/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m50s Epoch: 572/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m53s Epoch: 573/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "22m55s Epoch: 574/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 95.45% | HA 97.73@153\n",
      "22m58s Epoch: 575/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m00s Epoch: 576/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m02s Epoch: 577/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m05s Epoch: 578/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m07s Epoch: 579/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m10s Epoch: 580/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m12s Epoch: 581/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m14s Epoch: 582/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m17s Epoch: 583/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m19s Epoch: 584/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m22s Epoch: 585/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m24s Epoch: 586/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m26s Epoch: 587/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m29s Epoch: 588/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m31s Epoch: 589/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 95.45% | HA 97.73@153\n",
      "23m33s Epoch: 590/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 83.81% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m36s Epoch: 591/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m38s Epoch: 592/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m41s Epoch: 593/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m43s Epoch: 594/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.07  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m45s Epoch: 595/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 82.95% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m48s Epoch: 596/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m50s Epoch: 597/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m53s Epoch: 598/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m55s Epoch: 599/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "23m57s Epoch: 600/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m00s Epoch: 601/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m02s Epoch: 602/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m05s Epoch: 603/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m07s Epoch: 604/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m09s Epoch: 605/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m12s Epoch: 606/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m14s Epoch: 607/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 95.45% | HA 97.73@153\n",
      "24m17s Epoch: 608/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m19s Epoch: 609/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m21s Epoch: 610/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m24s Epoch: 611/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m26s Epoch: 612/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m29s Epoch: 613/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m31s Epoch: 614/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 92.61% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m33s Epoch: 615/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m36s Epoch: 616/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m38s Epoch: 617/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m40s Epoch: 618/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 90.91% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m43s Epoch: 619/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m45s Epoch: 620/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m48s Epoch: 621/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m50s Epoch: 622/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m52s Epoch: 623/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.04  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m55s Epoch: 624/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "24m57s Epoch: 625/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m00s Epoch: 626/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m02s Epoch: 627/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m04s Epoch: 628/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m07s Epoch: 629/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m09s Epoch: 630/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m12s Epoch: 631/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m14s Epoch: 632/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m16s Epoch: 633/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m19s Epoch: 634/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m21s Epoch: 635/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m24s Epoch: 636/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m26s Epoch: 637/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m28s Epoch: 638/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m31s Epoch: 639/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m33s Epoch: 640/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m36s Epoch: 641/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m38s Epoch: 642/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m40s Epoch: 643/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m43s Epoch: 644/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m45s Epoch: 645/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m48s Epoch: 646/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m50s Epoch: 647/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m52s Epoch: 648/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m55s Epoch: 649/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "25m57s Epoch: 650/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m00s Epoch: 651/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 95.45% | HA 97.73@153\n",
      "26m02s Epoch: 652/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m04s Epoch: 653/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m07s Epoch: 654/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m09s Epoch: 655/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m12s Epoch: 656/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 91.19% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m14s Epoch: 657/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m16s Epoch: 658/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m19s Epoch: 659/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m21s Epoch: 660/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m24s Epoch: 661/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m26s Epoch: 662/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m28s Epoch: 663/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m31s Epoch: 664/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m33s Epoch: 665/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m36s Epoch: 666/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m38s Epoch: 667/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m40s Epoch: 668/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m43s Epoch: 669/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m45s Epoch: 670/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m48s Epoch: 671/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m50s Epoch: 672/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m52s Epoch: 673/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m55s Epoch: 674/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "26m57s Epoch: 675/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m00s Epoch: 676/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m02s Epoch: 677/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m04s Epoch: 678/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m07s Epoch: 679/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 90.34% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m09s Epoch: 680/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m12s Epoch: 681/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.04  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m14s Epoch: 682/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m16s Epoch: 683/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m19s Epoch: 684/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m21s Epoch: 685/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.04  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m24s Epoch: 686/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m26s Epoch: 687/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m28s Epoch: 688/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m31s Epoch: 689/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m33s Epoch: 690/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m35s Epoch: 691/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m38s Epoch: 692/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m40s Epoch: 693/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m43s Epoch: 694/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m45s Epoch: 695/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m47s Epoch: 696/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m50s Epoch: 697/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m52s Epoch: 698/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m55s Epoch: 699/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m57s Epoch: 700/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 82.67% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "27m59s Epoch: 701/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m02s Epoch: 702/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m04s Epoch: 703/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.04  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m07s Epoch: 704/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m09s Epoch: 705/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m11s Epoch: 706/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m14s Epoch: 707/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m16s Epoch: 708/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m19s Epoch: 709/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m21s Epoch: 710/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m23s Epoch: 711/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m26s Epoch: 712/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 92.90% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m28s Epoch: 713/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m31s Epoch: 714/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m33s Epoch: 715/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m35s Epoch: 716/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m38s Epoch: 717/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m40s Epoch: 718/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m43s Epoch: 719/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m45s Epoch: 720/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m47s Epoch: 721/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m50s Epoch: 722/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m52s Epoch: 723/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m55s Epoch: 724/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m57s Epoch: 725/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "28m59s Epoch: 726/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m02s Epoch: 727/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m04s Epoch: 728/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m07s Epoch: 729/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m09s Epoch: 730/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m11s Epoch: 731/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m14s Epoch: 732/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m16s Epoch: 733/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m18s Epoch: 734/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m21s Epoch: 735/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m23s Epoch: 736/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m26s Epoch: 737/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m28s Epoch: 738/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m31s Epoch: 739/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m33s Epoch: 740/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m35s Epoch: 741/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m38s Epoch: 742/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 82.95% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m40s Epoch: 743/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m43s Epoch: 744/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m45s Epoch: 745/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m47s Epoch: 746/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m50s Epoch: 747/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.06  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m52s Epoch: 748/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m54s Epoch: 749/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m57s Epoch: 750/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "29m59s Epoch: 751/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m02s Epoch: 752/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m04s Epoch: 753/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m06s Epoch: 754/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m09s Epoch: 755/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m11s Epoch: 756/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m14s Epoch: 757/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m16s Epoch: 758/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m18s Epoch: 759/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m21s Epoch: 760/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m23s Epoch: 761/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m26s Epoch: 762/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m28s Epoch: 763/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m30s Epoch: 764/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m33s Epoch: 765/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m35s Epoch: 766/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m38s Epoch: 767/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m40s Epoch: 768/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m42s Epoch: 769/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m45s Epoch: 770/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m47s Epoch: 771/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 83.81% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m49s Epoch: 772/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m52s Epoch: 773/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m54s Epoch: 774/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m57s Epoch: 775/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "30m59s Epoch: 776/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 95.45% | HA 97.73@153\n",
      "31m01s Epoch: 777/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m04s Epoch: 778/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 90.34% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m06s Epoch: 779/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m09s Epoch: 780/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m11s Epoch: 781/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m13s Epoch: 782/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m16s Epoch: 783/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m18s Epoch: 784/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m21s Epoch: 785/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m23s Epoch: 786/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m25s Epoch: 787/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m28s Epoch: 788/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m30s Epoch: 789/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m33s Epoch: 790/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m35s Epoch: 791/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m37s Epoch: 792/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m40s Epoch: 793/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m42s Epoch: 794/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m44s Epoch: 795/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m47s Epoch: 796/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m49s Epoch: 797/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m52s Epoch: 798/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m54s Epoch: 799/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m56s Epoch: 800/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "31m59s Epoch: 801/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m01s Epoch: 802/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m04s Epoch: 803/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m06s Epoch: 804/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m08s Epoch: 805/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m11s Epoch: 806/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m13s Epoch: 807/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m16s Epoch: 808/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m18s Epoch: 809/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m20s Epoch: 810/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m23s Epoch: 811/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.04  Acc 91.19% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m25s Epoch: 812/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m28s Epoch: 813/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m30s Epoch: 814/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m32s Epoch: 815/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m35s Epoch: 816/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 95.45% | HA 97.73@153\n",
      "32m37s Epoch: 817/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m39s Epoch: 818/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.04  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m42s Epoch: 819/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m44s Epoch: 820/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m47s Epoch: 821/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m49s Epoch: 822/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m51s Epoch: 823/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m54s Epoch: 824/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 82.39% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m56s Epoch: 825/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "32m59s Epoch: 826/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m01s Epoch: 827/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m03s Epoch: 828/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m06s Epoch: 829/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m08s Epoch: 830/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m11s Epoch: 831/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m13s Epoch: 832/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m16s Epoch: 833/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 83.81% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m18s Epoch: 834/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m20s Epoch: 835/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m23s Epoch: 836/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m25s Epoch: 837/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 90.34% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m28s Epoch: 838/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m30s Epoch: 839/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m32s Epoch: 840/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m35s Epoch: 841/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m37s Epoch: 842/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m40s Epoch: 843/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m42s Epoch: 844/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m44s Epoch: 845/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m47s Epoch: 846/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m49s Epoch: 847/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m51s Epoch: 848/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m54s Epoch: 849/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m56s Epoch: 850/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "33m59s Epoch: 851/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m01s Epoch: 852/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m04s Epoch: 853/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m06s Epoch: 854/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m08s Epoch: 855/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m11s Epoch: 856/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m13s Epoch: 857/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m15s Epoch: 858/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m18s Epoch: 859/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m20s Epoch: 860/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m23s Epoch: 861/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m25s Epoch: 862/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m27s Epoch: 863/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m30s Epoch: 864/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.04  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m32s Epoch: 865/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m35s Epoch: 866/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 90.91% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m37s Epoch: 867/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m39s Epoch: 868/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m42s Epoch: 869/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m44s Epoch: 870/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m47s Epoch: 871/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m49s Epoch: 872/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m51s Epoch: 873/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m54s Epoch: 874/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m56s Epoch: 875/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "34m59s Epoch: 876/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m01s Epoch: 877/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 83.52% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m03s Epoch: 878/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m06s Epoch: 879/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m08s Epoch: 880/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m11s Epoch: 881/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m13s Epoch: 882/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m15s Epoch: 883/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m18s Epoch: 884/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 90.34% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m20s Epoch: 885/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m23s Epoch: 886/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m25s Epoch: 887/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m27s Epoch: 888/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m30s Epoch: 889/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m32s Epoch: 890/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.66% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m35s Epoch: 891/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m37s Epoch: 892/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m39s Epoch: 893/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m42s Epoch: 894/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m44s Epoch: 895/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m46s Epoch: 896/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m49s Epoch: 897/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m51s Epoch: 898/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m54s Epoch: 899/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m56s Epoch: 900/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "35m58s Epoch: 901/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m01s Epoch: 902/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m03s Epoch: 903/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m06s Epoch: 904/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m08s Epoch: 905/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m10s Epoch: 906/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m13s Epoch: 907/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m15s Epoch: 908/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 97.73% | HA 97.73@153\n",
      "36m18s Epoch: 909/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.04  Acc 90.34% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m20s Epoch: 910/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.04  Acc 90.34% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m22s Epoch: 911/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m25s Epoch: 912/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m27s Epoch: 913/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m29s Epoch: 914/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m32s Epoch: 915/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m34s Epoch: 916/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 89.49% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m37s Epoch: 917/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m39s Epoch: 918/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m41s Epoch: 919/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m44s Epoch: 920/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 90.34% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m46s Epoch: 921/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m49s Epoch: 922/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m51s Epoch: 923/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m53s Epoch: 924/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.04  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m56s Epoch: 925/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "36m58s Epoch: 926/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m01s Epoch: 927/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m03s Epoch: 928/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m05s Epoch: 929/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m08s Epoch: 930/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m10s Epoch: 931/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m13s Epoch: 932/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 97.73% | HA 97.73@153\n",
      "37m15s Epoch: 933/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m17s Epoch: 934/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m20s Epoch: 935/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m22s Epoch: 936/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m25s Epoch: 937/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m27s Epoch: 938/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m29s Epoch: 939/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.77% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m32s Epoch: 940/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m34s Epoch: 941/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m37s Epoch: 942/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m39s Epoch: 943/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m41s Epoch: 944/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m44s Epoch: 945/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m46s Epoch: 946/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m49s Epoch: 947/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m51s Epoch: 948/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 97.73% | HA 97.73@153\n",
      "37m53s Epoch: 949/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m56s Epoch: 950/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "37m58s Epoch: 951/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.65% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m01s Epoch: 952/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m03s Epoch: 953/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m06s Epoch: 954/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.07  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m08s Epoch: 955/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m10s Epoch: 956/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 87.78% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m13s Epoch: 957/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m15s Epoch: 958/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m18s Epoch: 959/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m20s Epoch: 960/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m22s Epoch: 961/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m25s Epoch: 962/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m27s Epoch: 963/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m30s Epoch: 964/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m32s Epoch: 965/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m34s Epoch: 966/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m37s Epoch: 967/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m39s Epoch: 968/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m42s Epoch: 969/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m44s Epoch: 970/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m46s Epoch: 971/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m49s Epoch: 972/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 82.39% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m51s Epoch: 973/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 82.95% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m54s Epoch: 974/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 87.50% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m56s Epoch: 975/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.04  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "38m59s Epoch: 976/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 88.07% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m01s Epoch: 977/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.09% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m03s Epoch: 978/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.04  Acc 89.20% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m06s Epoch: 979/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m08s Epoch: 980/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 87.22% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m11s Epoch: 981/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m13s Epoch: 982/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m15s Epoch: 983/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 86.08% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m18s Epoch: 984/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 83.81% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m20s Epoch: 985/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 90.06% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m23s Epoch: 986/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.80% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m25s Epoch: 987/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.35% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m27s Epoch: 988/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 85.51% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m30s Epoch: 989/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.93% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m32s Epoch: 990/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.92% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m35s Epoch: 991/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 88.64% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m37s Epoch: 992/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.07  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m39s Epoch: 993/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 84.94% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m42s Epoch: 994/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m44s Epoch: 995/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 83.81% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m47s Epoch: 996/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m49s Epoch: 997/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 90.34% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m51s Epoch: 998/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 84.38% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m54s Epoch: 999/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.05  Acc 86.36% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "39m56s Epoch: 1000/1000 | Time: 0m02s (Train 0m02s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.06  Acc 85.23% | Val: Loss nan  Acc(top1) 96.59% | HA 97.73@153\n",
      "Execution finished in: 39m56s\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae31fa-cf50-449c-88b3-fdca70c66228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee28e6-b711-4642-8ac9-7e506be8ad4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
