{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692379c8-c60a-4ddb-a425-09c0b030bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import glob;\n",
    "import math;\n",
    "import random;\n",
    "import torch;\n",
    "import torch.nn as nn;\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5939dcb3-4805-42d7-9900-637088d59e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1585005-0d41-4964-a79f-63ac0a2fc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7733cacf-1504-4ce6-983c-871ee1dc116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.opts as opts;\n",
    "import th.resources.models as models;\n",
    "import th.resources.calculator as calc;\n",
    "# import resources.train_generator as train_generator;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df514eb-ae13-469e-bb95-c92fbdd7cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# import common.tlopts as tlopts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d04c10f-d1a7-454a-b163-cac1d59dd7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.utils as U;\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "#the following three imports are for fully quantized\n",
    "from torch.quantization import quantize_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "from torch.ao.quantization.fx.custom_config import PrepareCustomConfig\n",
    "from tinynn.converter import TFLiteConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ded3e2c-21bc-4bac-9c9f-fa5e6697b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9de080e-60bb-4cba-b9ac-8727f3b422d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076adfb-46fb-44ba-98e0-c38a9db82f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a1baf79-1e4b-4fe0-8f76-7b7163ec3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDataTimeStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H:%M:%S').replace('-',\"\").replace(' ',\"\").replace(':',\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16dc7499-7cd8-4d23-90d5-ef06b0623e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using following function to\n",
    "def quantize_int8_np(x, axis):\n",
    "    '''Quantization into int8_t precision, operating on x along axis'''\n",
    "    scaling_factor_shape = (1,1,1,32)#tuple(np.append([len(x)],np.ones(x.ndim - 1, dtype = int)))\n",
    "    print(f\"scaling_factor_shape:{scaling_factor_shape}\");\n",
    "    epsilon = 0.000000001\n",
    "    x_scaling_factor = (2 * np.max(np.abs(x), axis) / 255) + epsilon\n",
    "    x_scaling_factor = x_scaling_factor.reshape(scaling_factor_shape)\n",
    "    x_zero_offset = -0.5\n",
    "    result = (x / x_scaling_factor) + x_zero_offset\n",
    "    return np.rint(result).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ab2e3deb-c174-42cb-8b3b-f11faed7d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_int8_torch(x):\n",
    "    '''Quantization into int8_t precision, operating on x along axis'''\n",
    "    # scaling_factor_shape = tuple(np.append([len(x)],np.ones(x.ndim - 1, dtype = int)))\n",
    "    # print(f\"scaling_factor_shape:{scaling_factor_shape}\");\n",
    "    epsilon = 0.000000001\n",
    "    x_scaling_factor = (2 * torch.max(torch.abs(x)) / 255) + epsilon\n",
    "    # x_scaling_factor = x_scaling_factor.reshape(scaling_factor_shape)\n",
    "    x_zero_offset = -0.5\n",
    "    result = (x / x_scaling_factor) + x_zero_offset\n",
    "    return torch.round(result).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fab4a323-4291-421b-867a-d52ee41063ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = dict([(52,1),(99,2)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[label1]- 1\n",
    "            idx2 = self.mapdict[label2] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb20ff6-42ad-4671-969f-d6ad8abe5bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a30933e-1045-41a0-b258-2ef29e94bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None):\n",
    "    # dataset = np.load(os.path.join(opt.data, opt.dataset, 'wav{}.npz'.format(opt.sr // 1000)), allow_pickle=True);\n",
    "    # dataset = np.load(\"../datasets/fold1_test16000.npz\", allow_pickle=True);\n",
    "    dataset = np.load(\"../datasets/forOneClassModel_alarm/train/trainSet_20240119002902.npz\", allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # print(len(dataset['x']))\n",
    "    # for i in range(1, opt.nFolds + 1):\n",
    "\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['fold{}'.format(1)].item()['sounds']\n",
    "    train_labels = dataset['fold{}'.format(1)].item()['labels']\n",
    "    # print(train_sounds)\n",
    "\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbc776e4-0edf-4424-a3af-a946d33db7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='./datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 32;\n",
    "    opt.weightDecay = 5e-3;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.nEpochs = 800;#2000;\n",
    "    opt.LR = 0.1;\n",
    "    opt.schedule = [0.3, 0.6, 0.9];\n",
    "    opt.warmup = 10;\n",
    "\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 2#50;\n",
    "    opt.nFolds = 1;#5;\n",
    "    opt.split = 1#[i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    opt.ch_config = [8,64,32,64,64,128,128,256,256,512,512,2];\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3d56ed47-bc98-4899-9e46-d4726cfa565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, opt=None, split=0):\n",
    "        self.opt = opt;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.trainX = None;\n",
    "        self.trainY = None;\n",
    "        # self.opt.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "        self.opt.device = torch.device(\"cpu\")\n",
    "        self.trainGen = getTrainGen(self.opt)#train_generator.setup(self.opt, self.opt.split);\n",
    "        self.qunt_nClass = 2;\n",
    "\n",
    "    def load_train_data(self):\n",
    "        print('Preparing calibration dataset..');\n",
    "        x,y = self.trainGen.__getitem__(0);\n",
    "        self.trainX = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "        \"\"\"\n",
    "        trainX size:torch.Size([1, 1, 30225]), but must be [1,1,1,30225]\n",
    "        Due to the reason: raise ValueError(\"Input shape must be `(N, C, H, W)`!\")\n",
    "        \"\"\"\n",
    "        # print(f\"trainX[0] shape:{self.trainX[0].shape}\")\n",
    "        self.trainY = torch.tensor(y).to(self.opt.device);\n",
    "        print('Calibration dataset is ready');\n",
    "        # self.opt.batchSize = 32;\n",
    "\n",
    "    def load_test_data(self):\n",
    "        if(self.testX is None):\n",
    "            data = np.load('../datasets/forOneClassModel_alarm/test_val/final_val_test_npz/final_valSet_20240119004614.npz', allow_pickle=True);\n",
    "            dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "            self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "            self.testY = torch.tensor(data['y']).to(self.opt.device);\n",
    "\n",
    "    def __validate(self, net, testX, testY):\n",
    "        net.eval();\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = len(self.testX);\n",
    "            x = self.testX[:];\n",
    "            \n",
    "            # batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "            # for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "            #     x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "            #     #print(x.shape);\n",
    "            #     # exit();\n",
    "            #     scores = net(x);\n",
    "            #     y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "            scores = net(x);\n",
    "            y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "            acc = self.__compute_accuracy(y_pred, self.testY);\n",
    "        return acc;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def __compute_accuracy(self, y_pred, y_target):\n",
    "        print(y_pred.shape);\n",
    "        with torch.no_grad():\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1);\n",
    "\n",
    "            y_pred = y_pred.argmax(dim=1);\n",
    "            y_target = y_target.argmax(dim=1);\n",
    "\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "        return acc;\n",
    "\n",
    "    def __load_model(self, quant=False):\n",
    "        state = torch.load(self.opt.model_path, map_location=self.opt.device);\n",
    "        print(state['config']);\n",
    "        net = None;\n",
    "        if quant:\n",
    "            net = models.GetACDNetQuantModel(input_len=self.opt.inputLength, nclass=self.qunt_nClass, sr=self.opt.sr, channel_config=state['config']).to(self.opt.device);\n",
    "        else:\n",
    "            net = models.GetACDNetModel(input_len=self.opt.inputLength, nclass=self.qunt_nClass, sr=self.opt.sr, channel_config=state['config']).to(self.opt.device);\n",
    "            # net = GetTLACDNet(opt=self.opt);\n",
    "        calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        net.load_state_dict(state['weight']);\n",
    "        return net;\n",
    "\n",
    "    def __calibrate(self, net):\n",
    "        self.load_train_data();\n",
    "        net.eval();\n",
    "        # with torch.no_grad():\n",
    "        with torch.inference_mode():\n",
    "            for i in range(1,2):\n",
    "                x_pred = None;\n",
    "                for idx in range(math.ceil(len(self.trainX)/self.opt.batchSize)):\n",
    "                    x = self.trainX[idx*self.opt.batchSize : (idx+1)*self.opt.batchSize];\n",
    "                    scores = net(x);\n",
    "                    x_pred = scores.data if x_pred is None else torch.cat((x_pred, scores.data));\n",
    "                x_pred = x_pred.argmax(dim=1);\n",
    "                x_target = self.trainY.argmax(dim=1);\n",
    "\n",
    "                acc = (((x_pred==x_target)*1).float().mean()*100).item();\n",
    "                print('calibrate accuracy is: {:.2f}'.format(acc));\n",
    "        return acc;\n",
    "\n",
    "    def QuantizeModel(self):\n",
    "        net = self.__load_model(True);\n",
    "        # net = self.__load_model(False);\n",
    "        config = net.ch_config;\n",
    "        net.eval();\n",
    "\n",
    "        #Fuse modules to\n",
    "        torch.quantization.fuse_modules(net.sfeb, ['0','1','2'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.sfeb, ['3','4','5'], inplace=True);\n",
    "\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['0','1','2'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['4','5','6'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['7','8','9'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['11','12','13'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['14','15','16'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['18','19','20'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['21','22','23'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['25','26','27'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['28','29','30'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['33','34','35'], inplace=True);\n",
    "\n",
    "        \"\"\"\n",
    "        backend_int = \"x86\"  # running on a x86 CPU. Use \"qnnpack\" if running on ARM.\n",
    "        qconfig_dict_int = {\"\": torch.quantization.get_default_qconfig(backend_int)}\n",
    "        prepare_custom_config = PrepareCustomConfig() \n",
    "        prepare_custom_config.set_input_quantized_indexes([0])  \n",
    "        prepare_custom_config.set_output_quantized_indexes([0])\n",
    "        # Prepare\n",
    "        example_input_int = torch.randint(255, [1,2,28,28])\n",
    "        # example_input_int = torch.rand(1,2,28,28)\n",
    "        model_prepared_int = quantize_fx.prepare_fx(m_int, qconfig_dict_int,example_input_int, prepare_custom_config)\n",
    "        \"\"\"\n",
    "        # Specify quantization configuration\n",
    "        # net.qconfig = torch.quantization.get_default_qconfig('qnnpack');\n",
    "        # torch.backends.quantized.engine = 'qnnpack';\n",
    "        # print(net.qconfig);\n",
    "        # torch.quantization.prepare(net, inplace=True);\n",
    "        #Rick modify to fully quantized 24/01/26\n",
    "        backend_int = \"qnnpack\"\n",
    "        qconfig_dict_int = {\"\": torch.quantization.get_default_qconfig(backend_int)}\n",
    "        prepare_custom_config = PrepareCustomConfig() \n",
    "        prepare_custom_config.set_input_quantized_indexes([0])  \n",
    "        prepare_custom_config.set_output_quantized_indexes([0])\n",
    "        example_input_int = torch.randint(128,[1, 1, 1, 30225]);\n",
    "        net = quantize_fx.prepare_fx(net, qconfig_dict_int,example_input_int, prepare_custom_config)\n",
    "        # Calibrate with the training data\n",
    "        self.__calibrate(net);\n",
    "\n",
    "        # Convert to quantized model\n",
    "        # torch.quantization.convert(net, inplace=True);\n",
    "        net = quantize_fx.convert_fx(net)\n",
    "        print('Post Training Quantization: Convert done');\n",
    "\n",
    "        print(\"Size of model after quantization\");\n",
    "        torch.save(net.state_dict(), \"temp.p\")\n",
    "        print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "        os.remove('temp.p')\n",
    "        \n",
    "        self.load_test_data();\n",
    "        val_acc = self.__validate(net, self.testX, self.testY);\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "\n",
    "        # torch.jit.save(torch.jit.script(net), '{}/th/quantized_models/{}.pt'.format(os.getcwd(), self.opt.model_name.format()));\n",
    "        torch.jit.save(torch.jit.script(net), '../th/quantized_models/{}.pt'.format(self.opt.model_name.format()));\n",
    "\n",
    "        # **************convert to tflite**********\n",
    "        with torch.no_grad():\n",
    "            # dummy_input = torch.randn(1, 1, 30225, 1); wrong: RuntimeError: quantized::conv2d (qnnpack): each dimension of output tensor should be greater than 0.\n",
    "            dummy_input = torch.randint(128,[1, 1, 1, 30225]); #correct,workable\n",
    "            # dummy_input = torch.randn(30225,1,1,1); wrong: RuntimeError: quantized::conv2d (qnnpack): each dimension of output tensor should be greater than 0.\n",
    "            # dummy_input = torch.randn(1,30225,1,1); wrong:RuntimeError: Input channel size of weight and bias must match.\n",
    "            converter = TFLiteConverter(net,\n",
    "                                        dummy_input,\n",
    "                                        tflite_path=\"../th/quantized_models/{}.tflite\".format(self.opt.model_name))\n",
    "            converter.convert()\n",
    "            \n",
    "            # qmodel = copy.deepcopy(mynn)\n",
    "            # torch.quantization.convert(qmodel, inplace=False)\n",
    "            #\n",
    "            # torch.backends.quantized.engine = 'qnnpack'\n",
    "            # converter = TFLiteConverter(qmodel.module,\n",
    "            #                             torch.randn(1, 64, nn_h, nn_w,\n",
    "            #                             tflite_path=\"qmodel.tflite\")\n",
    "            # converter.convert()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def TestModel(self, quant=False):\n",
    "        if quant:\n",
    "            net = torch.jit.load('../th/quantized_models/{}.pt'.format(self.opt.model_name))\n",
    "        else:\n",
    "            net = self.__load_model();\n",
    "            # calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        self.load_test_data();\n",
    "        net.eval();\n",
    "        val_acc = self.__validate(net, self.testX, self.testY);\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "\n",
    "    def GetModelSize(self):\n",
    "        orig_net_path = self.opt.model_path;\n",
    "        print('Full precision model size (KB):', os.path.getsize(orig_net_path)/(1024));\n",
    "        save_onnx_name = \"{}.onnx\".format(self.opt.model_name);\n",
    "        quant_net_path = \"../th/onnx_models/\"+save_onnx_name;\n",
    "        print('Quantized model size (KB):', os.path.getsize(quant_net_path)/(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "58eff9fb-5fd1-423b-9a60-61c9e9824dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_convert_model_path = \"./th/pruned_models/second_stage_pruned_models/magnitude_pruning/acdnet_tl_hybrid_pruning_magnitude_model_202312281149_80.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ce59dbc6-1088-427e-a3bb-5e88d07465d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    opt = getOpts();#opts.parse();\n",
    "    opt.device = 'cpu';\n",
    "    opt.model_path = \"./retrained_models_after_pruned/retrained_model_20240124123209_acc_95.45455169677734_795th_epoch.pt\"\n",
    "    # valid_path = False;\n",
    "    # while not valid_path:\n",
    "    #     model_path = input(\"Enter the model PATH for 8-bit post training quantization\\n:\");\n",
    "    #     file_paths = glob.glob(os.path.join(os.getcwd(), model_path));\n",
    "    #     if len(file_paths)>0 and os.path.isfile(file_paths[0]):\n",
    "    #         state = torch.load(file_paths[0], map_location='cpu');\n",
    "    #         opt.model_path = file_paths[0];\n",
    "    #         print('Model has been found at: {}'.format(opt.model_path));\n",
    "    #         valid_path = True;\n",
    "\n",
    "    opt.model_name = \"quant_retrained_model_95.4_{}\".format(genDataTimeStr());\n",
    "    # valid_model_name = False;\n",
    "    # while not valid_model_name:\n",
    "    #     model_name = input('Enter a name that will be used to save the quantized model model: ');\n",
    "    #     if model_name != '':\n",
    "    #         opt.model_name = model_name;\n",
    "    #         valid_model_name = True;\n",
    "    opt.split = 1;\n",
    "    opt.hasQuated = False;\n",
    "    trainer = Trainer(opt);\n",
    "\n",
    "    print('Testing performance of the provided model.....');\n",
    "    trainer.TestModel();\n",
    "\n",
    "    print('Quantization process is started.....');\n",
    "    trainer.QuantizeModel();\n",
    "    print('Quantization done');\n",
    "\n",
    "    print('Testing quantized model.');\n",
    "    trainer.TestModel(True);\n",
    "    print('Finished');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a95756a3-5cc4-4276-9272-a22ec24efaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of samples:325\n",
      "Testing performance of the provided model.....\n",
      "[5, 32, 9, 16, 23, 33, 29, 56, 47, 65, 90, 2]\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (5, 1, 15109)         45      679,905\n",
      "  BatchNorm2d-2     (5, 1, 15109)     (5, 1, 15109)         10            0\n",
      "         ReLu-3     (5, 1, 15109)     (5, 1, 15109)          0       75,545\n",
      "       Conv2d-4     (5, 1, 15109)     (32, 1, 7553)        800    6,042,400\n",
      "  BatchNorm2d-5     (32, 1, 7553)     (32, 1, 7553)         64            0\n",
      "         ReLu-6     (32, 1, 7553)     (32, 1, 7553)          0      241,696\n",
      "    MaxPool2d-7     (32, 1, 7553)      (32, 1, 151)          0      241,600\n",
      "      Permute-8      (32, 1, 151)      (1, 32, 151)          0            0\n",
      "       Conv2d-9      (1, 32, 151)      (9, 32, 151)         81      391,392\n",
      " BatchNorm2d-10      (9, 32, 151)      (9, 32, 151)         18            0\n",
      "        ReLu-11      (9, 32, 151)      (9, 32, 151)          0       43,488\n",
      "   MaxPool2d-12      (9, 32, 151)       (9, 16, 75)          0       43,200\n",
      "      Conv2d-13       (9, 16, 75)      (16, 16, 75)      1,296    1,555,200\n",
      " BatchNorm2d-14      (16, 16, 75)      (16, 16, 75)         32            0\n",
      "        ReLu-15      (16, 16, 75)      (16, 16, 75)          0       19,200\n",
      "      Conv2d-16      (16, 16, 75)      (23, 16, 75)      3,312    3,974,400\n",
      " BatchNorm2d-17      (23, 16, 75)      (23, 16, 75)         46            0\n",
      "        ReLu-18      (23, 16, 75)      (23, 16, 75)          0       27,600\n",
      "   MaxPool2d-19      (23, 16, 75)       (23, 8, 37)          0       27,232\n",
      "      Conv2d-20       (23, 8, 37)       (33, 8, 37)      6,831    2,021,976\n",
      " BatchNorm2d-21       (33, 8, 37)       (33, 8, 37)         66            0\n",
      "        ReLu-22       (33, 8, 37)       (33, 8, 37)          0        9,768\n",
      "      Conv2d-23       (33, 8, 37)       (29, 8, 37)      8,613    2,549,448\n",
      " BatchNorm2d-24       (29, 8, 37)       (29, 8, 37)         58            0\n",
      "        ReLu-25       (29, 8, 37)       (29, 8, 37)          0        8,584\n",
      "   MaxPool2d-26       (29, 8, 37)       (29, 4, 18)          0        8,352\n",
      "      Conv2d-27       (29, 4, 18)       (56, 4, 18)     14,616    1,052,352\n",
      " BatchNorm2d-28       (56, 4, 18)       (56, 4, 18)        112            0\n",
      "        ReLu-29       (56, 4, 18)       (56, 4, 18)          0        4,032\n",
      "      Conv2d-30       (56, 4, 18)       (47, 4, 18)     23,688    1,705,536\n",
      " BatchNorm2d-31       (47, 4, 18)       (47, 4, 18)         94            0\n",
      "        ReLu-32       (47, 4, 18)       (47, 4, 18)          0        3,384\n",
      "   MaxPool2d-33       (47, 4, 18)        (47, 2, 9)          0        3,384\n",
      "      Conv2d-34        (47, 2, 9)        (65, 2, 9)     27,495      494,910\n",
      " BatchNorm2d-35        (65, 2, 9)        (65, 2, 9)        130            0\n",
      "        ReLu-36        (65, 2, 9)        (65, 2, 9)          0        1,170\n",
      "      Conv2d-37        (65, 2, 9)        (90, 2, 9)     52,650      947,700\n",
      " BatchNorm2d-38        (90, 2, 9)        (90, 2, 9)        180            0\n",
      "        ReLu-39        (90, 2, 9)        (90, 2, 9)          0        1,620\n",
      "   MaxPool2d-40        (90, 2, 9)        (90, 1, 4)          0        1,440\n",
      "      Conv2d-41        (90, 1, 4)         (2, 1, 4)        180          720\n",
      " BatchNorm2d-42         (2, 1, 4)         (2, 1, 4)          4            0\n",
      "        ReLu-43         (2, 1, 4)         (2, 1, 4)          0            8\n",
      "   AvgPool2d-44         (2, 1, 4)         (2, 1, 1)          0            8\n",
      "     Flatten-45         (2, 1, 1)            (1, 2)          0            0\n",
      "      Linear-46            (1, 2)            (1, 2)          6            6\n",
      "     Softmax-47            (1, 2)            (1, 2)          0            2\n",
      "==============================================================================\n",
      "Total Params: 140,427\n",
      "Total FLOPs : 22,177,258\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 0.54\n",
      "Total size (MB) : 0.65\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([176, 2])\n",
      "Testing: Acc(top1) 95.45%\n",
      "Quantization process is started.....\n",
      "[5, 32, 9, 16, 23, 33, 29, 56, 47, 65, 90, 2]\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (5, 1, 15109)         45      679,905\n",
      "  BatchNorm2d-2     (5, 1, 15109)     (5, 1, 15109)         10            0\n",
      "         ReLu-3     (5, 1, 15109)     (5, 1, 15109)          0       75,545\n",
      "       Conv2d-4     (5, 1, 15109)     (32, 1, 7553)        800    6,042,400\n",
      "  BatchNorm2d-5     (32, 1, 7553)     (32, 1, 7553)         64            0\n",
      "         ReLu-6     (32, 1, 7553)     (32, 1, 7553)          0      241,696\n",
      "    MaxPool2d-7     (32, 1, 7553)      (32, 1, 151)          0      241,600\n",
      "      Permute-8      (32, 1, 151)      (1, 32, 151)          0            0\n",
      "       Conv2d-9      (1, 32, 151)      (9, 32, 151)         81      391,392\n",
      " BatchNorm2d-10      (9, 32, 151)      (9, 32, 151)         18            0\n",
      "        ReLu-11      (9, 32, 151)      (9, 32, 151)          0       43,488\n",
      "   MaxPool2d-12      (9, 32, 151)       (9, 16, 75)          0       43,200\n",
      "      Conv2d-13       (9, 16, 75)      (16, 16, 75)      1,296    1,555,200\n",
      " BatchNorm2d-14      (16, 16, 75)      (16, 16, 75)         32            0\n",
      "        ReLu-15      (16, 16, 75)      (16, 16, 75)          0       19,200\n",
      "      Conv2d-16      (16, 16, 75)      (23, 16, 75)      3,312    3,974,400\n",
      " BatchNorm2d-17      (23, 16, 75)      (23, 16, 75)         46            0\n",
      "        ReLu-18      (23, 16, 75)      (23, 16, 75)          0       27,600\n",
      "   MaxPool2d-19      (23, 16, 75)       (23, 8, 37)          0       27,232\n",
      "      Conv2d-20       (23, 8, 37)       (33, 8, 37)      6,831    2,021,976\n",
      " BatchNorm2d-21       (33, 8, 37)       (33, 8, 37)         66            0\n",
      "        ReLu-22       (33, 8, 37)       (33, 8, 37)          0        9,768\n",
      "      Conv2d-23       (33, 8, 37)       (29, 8, 37)      8,613    2,549,448\n",
      " BatchNorm2d-24       (29, 8, 37)       (29, 8, 37)         58            0\n",
      "        ReLu-25       (29, 8, 37)       (29, 8, 37)          0        8,584\n",
      "   MaxPool2d-26       (29, 8, 37)       (29, 4, 18)          0        8,352\n",
      "      Conv2d-27       (29, 4, 18)       (56, 4, 18)     14,616    1,052,352\n",
      " BatchNorm2d-28       (56, 4, 18)       (56, 4, 18)        112            0\n",
      "        ReLu-29       (56, 4, 18)       (56, 4, 18)          0        4,032\n",
      "      Conv2d-30       (56, 4, 18)       (47, 4, 18)     23,688    1,705,536\n",
      " BatchNorm2d-31       (47, 4, 18)       (47, 4, 18)         94            0\n",
      "        ReLu-32       (47, 4, 18)       (47, 4, 18)          0        3,384\n",
      "   MaxPool2d-33       (47, 4, 18)        (47, 2, 9)          0        3,384\n",
      "      Conv2d-34        (47, 2, 9)        (65, 2, 9)     27,495      494,910\n",
      " BatchNorm2d-35        (65, 2, 9)        (65, 2, 9)        130            0\n",
      "        ReLu-36        (65, 2, 9)        (65, 2, 9)          0        1,170\n",
      "      Conv2d-37        (65, 2, 9)        (90, 2, 9)     52,650      947,700\n",
      " BatchNorm2d-38        (90, 2, 9)        (90, 2, 9)        180            0\n",
      "        ReLu-39        (90, 2, 9)        (90, 2, 9)          0        1,620\n",
      "   MaxPool2d-40        (90, 2, 9)        (90, 1, 4)          0        1,440\n",
      "      Conv2d-41        (90, 1, 4)         (2, 1, 4)        180          720\n",
      " BatchNorm2d-42         (2, 1, 4)         (2, 1, 4)          4            0\n",
      "        ReLu-43         (2, 1, 4)         (2, 1, 4)          0            8\n",
      "   AvgPool2d-44         (2, 1, 4)         (2, 1, 1)          0            8\n",
      "     Flatten-45         (2, 1, 1)            (1, 2)          0            0\n",
      "      Linear-46            (1, 2)            (1, 2)          6            6\n",
      "     Softmax-47            (1, 2)            (1, 2)          0            2\n",
      "==============================================================================\n",
      "Total Params: 140,427\n",
      "Total FLOPs : 22,177,258\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 0.54\n",
      "Total size (MB) : 0.65\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Preparing calibration dataset..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/torch/ao/quantization/fx/utils.py:850: UserWarning: QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize for fixed qparams ops, ignoring QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=False){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7ff903d86710>}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x7ff903d86710>}).\n",
      "Please use torch.ao.quantization.get_default_qconfig_mapping or torch.ao.quantization.get_default_qat_qconfig_mapping. Example:\n",
      "    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n",
      "    model = prepare_fx(model, qconfig_mapping, example_inputs)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration dataset is ready\n",
      "max ele of x is 1.716275930404663\n",
      "calibrate accuracy is: 90.62\n",
      "Post Training Quantization: Convert done\n",
      "Size of model after quantization\n",
      "Size (MB): 0.157958\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::conv2d_relu.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d_relu.new' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qconv.cpp:1874 [kernel]\nQuantizedCUDA: registered at ../aten/src/ATen/native/quantized/cudnn/Conv.cpp:388 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[102], line 30\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m trainer\u001b[38;5;241m.\u001b[39mTestModel();\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantization process is started.....\u001b[39m\u001b[38;5;124m'\u001b[39m);\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuantizeModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantization done\u001b[39m\u001b[38;5;124m'\u001b[39m);\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting quantized model.\u001b[39m\u001b[38;5;124m'\u001b[39m);\n",
      "Cell \u001b[0;32mIn[100], line 157\u001b[0m, in \u001b[0;36mTrainer.QuantizeModel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp.p\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_test_data();\n\u001b[0;32m--> 157\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtestX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtestY\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting: Acc(top1) \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(val_acc));\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# torch.jit.save(torch.jit.script(net), '{}/th/quantized_models/{}.pt'.format(os.getcwd(), self.opt.model_name.format()));\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[100], line 47\u001b[0m, in \u001b[0;36mTrainer.__validate\u001b[0;34m(self, net, testX, testY)\u001b[0m\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestX[:];\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# for idx in range(math.ceil(len(self.testX)/batch_size)):\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#     x = self.testX[idx*batch_size : (idx+1)*batch_size];\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#     scores = net(x);\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#     y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     48\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m y_pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((y_pred, scores\u001b[38;5;241m.\u001b[39mdata));\n\u001b[1;32m     49\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__compute_accuracy(y_pred, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestY);\n",
      "File \u001b[0;32m~/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/torch/fx/graph_module.py:678\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/torch/fx/graph_module.py:284\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/torch/fx/graph_module.py:274\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<eval_with_key>.80:5\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 5\u001b[0m     sfeb_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msfeb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m;  x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     sfeb_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msfeb, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)(sfeb_0);  sfeb_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     sfeb_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msfeb, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m)(sfeb_1);  sfeb_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py:98\u001b[0m, in \u001b[0;36mConvReLU2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     95\u001b[0m     _reversed_padding_repeated_twice \u001b[38;5;241m=\u001b[39m _reverse_repeat_padding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, _reversed_padding_repeated_twice,\n\u001b[1;32m     97\u001b[0m                   mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_relu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_point\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/torch/_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::conv2d_relu.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d_relu.new' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qconv.cpp:1874 [kernel]\nQuantizedCUDA: registered at ../aten/src/ATen/native/quantized/cudnn/Conv.cpp:388 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3ca04-7384-4a90-853e-050b593a0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt2 = getOpts();#opts.parse();\n",
    "# state2 = torch.load(to_convert_model_path, map_location=\"cuda:0\");\n",
    "# tmpnet = models.GetACDNetModel(input_len=opt2.inputLength, nclass=6, sr=20000, channel_config=state2['config']).to(\"cuda:0\");\n",
    "# tmpnet.load_state_dict(state2['weight']);\n",
    "# print(tmpnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59060995-a696-4bc6-8980-5a5d18cf4bd1",
   "metadata": {},
   "source": [
    "## Convert tflite to C array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab64fe84-ccca-4d35-80a3-edfd7012b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_TFLITE = os.path.join(MODELS_DIR, 'model.tflite')\n",
    "# FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'float_model.tflite')\n",
    "# MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'model.cc')\n",
    "tflite_model = \"../th/quantized_models/quant_retrained_model_95.4_20240125130552.tflite\"\n",
    "target_c_file =\"../th/Model_C_Files/quant_from_retrain_95.4_240125120552.c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce47b1c0-ac1e-47d6-abf9-72dd595e94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get -qq install xxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "274d47a5-acc2-4eeb-bd3b-2c57ae556cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get -qq install xxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "190ecc22-9963-4c04-b389-7f339a8e6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!xxd -i {tflite_model} > {target_c_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6812387-b134-4bee-acc6-5d4402d34559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update variable names\n",
    "REPLACE_TEXT = \"uec_iot_model_alarm\";#MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
    "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {target_c_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b37ec-3980-4aed-8b2e-e86e36718ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
