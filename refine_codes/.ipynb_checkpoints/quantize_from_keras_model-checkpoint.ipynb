{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eea6844-279b-4304-9bcf-79287652f1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 16:01:52.284662: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-30 16:01:52.304535: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-30 16:01:52.304555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-30 16:01:52.305071: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-30 16:01:52.308462: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-30 16:01:52.651350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "183b2bf1-0a71-4a8f-b00d-819df2329034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a98c06-d390-4260-af3e-fd808278c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../deployment/\"))\n",
    "sys.path.append(os.path.abspath(\"../common/\"))\n",
    "sys.path.append(os.path.abspath(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db3de59e-a0db-4ab8-9791-4ba690bfa843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lib_home import bytes,constants,quantize\n",
    "from results import *\n",
    "from Libs.datetime_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94bacca4-5475-4583-b0ec-7b907dd1ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import for data generator\n",
    "import common.utils as U;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "794d8117-23f9-41f9-914a-3d09497b1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d016f94-c32d-430b-8a97-a0245e0f8aba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lib_original.bytes import to_bytes, from_bytes, byte_conversion_tests, load_data, load_raw, save_raw, save_scores\n",
    "from lib_original.constants import quant_support, crops, feature_count\n",
    "from lib_original.quantize import quantization_tests, get_cast\n",
    "# from lib.opts import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0556c27-38bb-4e1c-ab98-d5d15ac071fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "526e5f2b-2104-40be-a128-83ad2e41c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='ACDNet_TL_Model_Extend',  required=False);\n",
    "    parser.add_argument('--data', default='../datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args();\n",
    "    \n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 64;\n",
    "    opt.LR = 0.1;\n",
    "    opt.weightDecay = 5e-2#9e-3;#5e-3;#5e-2;#1e-2;#5e-4;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.nEpochs = 1000;\n",
    "    opt.schedule = [0.3, 0.6, 0.9];\n",
    "    opt.warmup = 10;\n",
    "    if torch.backends.mps.is_available():\n",
    "        opt.device=\"mps\"; #for apple m2 gpu\n",
    "    elif torch.cuda.is_available():\n",
    "        opt.device=\"cuda:0\"; #for nVidia gpu\n",
    "    else:\n",
    "        opt.device=\"cpu\"\n",
    "    print(f\"***Use device:{opt.device}\");\n",
    "    # opt.device = torch.device(\"cuda:0\" if  else \"cpu\");\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 2#50;\n",
    "    opt.nFolds = 1;\n",
    "    opt.splits = [i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    # opt.TLAcdnetConfig = [8,64,32,64,64,128,128,256,256,512,512,2];\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9562c0f2-425a-438d-9cd3-7a8180055629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.utils.Sequence):\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = {\n",
    "                52:1, #alarm\n",
    "                99:2, #other_sounds\n",
    "        };\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[label1]- 1\n",
    "            idx2 = self.mapdict[label2] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;\n",
    "\n",
    "def setup(opt, split):\n",
    "    print(opt.trainData)\n",
    "    dataset = np.load(opt.trainData, allow_pickle=True);\n",
    "    print(dataset)\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # for i in range(1, opt.nFolds + 1):\n",
    "    sounds = dataset['fold1'].item()['sounds']\n",
    "    labels = dataset['fold1'].item()['labels']\n",
    "    # if i != split:\n",
    "    train_sounds.extend(sounds)\n",
    "    train_labels.extend(labels)\n",
    "\n",
    "    # print(train_labels)\n",
    "    trainGen = Generator(train_sounds, train_labels, opt);\n",
    "\n",
    "    return trainGen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4edfeb7-4ba0-4653-ad36-55a379855f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#codes to debug generator, trainer\n",
    "# testopt = getOpts()\n",
    "# testopt.model = \"./keras_models/retrain_pruned_cp_weights.h5\"\n",
    "# testopt.trainData = \"../datasets/forOneClassModel_alarm/train/trainSet_20240119002902.npz\"\n",
    "# testopt.valData = \"../datasets/forOneClassModel_alarm/test_val/final_val_test_npz/final_valSet_20240119004614.npz\"\n",
    "# testopt.split = 1\n",
    "# test_generate = setup(opt=testopt, split=1)\n",
    "# tmp_x, tmp_y = test_generate.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbafbe3-9f50-4763-948c-4242ea6b5659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a79423d-c859-48b1-a57f-30ea3c894b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, opt=None):\n",
    "        self.opt = opt;\n",
    "        self.trainX = None;\n",
    "        self.trainY = None;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "\n",
    "    def load_training_data(self):\n",
    "        if self.trainX is None:\n",
    "            print('Loading training/calibration data');\n",
    "            #Loading all the 1600 samples to train one epoch at once.\n",
    "            trainGen = setup(self.opt, self.opt.split);\n",
    "            self.opt.sr = 20000;\n",
    "            self.opt.inputLength = 30225;\n",
    "            self.trainX, self.trainY = trainGen.__getitem__(0);\n",
    "            self.opt.batchSize = len(self.trainX);\n",
    "            print('Done');\n",
    "\n",
    "            #Revert back the batch size settings to its original state\n",
    "            #my all training data is 325\n",
    "            self.opt.batchSize = 325;\n",
    "\n",
    "    def load_test_data(self):\n",
    "        if self.testX is None:\n",
    "            print('Loading test data');\n",
    "            # data = np.load(os.path.join(self.opt.data, self.opt.dataset, 'test_data_20khz/fold{}_test4000.npz'.format(self.opt.split)), allow_pickle=True);\n",
    "            data = np.load(opt.valData,allow_pickle=True)\n",
    "            #total test data is 176\n",
    "            self.testX = data['x'];\n",
    "            self.testY = data['y'];\n",
    "            print('Done');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7bc5165b-2f5f-4815-a45a-7f87a30adc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasConverter():\n",
    "  def __init__(self, model_file, dtype, tflite_save_path, trainer:Trainer):\n",
    "    '''Initialise the Keras model converter'''\n",
    "\n",
    "    self.model_file = model_file\n",
    "    self.dtype = dtype\n",
    "    self.quant_support = quant_support[dtype]\n",
    "    self.tflite_path = tflite_save_path;\n",
    "    print(f\"tflite save path:{self.tflite_path}\");\n",
    "    # self.cc_path = f'{dest_path}/retrain_pruned_model_from_keras_model.cc'\n",
    "    # print(f\"cc save path:{self.cc_path}\");\n",
    "    # self.h_path = f'{dest_path}/retrain_pruned_model_from_keras_model.h'\n",
    "    # print(f\"header file save path:{self.h_path}\");\n",
    "    self.trainer = trainer    \n",
    "\n",
    "  def load_model(self):\n",
    "    print(f'Loading model: {self.model_file}')\n",
    "    #original\n",
    "    self.model = keras.models.load_model(self.model_file)\n",
    "    #test onnx2keras model\n",
    "    # self.model = keras.models.load_model(self.model_file,safe_mode=False)\n",
    "  \n",
    "  def get_model_size(self):\n",
    "    '''Returns the input size and output size'''\n",
    "    return self.model.inputs[0].shape[-2], self.model.outputs[0].shape[-1]\n",
    "\n",
    "  def get_tf_summary(self):\n",
    "    '''Displays a model summary'''\n",
    "    stringlist = []\n",
    "    self.model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    short_model_summary = \"\\n\".join(stringlist)\n",
    "    return short_model_summary\n",
    "\n",
    "  def get_tflite_summary(self):\n",
    "    '''Displays a model summary'''\n",
    "    with tf.io.gfile.GFile(self.tflite_path, 'rb') as f:\n",
    "      model_content = f.read()\n",
    "        \n",
    "    interpreter = tf.lite.Interpreter(model_content = model_content)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    return (interpreter.get_tensor_details(), \\\n",
    "      interpreter._get_ops_details())\n",
    "\n",
    "  def get_arena_size(self):\n",
    "    '''Returns the approx tensor_arena size'''\n",
    "    tensor_details, op_details = self.get_tflite_summary()\n",
    "\n",
    "    get_tensor = lambda index : [t for t in tensor_details if t['index'] == index][0]\n",
    "    get_node_tensors = lambda n : [get_tensor(t) for t in np.concatenate((n['inputs'],n['outputs']), axis= None)]    \n",
    "    get_tensor_size = lambda t : np.prod(t['shape']) * np.dtype(t['dtype']).itemsize\n",
    "    get_node_tensor_sizes = lambda o : np.sum([get_tensor_size(t) for t in get_node_tensors(o)])   \n",
    "    get_max_node_size = lambda : np.max([get_node_tensor_sizes(o) for o in op_details])\n",
    "\n",
    "    return get_max_node_size() \n",
    "\n",
    "  def get_input_data(self):\n",
    "    '''Retrieves the input data set'''\n",
    "    x,y = self.trainer.testX, self.trainer.testY\n",
    "    return x, y\n",
    "\n",
    "  def get_cast_input_data(self, dtype = None):\n",
    "    '''Retrieves the input data set, casting to target dtype'''\n",
    "    x, y = self.get_input_data()\n",
    "\n",
    "    if dtype is None:\n",
    "      print('dtype not provided')\n",
    "      return x,y   \n",
    "    return get_cast(dtype)(x, axis = -2), y\n",
    "  \n",
    "  def get_rep_data(self, dtype = 'int8'):\n",
    "    '''Retrieves the reprepresentative data set'''\n",
    "    x, y = self.trainer.trainX, self.trainer.trainY\n",
    "\n",
    "    if dtype is None:\n",
    "      return x,y\n",
    "    return get_cast(dtype)(x, axis=-2), y\n",
    "\n",
    "  def evaluate_accuracy(self, y_pred, y_target, crops = 1):\n",
    "    '''A common accuracy operation which supports multi-crop'''\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0]//crops, crops, y_pred.shape[1])\n",
    "    y_target = y_target.reshape(y_target.shape[0]//crops, crops, y_target.shape[1])\n",
    "\n",
    "    #Calculate the average of class predictions for 10 crops of a sample\n",
    "    y_pred = np.mean(y_pred, axis=1)\n",
    "    y_target = np.mean(y_target,axis=1)\n",
    "\n",
    "    #Get the indices that has highest average value for each sample\n",
    "    y_pred = y_pred.argmax(axis=1)\n",
    "    y_target = y_target.argmax(axis=1)\n",
    "\n",
    "    accuracy = (y_pred==y_target).mean()\n",
    "    return accuracy\n",
    "\n",
    "  def predict_tf(self, x_data):\n",
    "    '''Calculate the output of a single inference of the TF model'''\n",
    "    x = tf.expand_dims(x_data, 0).numpy()\n",
    "    return self.model.predict([x])\n",
    "\n",
    "  def get_tf_accuracy(self, crops = 1):\n",
    "    '''Calculate accuracy of the TF model'''\n",
    "    x_data, y_data = self.get_input_data()\n",
    "\n",
    "    y_pred = self.model.predict([x_data])\n",
    "\n",
    "    accuracy = self.evaluate_accuracy(y_pred, y_data, crops)\n",
    "    return accuracy, y_pred, y_data\n",
    "\n",
    "  def predict_tflite(self, x_data):\n",
    "    '''Calculate output of single inference of the TFLite model'''\n",
    "    with tf.io.gfile.GFile(self.tflite_path, 'rb') as f:\n",
    "      model_content = f.read()\n",
    "        \n",
    "    interpreter = tf.lite.Interpreter(model_content = model_content)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_index = interpreter.get_input_details()[0]['index']\n",
    "    output_index = interpreter.get_output_details()[0]['index']\n",
    "\n",
    "    input_dtype = interpreter.get_input_details()[0]['dtype']\n",
    "    x = get_cast(input_dtype)(x_data, axis=-2)\n",
    "\n",
    "    interpreter.set_tensor(input_index, x)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    return interpreter.get_tensor(output_index)[0]\n",
    "\n",
    "  def get_tflite_accuracy(self, crops = 1):\n",
    "    '''Calculate the accuracy of the TFLite model'''\n",
    "    with tf.io.gfile.GFile(self.tflite_path, 'rb') as f:\n",
    "      model_content = f.read()\n",
    "        \n",
    "    interpreter = tf.lite.Interpreter(model_content = model_content)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_index = interpreter.get_input_details()[0]['index']\n",
    "    output_index = interpreter.get_output_details()[0]['index']\n",
    "\n",
    "    input_dtype = interpreter.get_input_details()[0]['dtype']\n",
    "    print(f'Input dtype {str(input_dtype)}')\n",
    "\n",
    "    x_data, y_data = self.get_cast_input_data(input_dtype)\n",
    "\n",
    "    def predict(x_input):\n",
    "      \n",
    "      x_input = tf.expand_dims(x_input, 0).numpy()\n",
    "      interpreter.set_tensor(input_index, x_input)\n",
    "      # Run inference.\n",
    "      interpreter.invoke()\n",
    "      return interpreter.get_tensor(output_index)[0]\n",
    "    \n",
    "    y_pred = np.array([predict(x) for x in x_data])\n",
    "    print(y_pred.shape)\n",
    "    print(y_data.shape)\n",
    "    accuracy = self.evaluate_accuracy(y_pred, y_data, crops)\n",
    "    return accuracy, y_pred, y_data  \n",
    "\n",
    "  def generate_tflite(self):\n",
    "    '''Generates a TFLite file from a Keras model'''\n",
    "\n",
    "    # Construction of a TFLite converter\n",
    "    tf_converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n",
    "    \n",
    "    tf_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    if 'supported_ops' in self.quant_support:\n",
    "      print(f'Targetting Supported Ops {self.quant_support[\"supported_ops\"]}')\n",
    "      tf_converter.target_spec.supported_ops = self.quant_support['supported_ops']\n",
    "\n",
    "    if 'supported_types' in self.quant_support:\n",
    "      print(f'Targetting Supported Types{self.quant_support[\"supported_types\"]}')\n",
    "      tf_converter.target_spec.supported_types = self.quant_support['supported_types']\n",
    "\n",
    "    if 'input_type' in self.quant_support:\n",
    "      print(f'Targetting input type : {self.quant_support[\"input_type\"]}')\n",
    "      tf_converter.inference_input_type = self.quant_support['input_type']\n",
    "\n",
    "    if 'output_type' in self.quant_support:\n",
    "      print(f'Targetting output type : {self.quant_support[\"output_type\"]}')\n",
    "      tf_converter.inference_output_type = self.quant_support['output_type']\n",
    "\n",
    "    # Supplying a representative dataset is required for full integer \n",
    "    # quantization, and also avoids dynamic range quantization\n",
    "\n",
    "    # rep_data, _ = self.get_rep_data(None) #original\n",
    "    rep_data, _ = self.get_rep_data(dtype=None)\n",
    "    print(f\"rep_data(quantized data from training data):\\n{rep_data}\")\n",
    "    print(f'Representative dataset dtype : {rep_data.dtype}')\n",
    "\n",
    "    def representative_dataset_no_padding():\n",
    "      for i in range(len(rep_data)):\n",
    "        if rep_data[i:i+1,:,0,:] != 0 and rep_data[i:i+1,:,-1,:] != 0:\n",
    "          yield([rep_data[i:i+1,:,:,:]])\n",
    "\n",
    "    def representative_dataset():\n",
    "      for i in range(len(rep_data)):\n",
    "        yield([rep_data[i:i+1,:,:,:]])\n",
    "\n",
    "    tf_converter.representative_dataset = representative_dataset\n",
    "\n",
    "    tflite_model = tf_converter.convert()\n",
    "    bytes_written = open(self.tflite_path, 'wb').write(tflite_model)\n",
    "    print(\"***************************\\n model saved \\n******************************\")\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "    input_type = interpreter.get_input_details()[0]['dtype']\n",
    "    output_type = interpreter.get_output_details()[0]['dtype']\n",
    "\n",
    "    print('TFLite input dtype : ', input_type)\n",
    "    print('TFLite output dtype : ', output_type)\n",
    "    \n",
    "    return bytes_written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c2b2b-bdeb-4843-a9cf-7a118f801a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "43035d2d-71f8-4e09-81f6-857655304368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_path, dtype, results_path, dest_path, trainer):\n",
    "  '''This operation converts the models into TFLite and c++ format'''\n",
    "  \n",
    "  assert dtype in quant_support.keys(), \\\n",
    "    f'Unknown quantization target dtype {dtype}'\n",
    "\n",
    "  cwd_path = os.path.abspath(\"./quanted_keras_models/\");#os.path.dirname(os.path.abspath(__file__))\n",
    "  print(f\"cwd_path:{cwd_path}\")\n",
    "  crop_size = 2\n",
    "\n",
    "  results = []\n",
    "  report_datetime = datetime.now()  \n",
    "  results_path = os.path.join(results_path, f'results_{genDataTimeStr()}.npy')\n",
    "  print(f\"report save path:{results_path}\")\n",
    "  tflite_name = f\"retrain_pruned_model_from_keras_{genDataTimeStr()}\"\n",
    "  save_tflite_path = f\"./quanted_keras_models/{tflite_name}.tflite\"\n",
    "  converter = KerasConverter(model_path, dtype, save_tflite_path, trainer)\n",
    "  converter.load_model()\n",
    "  input_size, output_size = converter.get_model_size()\n",
    "  print(f\"input_size:{input_size}, output_size:{output_size}\")\n",
    "  try:    \n",
    "    # Conversion\n",
    "    converter.generate_tflite()\n",
    "\n",
    "    # Evaluation\n",
    "    print(f'Calculating TF Accuracy : {model_path}')\n",
    "    tf_accuracy, tf_pred, tf_data = converter.get_tf_accuracy(crop_size)\n",
    "    \n",
    "    print(f'Calculating TFLite Accuracy : {model_path} {dtype}')\n",
    "    tflite_accuracy, tflite_pred, tflite_data = converter.get_tflite_accuracy(crop_size)\n",
    "    tflite_arena_size = converter.get_arena_size()\n",
    "\n",
    "    print(f'Final accuracy : {model_path}')\n",
    "    print(f'TF Accuracy : {tf_accuracy}')            \n",
    "    print(f'TFLite Accuracy ({dtype}) : {tflite_accuracy}')\n",
    "    print(f'Arena size (approx): {tflite_arena_size}')\n",
    "\n",
    "    fold = trainer.opt.split\n",
    "    dataset = trainer.opt.dataset\n",
    "\n",
    "    # Summarize\n",
    "    result = {\n",
    "      'model' : model_path,\n",
    "      'dtype' : dtype,\n",
    "      'fold' :  fold,\n",
    "      'dataset' : dataset,\n",
    "      'tf_accuracy' :tf_accuracy,\n",
    "      'tf_pred' : tf_pred,\n",
    "      'tf_data' : tf_data,\n",
    "      'tflite_accuracy' : tflite_accuracy,\n",
    "      'tflite_pred' : tflite_pred,\n",
    "      'tflite_data' : tflite_data,\n",
    "      'tflite_arena' : tflite_arena_size,\n",
    "      'tf_summary' : converter.get_tf_summary(),\n",
    "      'tflite_summary' : converter.get_tflite_summary(),\n",
    "      'input_size' : input_size,\n",
    "      'output_size' : output_size\n",
    "    }\n",
    "\n",
    "    # results.append(result)\n",
    "\n",
    "    # np.save(results_path,results,allow_pickle=True)\n",
    "\n",
    "    # # Cleanup\n",
    "    # model_file = os.path.basename(model_path)\n",
    "\n",
    "    # src_tflite_path = \"./quanted_keras_models/retrain_pruned_model_from_keras.tflite\"#os.path.join(dest_path,'g_model.tflite')\n",
    "    # src_cc_path = \"./quanted_keras_models/retrain_pruned_model_from_keras.cc\"#os.path.join(dest_path,'g_model.cc')\n",
    "    # print(f\"src_tflite_path\")\n",
    "    # # Convert TFLite to CC file, for inclusion in c++ applications\n",
    "    # os.system(f'xxd -i {src_tflite_path} > {src_cc_path}')\n",
    "   \n",
    "    # # Copy tflite file into models folder\n",
    "    # dest_tflite_name = \\\n",
    "    #   f'{model_file}_{dtype}_{dataset}_fold{fold}_{genDataTimeStr()}.tflite'\n",
    "    \n",
    "    # # Embed model metadata in cc file\n",
    "    # modify_cc_file(src_cc_path, dest_tflite_name, input_size, tflite_arena_size)\n",
    "\n",
    "    # dest_tflite_path = \\\n",
    "    #     os.path.join(cwd_path, 'src','models',dest_tflite_name)\n",
    "    \n",
    "    # shutil.copy(src_tflite_path, dest_tflite_path)\n",
    "\n",
    "    # # Copy CC file into models folder\n",
    "    # dest_cc_name = \\\n",
    "    #   f'{model_file}_{dtype}_{dataset}_fold{fold}.cc'\n",
    "    \n",
    "    # dest_cc_path = \\\n",
    "    #   os.path.join(cwd_path, 'src','models', dest_cc_name)\n",
    "\n",
    "    # shutil.copy(src_cc_path, dest_cc_path)\n",
    "\n",
    "    # # Install into TFLITE Micro\n",
    "    # tflite_micro_model_path = \\\n",
    "    #   os.path.join(cwd_path,'src','tflite_micro', 'model.cc')\n",
    "\n",
    "    # shutil.copy(src_cc_path,  tflite_micro_model_path)\n",
    "\n",
    "    # # Install into TFLITE_x86_64 \n",
    "    # tflite_micro_model_path = \\\n",
    "    #   os.path.join(cwd_path,'src','tflite_x86_64','app', 'model.cpp')\n",
    "\n",
    "    # shutil.copy(src_cc_path,  tflite_micro_model_path)\n",
    "\n",
    "    # # Remove temp files\n",
    "    # os.unlink(src_tflite_path)\n",
    "    # os.unlink(src_cc_path)\n",
    "\n",
    "  except Exception as ex:\n",
    "      print(f'Error: {str(ex)}')\n",
    "  \n",
    "  # print_results(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e8f46f9e-6cf5-4d4b-bdaa-1d93cb6b2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter = tf.lite.TFLiteConverter.from_saved_model(\"./tensorflow_models/retrain_pruned_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "42692255-6912-42d4-84ba-396e9c905f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    byte_conversion_tests()\n",
    "    quantization_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3a7f2ed0-3dd4-46a2-b364-508b9d1f7fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Use device:cuda:0\n",
      "cwd_path:/home/ai/RLRepo/Works/Projects/TransferLearning_for_ACDNet/refine_codes/quanted_keras_models\n",
      "result_path:/home/ai/RLRepo/Works/Projects/TransferLearning_for_ACDNet/refine_codes/quanted_keras_models/results\n",
      "Loading training/calibration data\n",
      "../datasets/forOneClassModel_alarm/train/trainSet_20240119002902.npz\n",
      "NpzFile '../datasets/forOneClassModel_alarm/train/trainSet_20240119002902.npz' with keys: fold1\n",
      "Done\n",
      "Loading test data\n",
      "Done\n",
      "cwd_path:/home/ai/RLRepo/Works/Projects/TransferLearning_for_ACDNet/refine_codes/quanted_keras_models\n",
      "report save path:/home/ai/RLRepo/Works/Projects/TransferLearning_for_ACDNet/refine_codes/quanted_keras_models/results/results_20240130174350.npy\n",
      "tflite save path:./quanted_keras_models/retrain_pruned_model_from_keras_20240130174350.tflite\n",
      "Loading model: ./keras_models/retrain_pruned_cp_weights.h5\n",
      "input_size:30225, output_size:2\n",
      "Targetting Supported Ops [<OpsSet.TFLITE_BUILTINS_INT8: 'TFLITE_BUILTINS_INT8'>]\n",
      "Targetting input type : <dtype: 'int8'>\n",
      "Targetting output type : <dtype: 'int8'>\n",
      "rep_data(quantized data from training data):\n",
      "[[[[ 4.0470015e-02]\n",
      "   [ 3.2501969e-02]\n",
      "   [ 1.7646801e-02]\n",
      "   ...\n",
      "   [ 1.2070991e-02]\n",
      "   [-5.1284569e-05]\n",
      "   [-1.3754993e-02]]]\n",
      "\n",
      "\n",
      " [[[ 5.3913806e-02]\n",
      "   [ 2.3644228e-01]\n",
      "   [ 9.8547734e-02]\n",
      "   ...\n",
      "   [ 2.6961881e-01]\n",
      "   [ 3.4848693e-01]\n",
      "   [ 1.2657855e-01]]]\n",
      "\n",
      "\n",
      " [[[-9.8841358e-03]\n",
      "   [-2.1446925e-02]\n",
      "   [-1.2971803e-02]\n",
      "   ...\n",
      "   [-2.1116134e-02]\n",
      "   [ 2.8995700e-02]\n",
      "   [ 4.6130501e-02]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 5.3317344e-04]\n",
      "   [ 9.1317965e-04]\n",
      "   [ 1.3854944e-03]\n",
      "   ...\n",
      "   [-5.0080777e-04]\n",
      "   [-5.7533209e-04]\n",
      "   [-7.2515273e-04]]]\n",
      "\n",
      "\n",
      " [[[ 1.3912301e-01]\n",
      "   [ 1.3863479e-01]\n",
      "   [ 1.2619992e-01]\n",
      "   ...\n",
      "   [ 7.5867897e-01]\n",
      "   [ 7.0886081e-01]\n",
      "   [ 5.9499454e-01]]]\n",
      "\n",
      "\n",
      " [[[ 6.7236447e-03]\n",
      "   [ 1.0150543e-02]\n",
      "   [ 1.0115938e-02]\n",
      "   ...\n",
      "   [ 1.2517251e-02]\n",
      "   [ 3.1933740e-02]\n",
      "   [ 4.3902844e-02]]]]\n",
      "Representative dataset dtype : float32\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp43uode8v/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp43uode8v/assets\n",
      "/home/ai/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-01-30 17:43:52.614578: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-01-30 17:43:52.614595: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-01-30 17:43:52.614692: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp43uode8v\n",
      "2024-01-30 17:43:52.618275: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-01-30 17:43:52.618284: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp43uode8v\n",
      "2024-01-30 17:43:52.625365: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-01-30 17:43:52.678687: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp43uode8v\n",
      "2024-01-30 17:43:52.697054: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 82361 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 27, Total Ops 54, % non-converted = 50.00 %\n",
      " * 27 ARITH ops\n",
      "\n",
      "- arith.constant:   27 occurrences  (f32: 25, i32: 2)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 1)\n",
      "  (f32: 12)\n",
      "  (f32: 1)\n",
      "  (f32: 6)\n",
      "\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      " model saved \n",
      "******************************\n",
      "TFLite input dtype :  <class 'numpy.int8'>\n",
      "TFLite output dtype :  <class 'numpy.int8'>\n",
      "Calculating TF Accuracy : ./keras_models/retrain_pruned_cp_weights.h5\n",
      "Error: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    opt = getOpts()\n",
    "    opt.model = \"./keras_models/retrain_pruned_cp_weights.h5\"\n",
    "    opt.trainData = \"../datasets/forOneClassModel_alarm/train/trainSet_20240119002902.npz\"\n",
    "    opt.valData = \"../datasets/forOneClassModel_alarm/test_val/final_val_test_npz/final_valSet_20240119004614.npz\"\n",
    "    opt.split = 1\n",
    "    assert os.path.exists(opt.model), \\\n",
    "      f'File not found: {opt.model}\\n'\n",
    "\n",
    "    # Setup environment\n",
    "    run_tests()\n",
    "\n",
    "    # Setup required variables\n",
    "    # opt = opts.parse();\n",
    "    dtype = 'int8'\n",
    "    cwd_path = os.path.abspath(\"./quanted_keras_models/\")\n",
    "    print(f\"cwd_path:{cwd_path}\")\n",
    "    result_path = os.path.abspath(\"./quanted_keras_models/results/\")#os.path.join(cwd_path, 'results')\n",
    "    print(f\"result_path:{result_path}\")\n",
    "    \n",
    "    tmp_path = os.path.join(cwd_path, 'tmp')      \n",
    "    # Get the data sets\n",
    "    trainer = Trainer(opt);\n",
    "    trainer.load_training_data();\n",
    "    trainer.load_test_data();\n",
    "    k_model_path = \"./keras_models/retrain_pruned_cp_weights.h5\";\n",
    "    # Start conversion\n",
    "    main(k_model_path, dtype, result_path, tmp_path, trainer)\n",
    "    # main(sys.argv[1], dtype, result_path, tmp_path, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce949c-a5b7-44b8-b732-ada9adfec4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69298772-bfb7-471c-83ab-d93af85cca72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d33883-f892-4b20-b511-3685b6bdbd33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55a885-2fd9-4b36-8686-0b1f6bced7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
