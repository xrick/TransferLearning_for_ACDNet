{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d5c4b4-4778-4a65-bf00-fb929c3f3e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 11:02:58.101384: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-06 11:02:58.140792: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-06 11:02:58.140825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-06 11:02:58.141789: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-06 11:02:58.147315: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-06 11:02:58.668676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac005e2-049d-4fa8-a221-5fa56accff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "sys.path.append(os.path.abspath(\"../deployment/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39dda4d4-bb2c-471a-b389-67d6bdf5c309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deployment.lib_original import *\n",
    "from deployment.results import *\n",
    "from deployment.data_loader import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c524c1d-fba0-4b15-9856-9095cd1e0eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4084044-b531-4a1f-969e-c5cf47df466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasConverter():\n",
    "  def __init__(self, model_file=None, dtype=None, dest_path=None, trainer=None):\n",
    "    '''Initialise the Keras model converter'''\n",
    "\n",
    "    self.model_file = model_file\n",
    "    self.dtype = dtype\n",
    "    self.quant_support = quant_support[dtype]\n",
    "    self.tflite_path = model_file;#f'{dest_path}/g_model.tflite'\n",
    "    self.cc_path = f'{dest_path}/model.cc'\n",
    "    self.h_path = f'{dest_path}/model.h'\n",
    "    self.trainer = trainer    \n",
    "\n",
    "  def load_model(self):\n",
    "    print(f'Loading model: {self.model_file}')\n",
    "    self.model = keras.models.load_model(self.model_file)\n",
    "  \n",
    "  def get_model_size(self):\n",
    "    '''Returns the input size and output size'''\n",
    "    return self.model.inputs[0].shape[-2], self.model.outputs[0].shape[-1]\n",
    "\n",
    "  def get_tf_summary(self):\n",
    "    '''Displays a model summary'''\n",
    "    stringlist = []\n",
    "    self.model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    short_model_summary = \"\\n\".join(stringlist)\n",
    "    return short_model_summary\n",
    "\n",
    "  def get_tflite_summary(self):\n",
    "    '''Displays a model summary'''\n",
    "    with tf.io.gfile.GFile(self.tflite_path, 'rb') as f:\n",
    "      model_content = f.read()\n",
    "        \n",
    "    interpreter = tf.lite.Interpreter(model_content = model_content)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    return (interpreter.get_tensor_details(), \\\n",
    "      interpreter._get_ops_details())\n",
    "\n",
    "  def get_arena_size(self):\n",
    "    '''Returns the approx tensor_arena size'''\n",
    "    tensor_details, op_details = self.get_tflite_summary()\n",
    "\n",
    "    get_tensor = lambda index : [t for t in tensor_details if t['index'] == index][0]\n",
    "    get_node_tensors = lambda n : [get_tensor(t) for t in np.concatenate((n['inputs'],n['outputs']), axis= None)]    \n",
    "    get_tensor_size = lambda t : np.prod(t['shape']) * np.dtype(t['dtype']).itemsize\n",
    "    get_node_tensor_sizes = lambda o : np.sum([get_tensor_size(t) for t in get_node_tensors(o)])   \n",
    "    get_max_node_size = lambda : np.max([get_node_tensor_sizes(o) for o in op_details])\n",
    "\n",
    "    return get_max_node_size() \n",
    "\n",
    "  def get_input_data(self):\n",
    "    '''Retrieves the input data set'''\n",
    "    x,y = self.trainer.testX, self.trainer.testY\n",
    "    return x, y\n",
    "\n",
    "  def get_cast_input_data(self, dtype = None):\n",
    "    '''Retrieves the input data set, casting to target dtype'''\n",
    "    x, y = self.get_input_data()\n",
    "\n",
    "    if dtype is None:\n",
    "      print('dtype not provided')\n",
    "      return x,y   \n",
    "    return get_cast(dtype)(x, axis = -2), y\n",
    "  \n",
    "  def get_rep_data(self, dtype = None):\n",
    "    '''Retrieves the reprepresentative data set'''\n",
    "    x, y = self.trainer.trainX, self.trainer.trainY\n",
    "\n",
    "    if dtype is None:\n",
    "      return x,y\n",
    "    return get_cast(dtype)(x, axis=-2), y\n",
    "\n",
    "  def evaluate_accuracy(self, y_pred, y_target, crops = 1):\n",
    "    '''A common accuracy operation which supports multi-crop'''\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0]//crops, crops, y_pred.shape[1])\n",
    "    y_target = y_target.reshape(y_target.shape[0]//crops, crops, y_target.shape[1])\n",
    "\n",
    "    #Calculate the average of class predictions for 10 crops of a sample\n",
    "    y_pred = np.mean(y_pred, axis=1)\n",
    "    y_target = np.mean(y_target,axis=1)\n",
    "\n",
    "    #Get the indices that has highest average value for each sample\n",
    "    y_pred = y_pred.argmax(axis=1)\n",
    "    y_target = y_target.argmax(axis=1)\n",
    "\n",
    "    accuracy = (y_pred==y_target).mean()\n",
    "    return accuracy\n",
    "\n",
    "  def predict_tf(self, x_data):\n",
    "    '''Calculate the output of a single inference of the TF model'''\n",
    "    x = tf.expand_dims(x_data, 0).numpy()\n",
    "    return self.model.predict([x])\n",
    "\n",
    "  def get_tf_accuracy(self, crops = 1):\n",
    "    '''Calculate accuracy of the TF model'''\n",
    "    x_data, y_data = self.get_input_data()\n",
    "\n",
    "    y_pred = self.model.predict([x_data])\n",
    "\n",
    "    accuracy = self.evaluate_accuracy(y_pred, y_data, crops)\n",
    "    return accuracy, y_pred, y_data\n",
    "\n",
    "  def predict_tflite(self, x_data):\n",
    "    '''Calculate output of single inference of the TFLite model'''\n",
    "    with tf.io.gfile.GFile(self.tflite_path, 'rb') as f:\n",
    "      model_content = f.read()\n",
    "        \n",
    "    interpreter = tf.lite.Interpreter(model_content = model_content)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_index = interpreter.get_input_details()[0]['index']\n",
    "    output_index = interpreter.get_output_details()[0]['index']\n",
    "\n",
    "    input_dtype = interpreter.get_input_details()[0]['dtype']\n",
    "    x = get_cast(input_dtype)(x_data, axis=-2)\n",
    "\n",
    "    interpreter.set_tensor(input_index, x)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    return interpreter.get_tensor(output_index)[0]\n",
    "\n",
    "  def get_tflite_accuracy(self, crops = 1):\n",
    "    '''Calculate the accuracy of the TFLite model'''\n",
    "    with tf.io.gfile.GFile(self.tflite_path, 'rb') as f:\n",
    "      model_content = f.read()\n",
    "        \n",
    "    interpreter = tf.lite.Interpreter(model_content = model_content)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_index = interpreter.get_input_details()[0]['index']\n",
    "    output_index = interpreter.get_output_details()[0]['index']\n",
    "\n",
    "    input_dtype = interpreter.get_input_details()[0]['dtype']\n",
    "    print(f'Input dtype {str(input_dtype)}')\n",
    "\n",
    "    x_data, y_data = self.get_cast_input_data(input_dtype)\n",
    "\n",
    "    def predict(x_input):\n",
    "      \n",
    "      x_input = tf.expand_dims(x_input, 0).numpy()\n",
    "      interpreter.set_tensor(input_index, x_input)\n",
    "      # Run inference.\n",
    "      interpreter.invoke()\n",
    "      return interpreter.get_tensor(output_index)[0]\n",
    "    \n",
    "    y_pred = np.array([predict(x) for x in x_data])\n",
    "    print(y_pred.shape)\n",
    "    print(y_data.shape)\n",
    "    accuracy = self.evaluate_accuracy(y_pred, y_data, crops)\n",
    "    return accuracy, y_pred, y_data  \n",
    "\n",
    "  def generate_tflite(self):\n",
    "    '''Generates a TFLite file from a Keras model'''\n",
    "\n",
    "    # Construction of a TFLite converter\n",
    "    tf_converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n",
    "    \n",
    "    tf_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    if 'supported_ops' in self.quant_support:\n",
    "      print(f'Targetting Supported Ops {self.quant_support[\"supported_ops\"]}')\n",
    "      tf_converter.target_spec.supported_ops = self.quant_support['supported_ops']\n",
    "\n",
    "    if 'supported_types' in self.quant_support:\n",
    "      print(f'Targetting Supported Types{self.quant_support[\"supported_types\"]}')\n",
    "      tf_converter.target_spec.supported_types = self.quant_support['supported_types']\n",
    "\n",
    "    if 'input_type' in self.quant_support:\n",
    "      print(f'Targetting input type : {self.quant_support[\"input_type\"]}')\n",
    "      tf_converter.inference_input_type = self.quant_support['input_type']\n",
    "\n",
    "    if 'output_type' in self.quant_support:\n",
    "      print(f'Targetting output type : {self.quant_support[\"output_type\"]}')\n",
    "      tf_converter.inference_output_type = self.quant_support['output_type']\n",
    "\n",
    "    # Supplying a representative dataset is required for full integer \n",
    "    # quantization, and also avoids dynamic range quantization\n",
    "\n",
    "    rep_data, _ = self.get_rep_data(None)\n",
    "    print(f'Representative dataset dtype : {rep_data.dtype}')\n",
    "\n",
    "    def representative_dataset_no_padding():\n",
    "      for i in range(len(rep_data)):\n",
    "        if rep_data[i:i+1,:,0,:] != 0 and rep_data[i:i+1,:,-1,:] != 0:\n",
    "          yield([rep_data[i:i+1,:,:,:]])\n",
    "\n",
    "    def representative_dataset():\n",
    "      for i in range(len(rep_data)):\n",
    "        yield([rep_data[i:i+1,:,:,:]])\n",
    "\n",
    "    tf_converter.representative_dataset = representative_dataset\n",
    "\n",
    "    tflite_model = tf_converter.convert()\n",
    "    bytes_written = open(self.tflite_path, 'wb').write(tflite_model)\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "    input_type = interpreter.get_input_details()[0]['dtype']\n",
    "    output_type = interpreter.get_output_details()[0]['dtype']\n",
    "\n",
    "    print('TFLite input dtype : ', input_type)\n",
    "    print('TFLite output dtype : ', output_type)\n",
    "    \n",
    "    return bytes_written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa50a361-91c0-461f-acdf-db7f1816c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Calculate Arena Size');\n",
    "    parser.add_argument('--netType', default='ACDNet_TL_Model_Extend',  required=False);\n",
    "    parser.add_argument('--data', default='../datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args();\n",
    "    \n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 32;\n",
    "    opt.LR = 0.1;\n",
    "    opt.weightDecay = 5e-3#9e-3;#5e-3;#5e-2;#1e-2;#5e-4;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.nEpochs = 1000;\n",
    "    opt.schedule = [0.3, 0.6, 0.9];\n",
    "    opt.warmup = 10;\n",
    "    # if torch.backends.mps.is_available():\n",
    "    #     opt.device=\"mps\"; #for apple m2 gpu\n",
    "    # elif torch.cuda.is_available():\n",
    "    #     opt.device=\"cuda:0\"; #for nVidia gpu\n",
    "    # else:\n",
    "    opt.device=\"cpu\"\n",
    "    print(f\"***Use device:{opt.device}\");\n",
    "    # opt.device = torch.device(\"cuda:0\" if  else \"cpu\");\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 2#50;\n",
    "    opt.nFolds = 1;\n",
    "    opt.splits = [i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    opt.TLAcdnetConfig = [8,64,32,64,64,128,128,256,256,512,512,2];\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ac263-9f47-4f3d-86ae-dfb984d0cd3d",
   "metadata": {},
   "source": [
    "## 目前使用ACDNET中的KerasConverter中計算arena_size的函式，權宜之計把"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3345235a-c3d3-4ecd-bde6-0fd804566480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_arena_size(tflite_path):\n",
    "    '''Displays a model summary'''\n",
    "    with tf.io.gfile.GFile(tflite_path, 'rb') as f:\n",
    "      model_content = f.read()\n",
    "    interpreter = tf.lite.Interpreter(model_content = model_content)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    return (interpreter.get_tensor_details(), \\\n",
    "      interpreter._get_ops_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20965e00-1d0b-482b-9ec4-1df25b68ebcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arena size is 318169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "modelPath = \"../th/quantized_models/quant_retrained_model_0.9compress_acc_82.9_20240206104744.tflite\";\n",
    "d_type = 'int8'\n",
    "dest_path = \"../th/quantized_models/test_model.tflite\"\n",
    "# trainer = Trainer(getOpts());\n",
    "keras_converter = KerasConverter(model_file=modelPath,dest_path=dest_path,dtype=d_type,trainer=None);\n",
    "arena_size = keras_converter.get_arena_size();\n",
    "print(f\"arena size is {arena_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085afd50-8391-41f9-95b0-30b2a3b97b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
