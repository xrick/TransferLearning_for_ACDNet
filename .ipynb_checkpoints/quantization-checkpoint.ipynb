{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692379c8-c60a-4ddb-a425-09c0b030bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import glob;\n",
    "import math;\n",
    "import random;\n",
    "import torch;\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5939dcb3-4805-42d7-9900-637088d59e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7733cacf-1504-4ce6-983c-871ee1dc116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.opts as opts;\n",
    "import th.resources.models as models;\n",
    "import th.resources.calculator as calc;\n",
    "# import resources.train_generator as train_generator;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df514eb-ae13-469e-bb95-c92fbdd7cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# import common.tlopts as tlopts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d04c10f-d1a7-454a-b163-cac1d59dd7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.utils as U;\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from tinynn.converter import TFLiteConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9de080e-60bb-4cba-b9ac-8727f3b422d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb6c480-7332-43f6-9893-f023919f171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class ACDNetQuant(nn.Module):\n",
    "#     def __init__(self, input_length, n_class, sr, ch_conf=None):\n",
    "#         super(ACDNetQuant, self).__init__();\n",
    "#         self.input_length = input_length;\n",
    "#         self.ch_config = ch_conf;\n",
    "\n",
    "#         stride1 = 2;\n",
    "#         stride2 = 2;\n",
    "#         channels = 8;\n",
    "#         k_size = (3, 3);\n",
    "#         n_frames = (sr/1000)*10; #No of frames per 10ms\n",
    "\n",
    "#         sfeb_pool_size = int(n_frames/(stride1*stride2));\n",
    "#         # tfeb_pool_size = (2,2);\n",
    "#         if self.ch_config is None:\n",
    "#             self.ch_config = [channels, channels*8, channels*4, channels*8, channels*8, channels*16, channels*16, channels*32, channels*32, channels*64, channels*64, n_class];\n",
    "#         # avg_pool_kernel_size = (1,4) if self.ch_config[1] < 64 else (2,4);\n",
    "#         fcn_no_of_inputs = self.ch_config[-1];\n",
    "#         conv1, bn1 = self.make_layers(1, self.ch_config[0], (1, 9), (1, stride1));\n",
    "#         conv2, bn2 = self.make_layers(self.ch_config[0], self.ch_config[1], (1, 5), (1, stride2));\n",
    "#         conv3, bn3 = self.make_layers(1, self.ch_config[2], k_size, padding=1);\n",
    "#         conv4, bn4 = self.make_layers(self.ch_config[2], self.ch_config[3], k_size, padding=1);\n",
    "#         conv5, bn5 = self.make_layers(self.ch_config[3], self.ch_config[4], k_size, padding=1);\n",
    "#         conv6, bn6 = self.make_layers(self.ch_config[4], self.ch_config[5], k_size, padding=1);\n",
    "#         conv7, bn7 = self.make_layers(self.ch_config[5], self.ch_config[6], k_size, padding=1);\n",
    "#         conv8, bn8 = self.make_layers(self.ch_config[6], self.ch_config[7], k_size, padding=1);\n",
    "#         conv9, bn9 = self.make_layers(self.ch_config[7], self.ch_config[8], k_size, padding=1);\n",
    "#         conv10, bn10 = self.make_layers(self.ch_config[8], self.ch_config[9], k_size, padding=1);\n",
    "#         conv11, bn11 = self.make_layers(self.ch_config[9], self.ch_config[10], k_size, padding=1);\n",
    "#         conv12, bn12 = self.make_layers(self.ch_config[10], self.ch_config[11], (1, 1));\n",
    "#         fcn = nn.Linear(fcn_no_of_inputs, n_class);\n",
    "#         nn.init.kaiming_normal_(fcn.weight, nonlinearity='sigmoid') # kaiming with sigoid is equivalent to lecun_normal in keras\n",
    "\n",
    "#         self.sfeb = nn.Sequential(\n",
    "#             #Start: Filter bank\n",
    "#             conv1, bn1, nn.ReLU(),\\\n",
    "#             conv2, bn2, nn.ReLU(),\\\n",
    "#             nn.MaxPool2d(kernel_size=(1, sfeb_pool_size))\n",
    "#         );\n",
    "\n",
    "#         tfeb_modules = [];\n",
    "#         self.tfeb_width = int(((self.input_length / sr)*1000)/10); # 10ms frames of audio length in seconds\n",
    "#         tfeb_pool_sizes = self.get_tfeb_pool_sizes(self.ch_config[1], self.tfeb_width);\n",
    "#         p_index = 0;\n",
    "#         for i in [3,4,6,8,10]:\n",
    "#             tfeb_modules.extend([eval('conv{}'.format(i)), eval('bn{}'.format(i)), nn.ReLU()]);\n",
    "\n",
    "#             if i != 3:\n",
    "#                 tfeb_modules.extend([eval('conv{}'.format(i+1)), eval('bn{}'.format(i+1)), nn.ReLU()]);\n",
    "\n",
    "#             h, w = tfeb_pool_sizes[p_index];\n",
    "#             if h>1 or w>1:\n",
    "#                 tfeb_modules.append(nn.MaxPool2d(kernel_size = (h,w)));\n",
    "#             p_index += 1;\n",
    "\n",
    "#         tfeb_modules.append(nn.Dropout(0.2));\n",
    "#         tfeb_modules.extend([conv12, bn12, nn.ReLU()]);\n",
    "#         h, w = tfeb_pool_sizes[-1];\n",
    "#         if h>1 or w>1:\n",
    "#             tfeb_modules.append(nn.AvgPool2d(kernel_size = (h,w)));\n",
    "#         tfeb_modules.extend([nn.Flatten(), fcn]);\n",
    "\n",
    "#         self.tfeb = nn.Sequential(*tfeb_modules);\n",
    "\n",
    "#         self.output = nn.Sequential(\n",
    "#             nn.Softmax(dim=1)\n",
    "#         );\n",
    "\n",
    "#         self.quant = QuantStub();\n",
    "#         self.dequant = DeQuantStub();\n",
    "#     def forward(self, x):\n",
    "#         #Quantize input\n",
    "#         x = self.quant(x);\n",
    "#         x = self.sfeb(x);\n",
    "#         #swapaxes\n",
    "#         x = x.permute((0, 2, 1, 3));\n",
    "#         x = self.tfeb(x);\n",
    "#         #DeQuantize features before feeding to softmax\n",
    "#         x = self.dequant(x);\n",
    "#         y = self.output[0](x);\n",
    "#         return y;\n",
    "\n",
    "#     def make_layers(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=0, bias=False):\n",
    "#         conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias);\n",
    "#         nn.init.kaiming_normal_(conv.weight, nonlinearity='relu'); # kaiming with relu is equivalent to he_normal in keras\n",
    "#         bn = nn.BatchNorm2d(out_channels);\n",
    "#         return conv, bn;\n",
    "\n",
    "#     def get_tfeb_pool_sizes(self, con2_ch, width):\n",
    "#         h = self.get_tfeb_pool_size_component(con2_ch);\n",
    "#         w = self.get_tfeb_pool_size_component(width);\n",
    "#         # print(w);\n",
    "#         pool_size = [];\n",
    "#         for  (h1, w1) in zip(h, w):\n",
    "#             pool_size.append((h1, w1));\n",
    "#         return pool_size;\n",
    "\n",
    "#     def get_tfeb_pool_size_component(self, length):\n",
    "#         # print(length);\n",
    "#         c = [];\n",
    "#         index = 1;\n",
    "#         while index <= 6:\n",
    "#             if length >= 2:\n",
    "#                 if index == 6:\n",
    "#                     c.append(length);\n",
    "#                 else:\n",
    "#                     c.append(2);\n",
    "#                     length = length // 2;\n",
    "#             else:\n",
    "#                c.append(1);\n",
    "\n",
    "#             index += 1;\n",
    "\n",
    "#         return c;\n",
    "\n",
    "# def GetACDNetQuantModel(input_len=30225, nclass=50, sr=20000, channel_config=None):\n",
    "#     net = ACDNetQuant(input_len, nclass, sr, ch_conf=channel_config);\n",
    "#     return net;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab4a323-4291-421b-867a-d52ee41063ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = dict([(17,1),(18,2),(24,3),\n",
    "                             (51,4),(52,5),(53,6)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[label1]- 1\n",
    "            idx2 = self.mapdict[label2] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a30933e-1045-41a0-b258-2ef29e94bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None):\n",
    "    # dataset = np.load(os.path.join(opt.data, opt.dataset, 'wav{}.npz'.format(opt.sr // 1000)), allow_pickle=True);\n",
    "    # dataset = np.load(\"../datasets/fold1_test16000.npz\", allow_pickle=True);\n",
    "    dataset = np.load(\"./datasets/fold1_dataset.npz\", allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # print(len(dataset['x']))\n",
    "    # for i in range(1, opt.nFolds + 1):\n",
    "\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['fold{}'.format(1)].item()['sounds']\n",
    "    train_labels = dataset['fold{}'.format(1)].item()['labels']\n",
    "    # print(train_sounds)\n",
    "\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbc776e4-0edf-4424-a3af-a946d33db7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='./datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 64;\n",
    "    opt.weightDecay = 5e-4;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.nEpochs = 10;#2000;\n",
    "    opt.LR = 0.1;\n",
    "    opt.schedule = [0.3, 0.6, 0.9];\n",
    "    opt.warmup = 10;\n",
    "\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 6#50;\n",
    "    opt.nFolds = 1;#5;\n",
    "    opt.split = 1#[i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 16000#20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 5;\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d56ed47-bc98-4899-9e46-d4726cfa565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, opt=None, split=0):\n",
    "        self.opt = opt;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.trainX = None;\n",
    "        self.trainY = None;\n",
    "        #self.opt.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "        self.opt.device = torch.device(\"cpu\")\n",
    "        self.trainGen = getTrainGen(self.opt)#train_generator.setup(self.opt, self.opt.split);\n",
    "        self.qunt_nClass = 6;\n",
    "\n",
    "    def load_train_data(self):\n",
    "        print('Preparing calibration dataset..');\n",
    "        x,y = self.trainGen.__getitem__(0);\n",
    "        self.trainX = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "        \"\"\"\n",
    "        trainX size:torch.Size([1, 1, 30225]), but must be [1,1,1,30225]\n",
    "        Due to the reason: raise ValueError(\"Input shape must be `(N, C, H, W)`!\")\n",
    "        \"\"\"\n",
    "        # print(f\"trainX[0] shape:{self.trainX[0].shape}\")\n",
    "        self.trainY = torch.tensor(y).to(self.opt.device);\n",
    "        print('Calibration dataset is ready');\n",
    "        self.opt.batchSize = 64;\n",
    "\n",
    "    def load_test_data(self):\n",
    "        if(self.testX is None):\n",
    "            data = np.load('./datasets/fold1_test16000_65batch.npz', allow_pickle=True);\n",
    "            self.testX = torch.tensor(np.moveaxis(data['x'], 3, 1)).to(self.opt.device);\n",
    "            self.testY = torch.tensor(data['y']).to(self.opt.device);\n",
    "\n",
    "    def __validate(self, net, testX, testY):\n",
    "        net.eval();\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "            for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "                x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "                #print(x.shape);\n",
    "                # exit();\n",
    "                scores = net(x);\n",
    "                y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "\n",
    "            acc = self.__compute_accuracy(y_pred, self.testY);\n",
    "        return acc;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def __compute_accuracy(self, y_pred, y_target):\n",
    "        print(y_pred.shape);\n",
    "        with torch.no_grad():\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1);\n",
    "\n",
    "            y_pred = y_pred.argmax(dim=1);\n",
    "            y_target = y_target.argmax(dim=1);\n",
    "\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "        return acc;\n",
    "\n",
    "    def __load_model(self, quant=False):\n",
    "        state = torch.load(self.opt.model_path, map_location=self.opt.device);\n",
    "        if quant:\n",
    "            net = models.GetACDNetQuantModel(input_len=self.opt.inputLength, nclass=self.qunt_nClass, sr=self.opt.sr, channel_config=state['config']).to(self.opt.device);\n",
    "        else:\n",
    "            net = models.GetACDNetModel(input_len=self.opt.inputLength, nclass=self.qunt_nClass, sr=self.opt.sr, channel_config=state['config']).to(self.opt.device);\n",
    "        net.load_state_dict(state['weight']);\n",
    "        return net;\n",
    "\n",
    "    def __calibrate(self, net):\n",
    "        self.load_train_data();\n",
    "        net.eval();\n",
    "        with torch.no_grad():\n",
    "            for i in range(1,2):\n",
    "                x_pred = None;\n",
    "                for idx in range(math.ceil(len(self.trainX)/self.opt.batchSize)):\n",
    "                    x = self.trainX[idx*self.opt.batchSize : (idx+1)*self.opt.batchSize];\n",
    "                    #print(x.shape);\n",
    "                    # exit();\n",
    "                    scores = net(x);\n",
    "                    x_pred = scores.data if x_pred is None else torch.cat((x_pred, scores.data));\n",
    "\n",
    "                x_pred = x_pred.argmax(dim=1);\n",
    "                x_target = self.trainY.argmax(dim=1);\n",
    "\n",
    "                acc = (((x_pred==x_target)*1).float().mean()*100).item();\n",
    "                print('calibrate accuracy is: {:.2f}'.format(acc));\n",
    "        return acc;\n",
    "\n",
    "    def QuantizeModel(self):\n",
    "        net = self.__load_model(True);\n",
    "        config = net.ch_config;\n",
    "        net.eval();\n",
    "\n",
    "        #Fuse modules to\n",
    "        torch.quantization.fuse_modules(net.sfeb, ['0','1','2'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.sfeb, ['3','4','5'], inplace=True);\n",
    "\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['0','1','2'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['4','5','6'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['7','8','9'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['11','12','13'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['14','15','16'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['18','19','20'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['21','22','23'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['25','26','27'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['28','29','30'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['33','34','35'], inplace=True);\n",
    "\n",
    "        # Specify quantization configuration\n",
    "        net.qconfig = torch.quantization.get_default_qconfig('qnnpack');\n",
    "        torch.backends.quantized.engine = 'qnnpack';\n",
    "        print(net.qconfig);\n",
    "\n",
    "        torch.quantization.prepare(net, inplace=True);\n",
    "\n",
    "        # Calibrate with the training data\n",
    "        self.__calibrate(net);\n",
    "\n",
    "        # Convert to quantized model\n",
    "        torch.quantization.convert(net, inplace=True);\n",
    "        print('Post Training Quantization: Convert done');\n",
    "\n",
    "        print(\"Size of model after quantization\");\n",
    "        torch.save(net.state_dict(), \"temp.p\")\n",
    "        print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "        os.remove('temp.p')\n",
    "        # dummy_input = torch.randn(1, 1, 30225, 1); not work\n",
    "        # dummy_input = torch.randn(1, 1, 1, 30225); workable\n",
    "        dummy_input = torch.randn(1,1,30225);# correct\n",
    "        converter = TFLiteConverter(net,\n",
    "                                    dummy_input,\n",
    "                                    tflite_path=\"./th/tflite_models/{}.tflite\".format(self.opt.model_name))\n",
    "        converter.convert()\n",
    "        self.load_test_data();\n",
    "        val_acc = self.__validate(net, self.testX, self.testY);\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "\n",
    "        torch.jit.save(torch.jit.script(net), '{}/th/quantized_models/{}.pt'.format(os.getcwd(), self.opt.model_name));\n",
    "        # net.cpu();\n",
    "        # net.eval();\n",
    "        \n",
    "        \n",
    "    def TestModel(self, quant=False):\n",
    "        if quant:\n",
    "            net = torch.jit.load(os.getcwd() + '/th/quantized_models/' + self.opt.model_name + '.pt')\n",
    "        else:\n",
    "            net = self.__load_model();\n",
    "            calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        self.load_test_data();\n",
    "        net.eval();\n",
    "        val_acc = self.__validate(net, self.testX, self.testY);\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "\n",
    "    def GetModelSize(self):\n",
    "        orig_net_path = self.opt.model_path;\n",
    "        print('Full precision model size (KB):', os.path.getsize(orig_net_path)/(1024));\n",
    "        quant_net_path = os.getcwd()+'/20khz_l0_tay_full_80_86.5_1403_quant.onnx';\n",
    "        print('Quantized model size (KB):', os.path.getsize(quant_net_path)/(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58eff9fb-5fd1-423b-9a60-61c9e9824dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_convert_model_path = \"./th/pruned_models/second_stage_pruned_models/magnitude_pruning/acdnet_tl_hybrid_pruning_magnitude_model_202312281149_80.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce59dbc6-1088-427e-a3bb-5e88d07465d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    opt = getOpts();#opts.parse();\n",
    "    opt.batchSize = 1600;\n",
    "    opt.model_path = \"./th/pruned_models/second_stage_pruned_models/magnitude_pruning/acdnet_tl_hybrid_pruning_magnitude_model_202312281149_80.pt\"\n",
    "    \n",
    "    # valid_path = False;\n",
    "    # while not valid_path:\n",
    "    #     model_path = input(\"Enter the model PATH for 8-bit post training quantization\\n:\");\n",
    "    #     file_paths = glob.glob(os.path.join(os.getcwd(), model_path));\n",
    "    #     if len(file_paths)>0 and os.path.isfile(file_paths[0]):\n",
    "    #         state = torch.load(file_paths[0], map_location='cpu');\n",
    "    #         opt.model_path = file_paths[0];\n",
    "    #         print('Model has been found at: {}'.format(opt.model_path));\n",
    "    #         valid_path = True;\n",
    "\n",
    "    opt.model_name = \"acdnet_tl_quant_model_202312281837_beforgohome\"\n",
    "    # valid_model_name = False;\n",
    "    # while not valid_model_name:\n",
    "    #     model_name = input('Enter a name that will be used to save the quantized model model: ');\n",
    "    #     if model_name != '':\n",
    "    #         opt.model_name = model_name;\n",
    "    #         valid_model_name = True;\n",
    "    opt.split = 1;\n",
    "    trainer = Trainer(opt);\n",
    "\n",
    "    print('Testing performance of the provided model.....');\n",
    "    trainer.TestModel();\n",
    "\n",
    "    print('Quantization process is started.....');\n",
    "    trainer.QuantizeModel();\n",
    "    print('Quantization done');\n",
    "\n",
    "    print('Testing quantized model.');\n",
    "    trainer.TestModel(True);\n",
    "    print('Finished');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a95756a3-5cc4-4276-9272-a22ec24efaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of samples:65\n",
      "Testing performance of the provided model.....\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (8, 1, 15109)         72    1,087,848\n",
      "  BatchNorm2d-2     (8, 1, 15109)     (8, 1, 15109)         16            0\n",
      "         ReLu-3     (8, 1, 15109)     (8, 1, 15109)          0      120,872\n",
      "       Conv2d-4     (8, 1, 15109)     (33, 1, 7553)      1,320    9,969,960\n",
      "  BatchNorm2d-5     (33, 1, 7553)     (33, 1, 7553)         66            0\n",
      "         ReLu-6     (33, 1, 7553)     (33, 1, 7553)          0      249,249\n",
      "    MaxPool2d-7     (33, 1, 7553)      (33, 1, 188)          0      248,160\n",
      "      Permute-8      (33, 1, 188)      (1, 33, 188)          0            0\n",
      "       Conv2d-9      (1, 33, 188)     (20, 33, 188)        180    1,116,720\n",
      " BatchNorm2d-10     (20, 33, 188)     (20, 33, 188)         40            0\n",
      "        ReLu-11     (20, 33, 188)     (20, 33, 188)          0      124,080\n",
      "   MaxPool2d-12     (20, 33, 188)      (20, 16, 94)          0      120,320\n",
      "      Conv2d-13      (20, 16, 94)      (32, 16, 94)      5,760    8,663,040\n",
      " BatchNorm2d-14      (32, 16, 94)      (32, 16, 94)         64            0\n",
      "        ReLu-15      (32, 16, 94)      (32, 16, 94)          0       48,128\n",
      "      Conv2d-16      (32, 16, 94)      (40, 16, 94)     11,520   17,326,080\n",
      " BatchNorm2d-17      (40, 16, 94)      (40, 16, 94)         80            0\n",
      "        ReLu-18      (40, 16, 94)      (40, 16, 94)          0       60,160\n",
      "   MaxPool2d-19      (40, 16, 94)       (40, 8, 47)          0       60,160\n",
      "      Conv2d-20       (40, 8, 47)       (41, 8, 47)     14,760    5,549,760\n",
      " BatchNorm2d-21       (41, 8, 47)       (41, 8, 47)         82            0\n",
      "        ReLu-22       (41, 8, 47)       (41, 8, 47)          0       15,416\n",
      "      Conv2d-23       (41, 8, 47)       (38, 8, 47)     14,022    5,272,272\n",
      " BatchNorm2d-24       (38, 8, 47)       (38, 8, 47)         76            0\n",
      "        ReLu-25       (38, 8, 47)       (38, 8, 47)          0       14,288\n",
      "   MaxPool2d-26       (38, 8, 47)       (38, 4, 23)          0       13,984\n",
      "      Conv2d-27       (38, 4, 23)       (51, 4, 23)     17,442    1,604,664\n",
      " BatchNorm2d-28       (51, 4, 23)       (51, 4, 23)        102            0\n",
      "        ReLu-29       (51, 4, 23)       (51, 4, 23)          0        4,692\n",
      "      Conv2d-30       (51, 4, 23)       (44, 4, 23)     20,196    1,858,032\n",
      " BatchNorm2d-31       (44, 4, 23)       (44, 4, 23)         88            0\n",
      "        ReLu-32       (44, 4, 23)       (44, 4, 23)          0        4,048\n",
      "   MaxPool2d-33       (44, 4, 23)       (44, 2, 11)          0        3,872\n",
      "      Conv2d-34       (44, 2, 11)       (48, 2, 11)     19,008      418,176\n",
      " BatchNorm2d-35       (48, 2, 11)       (48, 2, 11)         96            0\n",
      "        ReLu-36       (48, 2, 11)       (48, 2, 11)          0        1,056\n",
      "      Conv2d-37       (48, 2, 11)       (46, 2, 11)     19,872      437,184\n",
      " BatchNorm2d-38       (46, 2, 11)       (46, 2, 11)         92            0\n",
      "        ReLu-39       (46, 2, 11)       (46, 2, 11)          0        1,012\n",
      "   MaxPool2d-40       (46, 2, 11)        (46, 1, 5)          0          920\n",
      "      Conv2d-41        (46, 1, 5)         (6, 1, 5)        276        1,380\n",
      " BatchNorm2d-42         (6, 1, 5)         (6, 1, 5)         12            0\n",
      "        ReLu-43         (6, 1, 5)         (6, 1, 5)          0           30\n",
      "   AvgPool2d-44         (6, 1, 5)         (6, 1, 1)          0           30\n",
      "     Flatten-45         (6, 1, 1)            (1, 6)          0            0\n",
      "      Linear-46            (1, 6)            (1, 6)         42           42\n",
      "     Softmax-47            (1, 6)            (1, 6)          0            6\n",
      "==============================================================================\n",
      "Total Params: 125,284\n",
      "Total FLOPs : 54,395,641\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 0.48\n",
      "Total size (MB) : 0.59\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([325, 6])\n",
      "Testing: Acc(top1) 21.54%\n",
      "Quantization process is started.....\n",
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=False){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "Preparing calibration dataset..\n",
      "Calibration dataset is ready\n",
      "calibrate accuracy is: 18.44\n",
      "Post Training Quantization: Convert done\n",
      "Size of model after quantization\n",
      "Size (MB): 0.142851\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input shape must be `(N, C, H, W)`!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 30\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m trainer\u001b[38;5;241m.\u001b[39mTestModel();\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantization process is started.....\u001b[39m\u001b[38;5;124m'\u001b[39m);\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuantizeModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantization done\u001b[39m\u001b[38;5;124m'\u001b[39m);\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting quantized model.\u001b[39m\u001b[38;5;124m'\u001b[39m);\n",
      "Cell \u001b[0;32mIn[33], line 129\u001b[0m, in \u001b[0;36mTrainer.QuantizeModel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m30225\u001b[39m);\u001b[38;5;66;03m# correct\u001b[39;00m\n\u001b[1;32m    126\u001b[0m converter \u001b[38;5;241m=\u001b[39m TFLiteConverter(net,\n\u001b[1;32m    127\u001b[0m                             dummy_input,\n\u001b[1;32m    128\u001b[0m                             tflite_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./th/tflite_models/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mmodel_name))\n\u001b[0;32m--> 129\u001b[0m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_test_data();\n\u001b[1;32m    131\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__validate(net, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestX, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestY);\n",
      "File \u001b[0;32m~/miniconda3/envs/torch121env/lib/python3.10/site-packages/tinynn/converter/base.py:476\u001b[0m, in \u001b[0;36mTFLiteConverter.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_flatten_inputs()\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_input_transpose()\n\u001b[0;32m--> 476\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_lowered_module()\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_common_graph()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch121env/lib/python3.10/site-packages/tinynn/converter/base.py:228\u001b[0m, in \u001b[0;36mTFLiteConverter.init_jit_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 228\u001b[0m     script \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# Remove reference to original model to save memory\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch121env/lib/python3.10/site-packages/torch/jit/_trace.py:750\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m ):\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    768\u001b[0m         func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m,\n\u001b[1;32m    769\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m: example_inputs},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m         _module_class,\n\u001b[1;32m    777\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch121env/lib/python3.10/site-packages/torch/jit/_trace.py:967\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    963\u001b[0m     argument_names \u001b[38;5;241m=\u001b[39m get_callable_argument_names(func)\n\u001b[1;32m    965\u001b[0m example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m--> 967\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch121env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch121env/lib/python3.10/site-packages/torch/nn/modules/module.py:1118\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1118\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/RLRepo/Works/Projects/TransferLearning_for_ACDNet/th/resources/models.py:205\u001b[0m, in \u001b[0;36mACDNetQuant.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m#Quantize input\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(x);\n\u001b[0;32m--> 205\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msfeb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m#swapaxes\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m));\n",
      "File \u001b[0;32m~/miniconda3/envs/torch121env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch121env/lib/python3.10/site-packages/torch/nn/modules/module.py:1118\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1118\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch121env/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch121env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch121env/lib/python3.10/site-packages/torch/nn/modules/module.py:1118\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1118\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch121env/lib/python3.10/site-packages/torch/nn/intrinsic/quantized/modules/conv_relu.py:87\u001b[0m, in \u001b[0;36mConvReLU2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# Temporarily using len(shape) instead of ndim due to JIT issue\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/23890\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape must be `(N, C, H, W)`!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     89\u001b[0m         _reversed_padding_repeated_twice \u001b[38;5;241m=\u001b[39m _reverse_repeat_padding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding)\n",
      "\u001b[0;31mValueError\u001b[0m: Input shape must be `(N, C, H, W)`!"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29a3ca04-7384-4a90-853e-050b593a0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt2 = getOpts();#opts.parse();\n",
    "# state2 = torch.load(to_convert_model_path, map_location=\"cuda:0\");\n",
    "# tmpnet = models.GetACDNetModel(input_len=opt2.inputLength, nclass=6, sr=20000, channel_config=state2['config']).to(\"cuda:0\");\n",
    "# tmpnet.load_state_dict(state2['weight']);\n",
    "# print(tmpnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b39481f2-f841-401b-bfac-119e40121d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantnet = torch.jit.load('./th/quantized_models/acdnet_tl_quant_model_202312281348_80.pt')\n",
    "# print(quantnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab64fe84-ccca-4d35-80a3-edfd7012b8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
