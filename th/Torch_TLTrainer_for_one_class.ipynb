{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7cc9cb1-7673-4e90-b4c6-a32aba2e4c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import glob;\n",
    "import math;\n",
    "import numpy as np;\n",
    "import glob;\n",
    "import random;\n",
    "import time;\n",
    "import torch;\n",
    "import torch.optim as optim;\n",
    "import torch.nn as nn;\n",
    "\n",
    "sys.path.append(os.getcwd());\n",
    "sys.path.append('../');\n",
    "# sys.path.append(os.path.join(os.getcwd(), 'torch/resources'));\n",
    "import common.utils as U;\n",
    "import common.opts as opts;\n",
    "import resources.models as models;\n",
    "import resources.calculator as calc;\n",
    "import common.tlopts as tlopts\n",
    "# import resources.train_generator as train_generator;\n",
    "import argparse\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0815030-17d2-4406-b087-0409a9863de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02fa20f3-fcd0-41e4-ae27-081ad0ac3144",
   "metadata": {},
   "source": [
    "## current best:\n",
    "    - acc:87.2  \n",
    "    - learning rate:0.01  \n",
    "    - batch_size:16  \n",
    "    - weightDecay = 5e-4;  \n",
    "    - momentum = 0.9;       \n",
    "    - schedule = [0.3, 0.6, 0.9];  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f53e33-c3ee-4c51-a448-5ebfc6da731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reproducibility\n",
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037be559-e258-4780-9416-dd969489d029",
   "metadata": {},
   "source": [
    "## define TLTraining Generator Class\n",
    "The Class is an python iterator class for generating data for trainer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9850c9d-6a7d-41ed-9244-a802f25b316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples=None, labels=None, options=None, classes_dict=None):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = classes_dict\n",
    "        #{\n",
    "            # 17:1, #pouring_water\n",
    "            # 18:2, #toilet_flushing\n",
    "            # 21:3, #snezzing\n",
    "            # 24:4, #coughing\n",
    "            # 51:5, #kettle_sound\n",
    "            # 52:6, #alarm\n",
    "            # #53:\"53_boiling_water_bubble_sound\", #boiling_water_bubble_sound\n",
    "            # 54:7, #rington\n",
    "            # 55:8, #shower_water\n",
    "            # 56:9, #pain_sounds\n",
    "            # 57:10, #footsteps\n",
    "            # 98:11, #silence\n",
    "            # 99:12, #other_sounds\n",
    "        #};\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            # print(f\"nClasses:{self.opt.nClasses}, type of mapdict:{type(self.mapdict)}, type of label1:{type(label1)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[label1]- 1\n",
    "            idx2 = self.mapdict[label2] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        print(f\"batchIndex is {batchIndex}, total sounds is {len(sounds)}\")\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1262f5-e327-4ba4-b8ea-f8570463bcaf",
   "metadata": {},
   "source": [
    "## ACDNetV2 define the acdnet model structure.\n",
    "定義原本的ACDNetV2，for載入pretrained acdnet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5099b20-938b-4841-85d7-fa6ee30ce834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACDNetV2(nn.Module):\n",
    "    def __init__(self, input_length, n_class, sr, ch_conf=None):\n",
    "        super(ACDNetV2, self).__init__();\n",
    "        self.input_length = input_length;\n",
    "        self.ch_config = ch_conf;\n",
    "\n",
    "        stride1 = 2;\n",
    "        stride2 = 2;\n",
    "        channels = 8;\n",
    "        k_size = (3, 3);\n",
    "        n_frames = (sr/1000)*10; #No of frames per 10ms\n",
    "\n",
    "        sfeb_pool_size = int(n_frames/(stride1*stride2));\n",
    "        # tfeb_pool_size = (2,2);\n",
    "        if self.ch_config is None:\n",
    "            self.ch_config = [channels, channels*8, channels*4, channels*8, channels*8, channels*16, channels*16, channels*32, channels*32, channels*64, channels*64, n_class];\n",
    "        # avg_pool_kernel_size = (1,4) if self.ch_config[1] < 64 else (2,4);\n",
    "        fcn_no_of_inputs = self.ch_config[-1];\n",
    "        conv1, bn1 = self.make_layers(1, self.ch_config[0], (1, 9), (1, stride1));\n",
    "        conv2, bn2 = self.make_layers(self.ch_config[0], self.ch_config[1], (1, 5), (1, stride2));\n",
    "        conv3, bn3 = self.make_layers(1, self.ch_config[2], k_size, padding=1);\n",
    "        conv4, bn4 = self.make_layers(self.ch_config[2], self.ch_config[3], k_size, padding=1);\n",
    "        conv5, bn5 = self.make_layers(self.ch_config[3], self.ch_config[4], k_size, padding=1);\n",
    "        conv6, bn6 = self.make_layers(self.ch_config[4], self.ch_config[5], k_size, padding=1);\n",
    "        conv7, bn7 = self.make_layers(self.ch_config[5], self.ch_config[6], k_size, padding=1);\n",
    "        conv8, bn8 = self.make_layers(self.ch_config[6], self.ch_config[7], k_size, padding=1);\n",
    "        conv9, bn9 = self.make_layers(self.ch_config[7], self.ch_config[8], k_size, padding=1);\n",
    "        conv10, bn10 = self.make_layers(self.ch_config[8], self.ch_config[9], k_size, padding=1);\n",
    "        conv11, bn11 = self.make_layers(self.ch_config[9], self.ch_config[10], k_size, padding=1);\n",
    "        conv12, bn12 = self.make_layers(self.ch_config[10], self.ch_config[11], (1, 1));\n",
    "        fcn = nn.Linear(fcn_no_of_inputs, n_class);\n",
    "        nn.init.kaiming_normal_(fcn.weight, nonlinearity='sigmoid') # kaiming with sigoid is equivalent to lecun_normal in keras\n",
    "\n",
    "        self.sfeb = nn.Sequential(\n",
    "            #Start: Filter bank\n",
    "            conv1, bn1, nn.ReLU(),\\\n",
    "            conv2, bn2, nn.ReLU(),\\\n",
    "            nn.MaxPool2d(kernel_size=(1, sfeb_pool_size))\n",
    "        );\n",
    "\n",
    "        tfeb_modules = [];\n",
    "        self.tfeb_width = int(((self.input_length / sr)*1000)/10); # 10ms frames of audio length in seconds\n",
    "        tfeb_pool_sizes = self.get_tfeb_pool_sizes(self.ch_config[1], self.tfeb_width);\n",
    "        p_index = 0;\n",
    "        for i in [3,4,6,8,10]:\n",
    "            tfeb_modules.extend([eval('conv{}'.format(i)), eval('bn{}'.format(i)), nn.ReLU()]);\n",
    "\n",
    "            if i != 3:\n",
    "                tfeb_modules.extend([eval('conv{}'.format(i+1)), eval('bn{}'.format(i+1)), nn.ReLU()]);\n",
    "\n",
    "            h, w = tfeb_pool_sizes[p_index];\n",
    "            if h>1 or w>1:\n",
    "                tfeb_modules.append(nn.MaxPool2d(kernel_size = (h,w)));\n",
    "            p_index += 1;\n",
    "\n",
    "        tfeb_modules.append(nn.Dropout(0.2));\n",
    "        tfeb_modules.extend([conv12, bn12, nn.ReLU()]);\n",
    "        h, w = tfeb_pool_sizes[-1];\n",
    "        if h>1 or w>1:\n",
    "            tfeb_modules.append(nn.AvgPool2d(kernel_size = (h,w)));\n",
    "        tfeb_modules.extend([nn.Flatten(), fcn]);\n",
    "\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules);\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Softmax(dim=1)\n",
    "        );\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sfeb(x);\n",
    "        #swapaxes\n",
    "        x = x.permute((0, 2, 1, 3));\n",
    "        x = self.tfeb(x);\n",
    "        y = self.output[0](x);\n",
    "        return y;\n",
    "\n",
    "    def make_layers(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias);\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu'); # kaiming with relu is equivalent to he_normal in keras\n",
    "        bn = nn.BatchNorm2d(out_channels);\n",
    "        return conv, bn;\n",
    "\n",
    "    def get_tfeb_pool_sizes(self, con2_ch, width):\n",
    "        h = self.get_tfeb_pool_size_component(con2_ch);\n",
    "        w = self.get_tfeb_pool_size_component(width);\n",
    "        # print(w);\n",
    "        pool_size = [];\n",
    "        for  (h1, w1) in zip(h, w):\n",
    "            pool_size.append((h1, w1));\n",
    "        return pool_size;\n",
    "\n",
    "    def get_tfeb_pool_size_component(self, length):\n",
    "        # print(length);\n",
    "        c = [];\n",
    "        index = 1;\n",
    "        while index <= 6:\n",
    "            if length >= 2:\n",
    "                if index == 6:\n",
    "                    c.append(length);\n",
    "                else:\n",
    "                    c.append(2);\n",
    "                    length = length // 2;\n",
    "            else:\n",
    "               c.append(1);\n",
    "\n",
    "            index += 1;\n",
    "\n",
    "        return c;\n",
    "\n",
    "def GetACDNetModel(input_len=30225, nclass=50, sr=20000, channel_config=None):\n",
    "    net = ACDNetV2(input_len, nclass, sr, ch_conf=channel_config);\n",
    "    return net;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47820f3a-2b2a-4eae-b56f-590433fef0ab",
   "metadata": {},
   "source": [
    "## load pretrained acdnet weights of 20khz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cbb28af-b51f-409c-ad38-ca8da8879874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acdnet_model = GetACDNetModel()\n",
    "# pretrain_weight= torch.load('./resources/pretrained_models/acdnet_20khz_trained_model_fold4_91.00.pt', map_location=torch.device('cpu'))['weight']\n",
    "\n",
    "# model_state = acdnet_model.state_dict()\n",
    "# model_state.update(pretrain_weight)\n",
    "# acdnet_model.load_state_dict(pretrain_weight, strict=False)\n",
    "\n",
    "# for k, v in pretrain_weight['weight'].items():\n",
    "#     print(\"name:\", k)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# remove the unexpected keys: weight and config\n",
    "# from collections import OrderedDict\n",
    "# new_state_dict = OrderedDict()\n",
    "# for k, v in checkpoint.items():\n",
    "#     name = k.replace(\"weight\", \"\") # remove `module.`\n",
    "#     new_state_dict[name] = v\n",
    "#     name = k.replace(\"config\", \"\") # remove `module.`\n",
    "#     new_state_dict[name] = v\n",
    "\n",
    "# model_state = acdnet_model.state_dict()\n",
    "# model_state.update(new_state_dict)\n",
    "# acdnet_model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "# print(\"acdnet_model state_dict:\\n\",acdnet_model.state_dict())\n",
    "# print(\"pretrain_weight: \\n\",pretrain_weight)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc665597-d662-466d-99d7-b32fa9a5afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_38_of_tfeb = list(acdnet_model.tfeb.children())[38]\n",
    "\n",
    "# print(layer_38_of_tfeb)\n",
    "# print(nn.Sequential(*list(acdnet_model.tfeb.children())[:-6]))\n",
    "# print(nn.Sequential(*list(acdnet_model.tfeb.children())))\n",
    "# print(acdnet_model)\n",
    "# for item_v in nn.Sequential(*list(acdnet_model.tfeb.children())):\n",
    "#     for internal_k, internal_v in item_v.named_parameters():\n",
    "#         print(internal_v.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa8d3d7-8658-4b73-a643-e7d68ddeab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(acdnet_model.fc)\n",
    "#acdnet 包含三部份：sfeb, tfeb and output\n",
    "# print(nn.Sequential(*list(acdnet_model.children())))\n",
    "# print(nn.Sequential(*list(acdnet_model.children())[:-1]))\n",
    "# for k, v in acdnet_model.named_parameters():\n",
    "#     print(\"key:\", k)\n",
    "#     v.requires_grad = False\n",
    "\n",
    "# acdnet_model.fcn = nn.Linear(num_ftrs, 10)\n",
    "# print(acdnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8e74198-72bd-48f2-90a9-8f4f29b9f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='ACDNet_TL_Model_Extend',  required=False);\n",
    "    parser.add_argument('--data', default='../datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args();\n",
    "    \n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 32;\n",
    "    opt.LR = 0.1;\n",
    "    opt.weightDecay = 5e-2;#5e-2;#1e-2;#5e-3;#5e-4;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.nEpochs = 1000;\n",
    "    opt.schedule = [0.3, 0.5, 0.9];\n",
    "    opt.warmup = 10;\n",
    "    if torch.backends.mps.is_available():\n",
    "        opt.device=\"mps\"; #for apple m2 gpu\n",
    "    elif torch.cuda.is_available():\n",
    "        opt.device=\"cuda:0\"; #for nVidia gpu\n",
    "    else:\n",
    "        opt.device=\"cpu\"\n",
    "    print(f\"***Use device:{opt.device}\");\n",
    "    # opt.device = torch.device(\"cuda:0\" if  else \"cpu\");\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 2#50;\n",
    "    opt.nFolds = 1;\n",
    "    opt.splits = [i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    return opt\n",
    "    # opt = parser.parse_args();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82574608-e5c1-47ea-817f-c1bfa8ffba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layers(in_channels, out_channels, kernel_size, stride=(1,1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias);\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu'); # kaiming with relu is equivalent to he_normal in keras\n",
    "        bn = nn.BatchNorm2d(out_channels);\n",
    "        return conv, bn;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "771ea414-dd74-4897-9f3d-f7566ea543fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_confing_10 = 8 * 64\n",
    "ch_n_class = 2\n",
    "fcn_no_of_inputs = 2\n",
    "# conv12, bn12 = self.make_layers(self.ch_config[10], self.ch_config[11], (1, 1));\n",
    "conv12, bn12 = make_layers(in_channels = ch_confing_10, out_channels = ch_n_class, kernel_size = (1, 1));\n",
    "fcn = nn.Linear(fcn_no_of_inputs, ch_n_class);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f157eff-03da-4e48-9701-606b472663e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACDNet_TL_Model_Extend(nn.Module):\n",
    "    def __init__(self, PretrainedWeights='./resources/pretrained_models/acdnet_20khz_trained_model_fold4_91.00.pt',opt=None):\n",
    "        super(ACDNet_TL_Model_Extend, self).__init__()\n",
    "        acdnet_model = GetACDNetModel(); # load original acdnet model first\n",
    "        # device = opt#torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"device is {opt.device}\")\n",
    "        pretrain_weight= torch.load(PretrainedWeights, map_location=torch.device(opt.device))['weight']\n",
    "        model_state = acdnet_model.state_dict()\n",
    "        model_state.update(pretrain_weight)\n",
    "        acdnet_model.load_state_dict(pretrain_weight, strict=False)\n",
    "        # print(type(acdnet_model))\n",
    "        # count = 0;\n",
    "        for k, v in acdnet_model.named_parameters():\n",
    "            # count += 1;\n",
    "            # print(f\"set {k} required_grade to False\");\n",
    "            v.requires_grad = False\n",
    "        # print(f\"count is {count}\");\n",
    "        self.sfeb = nn.Sequential(*list(acdnet_model.children())[0])\n",
    "        tfeb_modules = []\n",
    "        tfeb_modules.extend([*list(acdnet_model.tfeb.children())[:-6]])\n",
    "        tfeb_modules.extend([conv12, bn12, nn.ReLU()]);\n",
    "        tfeb_modules.append(nn.AvgPool2d(kernel_size = (2,4)));\n",
    "        tfeb_modules.extend([nn.Flatten(), fcn]);\n",
    "        # self.retrained_layers = nn.Sequential(*list(acdnet_model.tfeb.children())[:-1])\n",
    "        # fcn_no_of_inputs = 50, n_class=10\n",
    "        # n_class=6\n",
    "        # fc = nn.Linear(50, n_class);\n",
    "        # fc.requires_grad = True\n",
    "        # tfeb_modules.extend([fc])\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules)\n",
    "        self.output = nn.Sequential(\n",
    "        nn.Softmax(dim=1));\n",
    "        # print(f\"type of self.tfeb is {type(self.tfeb)}\")\n",
    "        # for k2, v2 in self.tfeb:\n",
    "        #     print(f\"k:{k}'s requires_grad is {v2.requires_grad}\");\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sfeb(x);\n",
    "        #swapaxes\n",
    "        x = x.permute((0, 2, 1, 3));\n",
    "        x = self.tfeb(x);\n",
    "        y = self.output[0](x);\n",
    "        return y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d89596f-04bf-4222-b23f-31ed75cd1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTLACDNet():\n",
    "    model = ACDNet_TL_Model_Extend(opt=getOpts());#ACDNet_TL_Model()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efbc0b0a-28fb-440f-97e3-c59ecb9fa47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = GetTLACDNet()\n",
    "# calc.summary(test_model, (1,1,30225))\n",
    "# print(test_model)\n",
    "# print(test_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e62e010-cd9a-42cb-ba31-66bb71ae1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c3ca4f4-2d45-488d-ba0f-3466bd397cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDataTimeStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H:%M:%S').replace('-',\"\").replace(' ',\"\").replace(':',\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45c56d94-f01d-4b0b-8c90-85f6e29421f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n報錯訊息：\\nTypeError: can't convert np.ndarray of type numpy.object_. \\nThe only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.\\n1\\n2\\n問題描述：\\n當把np轉換成torch tensor時，\\n\\ntrainx = torch.from_numpy(np.reshape(train_x, newshape=(-1,25)))\\n1\\n解決方法：\\n由於讀入的numpy陣列裡的元素是object類型，無法將此型別轉換成tensor。\\n\\n所以，將numpy數組進行強制型別轉換成float型別（或任何pytorch支援的型別：float64, float32, float16, int64, int32, int16, int8, uint8, and bool）即可。\\n\\ntrainx = trainx.astype(float)  # numpy強制轉型\\n————————————————\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "報錯訊息：\n",
    "TypeError: can't convert np.ndarray of type numpy.object_. \n",
    "The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.\n",
    "1\n",
    "2\n",
    "問題描述：\n",
    "當把np轉換成torch tensor時，\n",
    "\n",
    "trainx = torch.from_numpy(np.reshape(train_x, newshape=(-1,25)))\n",
    "1\n",
    "解決方法：\n",
    "由於讀入的numpy陣列裡的元素是object類型，無法將此型別轉換成tensor。\n",
    "\n",
    "所以，將numpy數組進行強制型別轉換成float型別（或任何pytorch支援的型別：float64, float32, float16, int64, int32, int16, int8, uint8, and bool）即可。\n",
    "\n",
    "trainx = trainx.astype(float)  # numpy強制轉型\n",
    "————————————————\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d36f12e-f8a5-41de-bc65-95bb84e23a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n解決RuntimeError: Input type and weight type should be the same\\n\\n\\n根據報錯資訊的意思可以推斷，這個錯誤是由輸入和權重的資料類型不一致引起的。因此解決方法很簡單，就是將輸入的資料和模型參數的資料類型統一即可。在這個例子中，有以下幾個解決方法。\\n\\n1.將輸入資料（torch.tensor 形式）轉換成FloatTensor形式，如下：\\n\\n# net_in是torch.tensor形式的输入数据\\nnet_in = net_in.float();\\n1\\n2\\n2.如果輸入資料在轉變為torch.tensor前是以numpy數組的形式儲存的，我們可以將資料提前轉變為float32形式，具體如下：\\n\\n# train_set是numpy.array形式的输入数据\\nimport numpy as np\\nX = train_set.astype(np.float32);\\n1\\n2\\n3\\n3.將模型參數類型轉換為與輸入張量（tensor）一致的型別。在這個例子裡，模型參數需轉換為DoubleTensor，如下所示：\\n\\nmodel.double()\\n1\\n可選擇以上任一方法解決這個問題。但在實際應用上需要注意，第三種解決方法會增加顯存的需求量。更多關於torch中張量（tensor）資料類型的介紹，可參考這個網頁Link。\\n————————————————\\n版权声明：本文为CSDN博主「Henry积少成多」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\\n原文链接：https://blog.csdn.net/qq_34612816/article/details/123372456\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "解決RuntimeError: Input type and weight type should be the same\n",
    "\n",
    "\n",
    "根據報錯資訊的意思可以推斷，這個錯誤是由輸入和權重的資料類型不一致引起的。因此解決方法很簡單，就是將輸入的資料和模型參數的資料類型統一即可。在這個例子中，有以下幾個解決方法。\n",
    "\n",
    "1.將輸入資料（torch.tensor 形式）轉換成FloatTensor形式，如下：\n",
    "\n",
    "# net_in是torch.tensor形式的输入数据\n",
    "net_in = net_in.float();\n",
    "1\n",
    "2\n",
    "2.如果輸入資料在轉變為torch.tensor前是以numpy數組的形式儲存的，我們可以將資料提前轉變為float32形式，具體如下：\n",
    "\n",
    "# train_set是numpy.array形式的输入数据\n",
    "import numpy as np\n",
    "X = train_set.astype(np.float32);\n",
    "1\n",
    "2\n",
    "3\n",
    "3.將模型參數類型轉換為與輸入張量（tensor）一致的型別。在這個例子裡，模型參數需轉換為DoubleTensor，如下所示：\n",
    "\n",
    "model.double()\n",
    "1\n",
    "可選擇以上任一方法解決這個問題。但在實際應用上需要注意，第三種解決方法會增加顯存的需求量。更多關於torch中張量（tensor）資料類型的介紹，可參考這個網頁Link。\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「Henry积少成多」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/qq_34612816/article/details/123372456\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "463a7a09-5a89-4097-b4d2-1f8e50f8b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLTrainer:\n",
    "    def __init__(self, opt=None, classes_dict=None):\n",
    "        self.opt = opt;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.bestAcc = 0.0;\n",
    "        self.bestAccEpoch = 0;\n",
    "        self.trainGen = getTrainGen(opt,classes_dict=classes_dict)#train_generator.setup(opt, split);\n",
    "        self.opt = opt;\n",
    "        # self.opt.trainer = self;\n",
    "        # self.trainGen = getTrainGen(self.opt, self.opt.splits)#train_generator.setup(self.opt, self.opt.split);\n",
    "        # self.pretrainedmodelpath = \"./resources/pretrained_models/acdnet20_20khz_fold4.h5\"\n",
    "\n",
    "    def Train(self):\n",
    "        train_start_time = time.time();\n",
    "        net = GetTLACDNet().to(self.opt.device)#models.GetACDNetModel().to(self.opt.device);\n",
    "        #print networks parameters' require_grade value\n",
    "        for k_, v_ in net.named_parameters():\n",
    "            print(f\"{k_}:{v_.requires_grad}\")\n",
    "        print('ACDNet model has been prepared for training');\n",
    "\n",
    "        calc.summary(net, (1,1,self.opt.inputLength));\n",
    "\n",
    "        # training_text = \"Re-Training\" if self.opt.retrain else \"Training from Scratch\";\n",
    "        # print(\"{} has been started. You will see update after finishing every training epoch and validation\".format(training_text));\n",
    "\n",
    "        lossFunc = torch.nn.KLDivLoss(reduction='batchmean');\n",
    "        optimizer = optim.SGD(net.parameters(), lr=self.opt.LR, weight_decay=self.opt.weightDecay, momentum=self.opt.momentum, nesterov=True);\n",
    "\n",
    "        # self.opt.nEpochs = 1957 if self.opt.split == 4 else 2000;\n",
    "        for epochIdx in range(self.opt.nEpochs):\n",
    "            epoch_start_time = time.time();\n",
    "            optimizer.param_groups[0]['lr'] = self.__get_lr(epochIdx+1);\n",
    "            cur_lr = optimizer.param_groups[0]['lr'];\n",
    "            running_loss = 0.0;\n",
    "            running_acc = 0.0;\n",
    "            n_batches = math.ceil(len(self.trainGen.data)/self.opt.batchSize);\n",
    "            for batchIdx in range(n_batches):\n",
    "                # with torch.no_grad():\n",
    "                x,y = self.trainGen.__getitem__(batchIdx)\n",
    "                x = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "                y = torch.tensor(y).to(self.opt.device);\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad();\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = net(x);\n",
    "                running_acc += (((outputs.data.argmax(dim=1) == y.argmax(dim=1))*1).float().mean()).item();\n",
    "                loss = lossFunc(outputs.log(), y);\n",
    "                loss.backward();\n",
    "                optimizer.step();\n",
    "\n",
    "                running_loss += loss.item();\n",
    "\n",
    "            tr_acc = (running_acc / n_batches)*100;\n",
    "            tr_loss = running_loss / n_batches;\n",
    "\n",
    "            #Epoch wise validation Validation\n",
    "            epoch_train_time = time.time() - epoch_start_time;\n",
    "\n",
    "            net.eval();\n",
    "            val_acc, val_loss = self.__validate(net, lossFunc);\n",
    "            #Save best model\n",
    "            self.__save_model(val_acc, epochIdx, net);\n",
    "            self.__on_epoch_end(epoch_start_time, epoch_train_time, epochIdx, cur_lr, tr_loss, tr_acc, val_loss, val_acc);\n",
    "\n",
    "            running_loss = 0;\n",
    "            running_acc = 0;\n",
    "            net.train();\n",
    "\n",
    "        total_time_taken = time.time() - train_start_time;\n",
    "        print(\"Execution finished in: {}\".format(U.to_hms(total_time_taken)));\n",
    "\n",
    "    def load_test_data(self):\n",
    "        # data = np.load(os.path.join(self.opt.data, self.opt.dataset, 'test_data_{}khz/fold{}_test4000.npz'.format(self.opt.sr//1000, self.opt.split)), allow_pickle=True);\n",
    "        data = np.load(self.opt.testData, allow_pickle=True);\n",
    "        print(f\"device is :{self.opt.device}\")\n",
    "        print(f\"len of Y:{len(data['y'])}\")\n",
    "        # self.testX = torch.tensor(np.moveaxis(data['x'], 3, 1)).to(self.opt.device);\n",
    "        dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "        self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "        self.testY = torch.tensor(data['y']).type(torch.float32).to(self.opt.device);\n",
    "\n",
    "    def __get_lr(self, epoch):\n",
    "        divide_epoch = np.array([self.opt.nEpochs * i for i in self.opt.schedule]);\n",
    "        decay = sum(epoch > divide_epoch);\n",
    "        if epoch <= self.opt.warmup:\n",
    "            decay = 1;\n",
    "        return self.opt.LR * np.power(0.1, decay);\n",
    "\n",
    "    def __get_batch(self, index):\n",
    "        x = self.trainX[index*self.opt.batchSize : (index+1)*self.opt.batchSize];\n",
    "        y = self.trainY[index*self.opt.batchSize : (index+1)*self.opt.batchSize];\n",
    "        return x.to(self.opt.device), y.to(self.opt.device);\n",
    "\n",
    "    def __validate(self, net, lossFunc):\n",
    "        if self.testX is None:\n",
    "            self.load_test_data();\n",
    "        net.eval();\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = len(self.testX);#(self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "#             for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "#             for idx in range(len(self.testX)):\n",
    "#             x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "            x = self.testX[:];\n",
    "            scores = net(x);\n",
    "            y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "            acc, loss = self.__compute_accuracy(y_pred, self.testY, lossFunc);\n",
    "#         with torch.no_grad():\n",
    "#             y_pred = None;\n",
    "#             batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "#             for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "#                 x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "#                 scores = net(x);\n",
    "#                 y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "\n",
    "#             acc, loss = self.__compute_accuracy(y_pred, self.testY, lossFunc);\n",
    "        net.train();\n",
    "        return acc, loss;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def __compute_accuracy(self, y_pred, y_target, lossFunc):\n",
    "        print(f\"shape of y_pred:{y_pred.shape}\");\n",
    "        print(f\"shape of y_target:{y_target.shape}\");\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #Reshape to shape theme like each sample comtains 10 samples, calculate mean and find theindices that has highest average value for each sample\n",
    "            if self.opt.nCrops == 1:\n",
    "                y_pred = y_pred.argmax(dim=1);\n",
    "                y_target = y_target.argmax(dim=1);\n",
    "            else:\n",
    "                y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "                y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "                print(f\"after: len of y_pred:{len(y_pred)}, len of y_target:{len(y_target)}\")\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "            # valLossFunc = torch.nn.KLDivLoss();\n",
    "            loss = lossFunc(y_pred.float().log(), y_target.float()).item();\n",
    "            # loss = 0.0;\n",
    "        return acc, loss;\n",
    "\n",
    "    def __on_epoch_end(self, start_time, train_time, epochIdx, lr, tr_loss, tr_acc, val_loss, val_acc):\n",
    "        epoch_time = time.time() - start_time;\n",
    "        val_time = epoch_time - train_time;\n",
    "        line = 'SP-{} Epoch: {}/{} | Time: {} (Train {}  Val {}) | Train: LR {}  Loss {:.2f}  Acc {:.2f}% | Val: Loss {:.2f}  Acc(top1) {:.2f}% | HA {:.2f}@{}\\n'.format(\n",
    "            self.opt.splits, epochIdx+1, self.opt.nEpochs, U.to_hms(epoch_time), U.to_hms(train_time), U.to_hms(val_time),\n",
    "            lr, tr_loss, tr_acc, val_loss, val_acc, self.bestAcc, self.bestAccEpoch);\n",
    "        # print(line)\n",
    "        sys.stdout.write(line);\n",
    "        sys.stdout.flush();\n",
    "\n",
    "    def __save_model(self, acc, epochIdx, net):\n",
    "        print(\"__save_model is called\")\n",
    "        print(f\"current best Acc is {self.bestAcc}\")\n",
    "        print(f\"pass in acc is {acc}\")\n",
    "        if acc > self.bestAcc:\n",
    "            dir = os.getcwd();\n",
    "            save_path = \"./trained_models/{}\".format(self.opt.model_name.format(genDataTimeStr(),acc,epochIdx));\n",
    "            # fname = \"{}/torch/trained_models/{}_fold{}.pt\";\n",
    "            # fname = \"{}/trained_models/acdnet_torch_20231218.pt\";\n",
    "            # old_model = fname.format(dir, self.opt.model_name.lower(), self.opt.splits);\n",
    "            # if os.path.isfile(old_model):\n",
    "            #     os.remove(old_model);\n",
    "            self.bestAcc = acc;\n",
    "            self.bestAccEpoch = epochIdx +1;\n",
    "            # torch.save({'weight':net.state_dict(), 'config':net.ch_config}, fname.format(dir, self.opt.model_name.lower(), self.opt.split));\n",
    "            # torch.save({'weight':net.state_dict()}, fname.format(dir, self.opt.model_name.lower(), self.opt.splits));\n",
    "            torch.save({'weight':net.state_dict()}, save_path);\n",
    "            print(f\"model saved....., acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebb03c91-9d87-44bf-86d5-793e2df7fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None, classes_dict=None):\n",
    "    # dataset = np.load(os.path.join(opt.data, opt.dataset, 'wav{}.npz'.format(opt.sr // 1000)), allow_pickle=True);\n",
    "    # dataset = np.load(\"../datasets/fold1_test16000.npz\", allow_pickle=True);\n",
    "    dataset = np.load(opt.trainData, allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # print(len(dataset['x']))\n",
    "    # for i in range(1, opt.nFolds + 1):\n",
    "\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['fold{}'.format(1)].item()['sounds']\n",
    "    train_labels = dataset['fold{}'.format(1)].item()['labels']\n",
    "    # print(train_sounds)\n",
    "\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt, classes_dict=classes_dict);\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "866c74b6-6287-4a45-90f7-99a84e6a47a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif the traing data size is too small we need larger weight_decay value to prevent overfit\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model acc:93.1\n",
    "epoch:183\n",
    "weightDecay = 5e-4;\n",
    "batch_size:32\n",
    "learning_rate:0.05\n",
    "momentum: 0.045\n",
    "schedule = [0.15, 0.25, 0.45]\n",
    "-------------------------\n",
    "model acc:86\n",
    "epoch:22\n",
    "weightDecay = 5e-4;\n",
    "batch_size:32\n",
    "learning_rate:0.1\n",
    "momentum: 0.09\n",
    "schedule = [0.3, 0.5, 0.9]\n",
    "--------------------------\n",
    "model acc:93.1\n",
    "epoch:303\n",
    "weightDecay = 5e-5;\n",
    "batch_size:32\n",
    "learning_rate:0.05\n",
    "momentum: 0.045\n",
    "schedule = [0.15, 0.25, 0.45]\n",
    "--------------------------\n",
    "model acc:46\n",
    "epoch:1500\n",
    "weightDecay = 5e-5;\n",
    "batch_size:16\n",
    "learning_rate:0.001\n",
    "momentum: 0.0009\n",
    "schedule = [0.003, 0.005, 0.009]\n",
    "--------------------------\n",
    "model acc: acc_89.6551742553711_196th_epoch\n",
    "epoch:196\n",
    "weightDecay = 25e-5;\n",
    "batch_size:32\n",
    "learning_rate:0.05\n",
    "momentum: 0.048\n",
    "schedule = [0.15, 0.25, 0.45]\n",
    "--------------------------\n",
    "model acc:acc_93.10344696044922_199th_epoch\n",
    "epoch:199\n",
    "weightDecay = 10e-4;\n",
    "batch_size:32\n",
    "learning_rate:0.05\n",
    "momentum: 0.048\n",
    "schedule = [0.15, 0.25, 0.45]\n",
    "--------------------------\n",
    "model acc:87.93103790283203_80th_epoch\n",
    "epoch:80\n",
    "weightDecay = 1e-2;\n",
    "batch_size:32\n",
    "learning_rate:0.01\n",
    "momentum: 0.009\n",
    "schedule = [0.03, 0.05, 0.09]\n",
    "--------------------------\n",
    "model acc:acc_96.55171966552734_174th_epoch\n",
    "epoch:174\n",
    "weightDecay = 5e-3;\n",
    "batch_size:32\n",
    "learning_rate:0.1\n",
    "momentum: 0.09\n",
    "schedule = [0.3, 0.5, 0.9]\n",
    "--------------------------\n",
    "model acc:94.82758331298828_180th_epoch\n",
    "epoch:180\n",
    "weightDecay = 8e-3;\n",
    "batch_size:32\n",
    "learning_rate:0.1\n",
    "momentum: 0.09\n",
    "schedule = [0.3, 0.5, 0.9]\n",
    "--------------------------\n",
    "model acc:93.10344696044922_13th_epoch\n",
    "epoch:13\n",
    "weightDecay = 5e-3;\n",
    "batch_size:64\n",
    "learning_rate:0.1\n",
    "momentum: 0.09\n",
    "schedule = [0.3, 0.5, 0.9]\n",
    "--------------------------\n",
    "model acc:96.55\n",
    "epoch: 278\n",
    "weightDecay = 5e-3;\n",
    "batch_size:24\n",
    "learning_rate:0.1\n",
    "momentum: 0.09\n",
    "schedule = [0.3, 0.5, 0.9]\n",
    "--------------------------\n",
    "model acc: 94.82758331298828_109th_epoch\n",
    "epoch:109\n",
    "weightDecay = 5e-3;\n",
    "batch_size:16\n",
    "learning_rate:0.1\n",
    "momentum: 0.09\n",
    "schedule = [0.3, 0.5, 0.9]\n",
    "--------------------------\n",
    "model acc:93.10344696044922_13th_epoch\n",
    "epoch:13\n",
    "weightDecay = 1e-2;\n",
    "batch_size:16\n",
    "learning_rate:0.1\n",
    "momentum: 0.09\n",
    "schedule = [0.3, 0.5, 0.9]\n",
    "--------------------------\n",
    "model acc:94.82758331298828_180th_epoch\n",
    "epoch:180\n",
    "weightDecay = 1e-2;\n",
    "batch_size:32\n",
    "learning_rate:0.1\n",
    "momentum: 0.09\n",
    "schedule = [0.3, 0.5, 0.9]\n",
    "--------------------------\n",
    "model acc:91.37931060791016_199th_epoch\n",
    "epoch:199\n",
    "weightDecay = 1e-2;\n",
    "batch_size:32\n",
    "learning_rate:0.05\n",
    "momentum: 0.045\n",
    "schedule = [0.15, 0.25, 0.45]\n",
    "--------------------------\n",
    "model acc:93.10344696044922_108th_epoch\n",
    "epoch:108\n",
    "weightDecay = 5e-2;\n",
    "batch_size:32\n",
    "learning_rate:0.1\n",
    "momentum: 0.09\n",
    "schedule = [0.3, 0.5, 0.9]\n",
    "--------------------------\n",
    "model acc:\n",
    "epoch:\n",
    "weightDecay = 2.5e-2;\n",
    "batch_size:32\n",
    "learning_rate:0.1\n",
    "momentum: 0.09\n",
    "schedule = [0.3, 0.5, 0.9]\n",
    "--------------------------\n",
    "model acc:\n",
    "epoch:\n",
    "weightDecay = 5e-2;\n",
    "batch_size:32\n",
    "learning_rate:0.1\n",
    "momentum: 0.09\n",
    "schedule = [0.3, 0.5, 0.9]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "if the traing data size is too small we need larger weight_decay value to prevent overfit\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51b05f11-5bf2-438b-bfbb-edb0d48b05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    map_dict_train = {\n",
    "        52:1, #alarm\n",
    "        99:2, #other_sounds\n",
    "    };\n",
    "    opt = getOpts();\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 30225;\n",
    "    opt.trainer = None\n",
    "    opt.trainData=\"../datasets/forOneClassModel_alarm/train_test_npz/trainData_20240108153111.npz\";\n",
    "    opt.testData=\"../datasets/forOneClassModel_alarm/train_test_npz/compressed_val_npz_20240108191921.npz\";\n",
    "    # import torch;\n",
    "    \n",
    "    tlopts.display_info(opt)\n",
    "    opt.model_name = \"acdnet_alarm_{}_acc_{}_{}th_epoch.pt\"\n",
    "    # valid_path = False;\n",
    "    print(\"Initializing TLTrainer Object.....\")\n",
    "    trainer = TLTrainer(opt,classes_dict=map_dict_train)\n",
    "    print(\"Start to training.....\")\n",
    "    trainer.Train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5215371b-d1a4-4980-a7a3-0cdce4c37515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94166430-c5c1-421a-84f9-bdde77ad1c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Use device:mps\n",
      "+------------------------------+\n",
      "| ACDNet_TL_Model_Extend Sound classification\n",
      "+------------------------------+\n",
      "| dataset  : uec_iot\n",
      "| nEpochs  : 1000\n",
      "| LRInit   : 0.1\n",
      "| schedule : [0.3, 0.5, 0.9]\n",
      "| warmup   : 10\n",
      "| batchSize: 32\n",
      "| nFolds: 1\n",
      "| Splits: [1]\n",
      "+------------------------------+\n",
      "Initializing TLTrainer Object.....\n",
      "length of samples:167\n",
      "Start to training.....\n",
      "***Use device:mps\n",
      "device is mps\n",
      "sfeb.0.weight:False\n",
      "sfeb.1.weight:False\n",
      "sfeb.1.bias:False\n",
      "sfeb.3.weight:False\n",
      "sfeb.4.weight:False\n",
      "sfeb.4.bias:False\n",
      "tfeb.0.weight:False\n",
      "tfeb.1.weight:False\n",
      "tfeb.1.bias:False\n",
      "tfeb.4.weight:False\n",
      "tfeb.5.weight:False\n",
      "tfeb.5.bias:False\n",
      "tfeb.7.weight:False\n",
      "tfeb.8.weight:False\n",
      "tfeb.8.bias:False\n",
      "tfeb.11.weight:False\n",
      "tfeb.12.weight:False\n",
      "tfeb.12.bias:False\n",
      "tfeb.14.weight:False\n",
      "tfeb.15.weight:False\n",
      "tfeb.15.bias:False\n",
      "tfeb.18.weight:False\n",
      "tfeb.19.weight:False\n",
      "tfeb.19.bias:False\n",
      "tfeb.21.weight:False\n",
      "tfeb.22.weight:False\n",
      "tfeb.22.bias:False\n",
      "tfeb.25.weight:False\n",
      "tfeb.26.weight:False\n",
      "tfeb.26.bias:False\n",
      "tfeb.28.weight:False\n",
      "tfeb.29.weight:False\n",
      "tfeb.29.bias:False\n",
      "tfeb.33.weight:True\n",
      "tfeb.34.weight:True\n",
      "tfeb.34.bias:True\n",
      "tfeb.38.weight:True\n",
      "tfeb.38.bias:True\n",
      "ACDNet model has been prepared for training\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 30225)     (8, 1, 15109)         72    1,087,848\n",
      "  BatchNorm2d-2     (8, 1, 15109)     (8, 1, 15109)         16            0\n",
      "         ReLu-3     (8, 1, 15109)     (8, 1, 15109)          0      120,872\n",
      "       Conv2d-4     (8, 1, 15109)     (64, 1, 7553)      2,560   19,335,680\n",
      "  BatchNorm2d-5     (64, 1, 7553)     (64, 1, 7553)        128            0\n",
      "         ReLu-6     (64, 1, 7553)     (64, 1, 7553)          0      483,392\n",
      "    MaxPool2d-7     (64, 1, 7553)      (64, 1, 151)          0      483,200\n",
      "      Permute-8      (64, 1, 151)      (1, 64, 151)          0            0\n",
      "       Conv2d-9      (1, 64, 151)     (32, 64, 151)        288    2,783,232\n",
      " BatchNorm2d-10     (32, 64, 151)     (32, 64, 151)         64            0\n",
      "        ReLu-11     (32, 64, 151)     (32, 64, 151)          0      309,248\n",
      "   MaxPool2d-12     (32, 64, 151)      (32, 32, 75)          0      307,200\n",
      "      Conv2d-13      (32, 32, 75)      (64, 32, 75)     18,432   44,236,800\n",
      " BatchNorm2d-14      (64, 32, 75)      (64, 32, 75)        128            0\n",
      "        ReLu-15      (64, 32, 75)      (64, 32, 75)          0      153,600\n",
      "      Conv2d-16      (64, 32, 75)      (64, 32, 75)     36,864   88,473,600\n",
      " BatchNorm2d-17      (64, 32, 75)      (64, 32, 75)        128            0\n",
      "        ReLu-18      (64, 32, 75)      (64, 32, 75)          0      153,600\n",
      "   MaxPool2d-19      (64, 32, 75)      (64, 16, 37)          0      151,552\n",
      "      Conv2d-20      (64, 16, 37)     (128, 16, 37)     73,728   43,646,976\n",
      " BatchNorm2d-21     (128, 16, 37)     (128, 16, 37)        256            0\n",
      "        ReLu-22     (128, 16, 37)     (128, 16, 37)          0       75,776\n",
      "      Conv2d-23     (128, 16, 37)     (128, 16, 37)    147,456   87,293,952\n",
      " BatchNorm2d-24     (128, 16, 37)     (128, 16, 37)        256            0\n",
      "        ReLu-25     (128, 16, 37)     (128, 16, 37)          0       75,776\n",
      "   MaxPool2d-26     (128, 16, 37)      (128, 8, 18)          0       73,728\n",
      "      Conv2d-27      (128, 8, 18)      (256, 8, 18)    294,912   42,467,328\n",
      " BatchNorm2d-28      (256, 8, 18)      (256, 8, 18)        512            0\n",
      "        ReLu-29      (256, 8, 18)      (256, 8, 18)          0       36,864\n",
      "      Conv2d-30      (256, 8, 18)      (256, 8, 18)    589,824   84,934,656\n",
      " BatchNorm2d-31      (256, 8, 18)      (256, 8, 18)        512            0\n",
      "        ReLu-32      (256, 8, 18)      (256, 8, 18)          0       36,864\n",
      "   MaxPool2d-33      (256, 8, 18)       (256, 4, 9)          0       36,864\n",
      "      Conv2d-34       (256, 4, 9)       (512, 4, 9)  1,179,648   42,467,328\n",
      " BatchNorm2d-35       (512, 4, 9)       (512, 4, 9)      1,024            0\n",
      "        ReLu-36       (512, 4, 9)       (512, 4, 9)          0       18,432\n",
      "      Conv2d-37       (512, 4, 9)       (512, 4, 9)  2,359,296   84,934,656\n",
      " BatchNorm2d-38       (512, 4, 9)       (512, 4, 9)      1,024            0\n",
      "        ReLu-39       (512, 4, 9)       (512, 4, 9)          0       18,432\n",
      "   MaxPool2d-40       (512, 4, 9)       (512, 2, 4)          0       16,384\n",
      "      Conv2d-41       (512, 2, 4)         (2, 2, 4)      1,024        8,192\n",
      " BatchNorm2d-42         (2, 2, 4)         (2, 2, 4)          4            0\n",
      "        ReLu-43         (2, 2, 4)         (2, 2, 4)          0           16\n",
      "   AvgPool2d-44         (2, 2, 4)         (2, 1, 1)          0           16\n",
      "     Flatten-45         (2, 1, 1)            (1, 2)          0            0\n",
      "      Linear-46            (1, 2)            (1, 2)          6            6\n",
      "     Softmax-47            (1, 2)            (1, 2)          0            2\n",
      "==============================================================================\n",
      "Total Params: 4,708,162\n",
      "Total FLOPs : 544,222,072\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.12\n",
      "Params size (MB): 17.96\n",
      "Total size (MB) : 18.08\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "batchIndex is 0, total sounds is 32\n",
      "batchIndex is 1, total sounds is 32\n",
      "batchIndex is 2, total sounds is 32\n",
      "batchIndex is 3, total sounds is 32\n",
      "batchIndex is 4, total sounds is 32\n",
      "batchIndex is 5, total sounds is 32\n",
      "device is :mps\n",
      "len of Y:116\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e748bbe-3e6b-4bef-9457-c995cac69a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c24dcd-bb04-433f-9294-b95709fa8be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
